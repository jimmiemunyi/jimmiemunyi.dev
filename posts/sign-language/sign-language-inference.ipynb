{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"sign language inference\"\n",
    "description: \"Creating a realtime sign language intepreter.\"\n",
    "date: \"2021-01-21\"\n",
    "formart:\n",
    "  html:\n",
    "    code-fold: false\n",
    "categories: [technical, project, computer-vision]\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXYjwxciJi2e"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2IpTq6ZJlEZ"
   },
   "source": [
    "After training our model in [Part A](/docs/posts/sign-language/sign-language-classification.html), we are now going to develop an application to run inference with for new data.\n",
    "\n",
    "I am going to be utilizing `opencv` to get live video from my webcam, then run our model against each frame in the video and get the prediction of what Sign Language Letter I am holding up.\n",
    "\n",
    "Here is an example of what the output will look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{< video https://youtu.be/-nggi8EwfOA >}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole code + training notebooks from Part A can be found in this [github repo](https://github.com/jimmiemunyi/Sign-Language-App)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPK94-_gQzbr"
   },
   "source": [
    "This tutorial assumes some basic understanding of the `cv2` library and general understanding of how to run inference using a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVnlvC_NMEH7"
   },
   "source": [
    "# The Full Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u28yphPtMGmn"
   },
   "source": [
    "Here is the full code of making the App if you just want the code.\n",
    "\n",
    "I will explain each part of the code and what was my thinking behind it in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vf6Wv2l38-8f"
   },
   "outputs": [],
   "source": [
    "from collections import deque, Counter\n",
    "\n",
    "import cv2\n",
    "from fastai.vision.all import *\n",
    "\n",
    "print('Loading our Inference model...')\n",
    "# load our inference model\n",
    "inf_model = load_learner('model/sign_language.pkl')\n",
    "print('Model Loaded')\n",
    "\n",
    "\n",
    "# define a deque to get rolling average of predictions\n",
    "# I go with the last 10 predictions\n",
    "rolling_predictions = deque([], maxlen=10)\n",
    "\n",
    "# get the most common item in the deque\n",
    "def most_common(D):\n",
    "    data = Counter(D)\n",
    "    return data.most_common(1)[0][0]\n",
    "\n",
    "\n",
    "def hand_area(img):\n",
    "    # specify where hand should go\n",
    "    hand = img[50:324, 50:324]\n",
    "    # the images in the model were trainind on 200x200 pixels\n",
    "    hand = cv2.resize(hand, (200,200))\n",
    "    return hand\n",
    "\n",
    "# capture video on the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "# get the dimensions on the frame\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "\n",
    "# define codec and create our VideoWriter to save the video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('output/sign-language.mp4', fourcc, 12, (frame_width, frame_height))\n",
    "\n",
    "\n",
    "# read video\n",
    "while True:\n",
    "    # capture each frame of the video\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # flip frame to feel more 'natural' to webcam\n",
    "    frame = cv2.flip(frame, flipCode = 1)\n",
    "\n",
    "    # draw a blue rectangle where to place hand\n",
    "    cv2.rectangle(frame, (50, 50), (324, 324), (255, 0, 0), 2)\n",
    "\n",
    "    # get the image\n",
    "    inference_image = hand_area(frame)\n",
    "\n",
    "    # get the current prediction on the hand\n",
    "    pred = inf_model.predict(inference_image)\n",
    "    # append the current prediction to our rolling predictions\n",
    "    rolling_predictions.append(pred[0])\n",
    "\n",
    "    # our prediction is going to be the most common letter\n",
    "    # in our rolling predictions\n",
    "    prediction_output = f'The predicted letter is {most_common(rolling_predictions)}'\n",
    "\n",
    "    # show predicted text\n",
    "    cv2.putText(frame, prediction_output, (10, 350), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "    # show the frame\n",
    "    cv2.imshow('frame', frame)\n",
    "    # save the frames to out file\n",
    "    out.write(frame)\n",
    "\n",
    "\n",
    "    # press `q` to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# release VideoCapture()\n",
    "cap.release()\n",
    "# release out file\n",
    "out.release()\n",
    "# close all frames and video windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5F4n0rfGMx2n"
   },
   "source": [
    "# Explaining the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3fyYM8YM0MG"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZnqSb9ZM3B2"
   },
   "source": [
    "Install [fastai](https://github.com/fastai/fastai) and [opencv-python](https://pypi.org/project/opencv-python/).\n",
    "\n",
    "Next, this are the packages I utilize for this App. `fastai` is going to be used to run Inference with, `cv2` is going to handle all the WebCam functionality and we are going to utilize `deque` and `Counter` from collections to apply a nifty trick I am going to show you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rZOonvOMzoW"
   },
   "outputs": [],
   "source": [
    "from collections import deque, Counter\n",
    "\n",
    "import cv2\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDKRcdqGNpXY"
   },
   "source": [
    "## Loading our Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_XF5fH_uNrqg"
   },
   "outputs": [],
   "source": [
    "print('Loading our Inference model...')\n",
    "# load our inference model\n",
    "inf_model = load_learner('model/sign_language.pkl')\n",
    "print('Model Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElDwPTLANt0B"
   },
   "source": [
    "The next part of our code loads the model we pickled in Part A and prints some useful information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZU5bXpcN9IT"
   },
   "source": [
    "## Rolling Average Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jq_i7hD7ODv5"
   },
   "source": [
    "When I first made the App, I noticed one problem when using it. A slight movement of my hand was changing the predictions. This is known as `flickering`. The video below shows how flickering affects our App:\n",
    "\n",
    "{{< video https://youtu.be/aPAG39MjN68>}}\n",
    "\n",
    "\n",
    "The Video you saw in the beginning shows how 'stable' our model is after using rolling predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2mbiDmVsN56J"
   },
   "outputs": [],
   "source": [
    "# define a deque to get rolling average of predictions\n",
    "# I go with the last 10 predictions\n",
    "rolling_predictions = deque([], maxlen=10)\n",
    "\n",
    "# get the most common item in the deque\n",
    "def most_common(D):\n",
    "    data = Counter(D)\n",
    "    return data.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WA-iDc5OuB7"
   },
   "source": [
    "To solve this, a utilized the deque from Collections. I used 10 as the maxlength of the deque since I wanted the App, when running inference, to output the most common prediction out of the last 10 predictions. This makes it more stable than when we are using only the current one.\n",
    "\n",
    "The function `most_common` will return the most common item in our deque."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCktGCjAPZD4"
   },
   "source": [
    "## Hand Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rfXu7xcqPYPH"
   },
   "outputs": [],
   "source": [
    "def hand_area(img):\n",
    "    # specify where hand should go\n",
    "    hand = img[50:324, 50:324]\n",
    "    # the images in the model were trainind on 200x200 pixels\n",
    "    hand = cv2.resize(hand, (200,200))\n",
    "    return hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RrF-hhd0Pc3X"
   },
   "source": [
    "Next, we define a function that tells our model which part of the video to run inference on. We do not want to run inference on the whole video which will include our face! We will eventually draw a blue rectangle in this area so that you'll know where to place your hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxcHbid8QTVV"
   },
   "source": [
    "## Capture Video on the WebCam and Define Our Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Nc_57yeQRGx"
   },
   "outputs": [],
   "source": [
    "# capture video on the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# get the dimensions on the frame\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "\n",
    "# define codec and create our VideoWriter to save the video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('sign-language.mp4', fourcc, 12, (frame_width, frame_height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1-PoD1XQc6B"
   },
   "source": [
    "Here, we define a `VideoCapture` that will record our video. The parameter 0 means capture on the first WebCam it finds. If you have multiple WebCams, this is the parameter you want to play around with until you find the correct one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWqnRtuRRA6Y"
   },
   "source": [
    "Next, we get the dimensions of the frame being recorded by the VideoCapture. We are going to use this dimensions when writing (outputting) the recorded video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZsO6CzziRQpN"
   },
   "source": [
    "Finally, we create a `VideoWriter` that we are going to use to output the video and write it to our Hard Disk. To do that, opencv requires us to define a codec to use, and so we create a `VideoWriter_fourcc` exactly for that purpose and we use 'mp4v' with it.\n",
    "\n",
    "In our writer, we first pass the name we want for the output file, here I use 'sign-language.mp4' which will be written in the current directory. You can change this location if you wish to. Next we pass in the codec. After that you pass in your fps (frame rate per second). I found that 12 worked best with my configuration but you probably want to play around with that until you get the best one for you. Finally, we pass in the frame sizes, which we had gotten earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RONDquVpTHQh"
   },
   "source": [
    "## The Main Video Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iPsiAxJjQxkT"
   },
   "outputs": [],
   "source": [
    "# read video\n",
    "while True:\n",
    "    # capture each frame of the video\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # flip frame to feel more 'natural' to webcam\n",
    "    frame = cv2.flip(frame, flipCode = 1)\n",
    "\n",
    "    # draw a blue rectangle where to place hand\n",
    "    cv2.rectangle(frame, (50, 50), (324, 324), (255, 0, 0), 2)\n",
    "\n",
    "    # get the image\n",
    "    inference_image = hand_area(frame)\n",
    "\n",
    "    # get the current prediction on the hand\n",
    "    pred = inf_model.predict(inference_image)\n",
    "    # append the current prediction to our rolling predictions\n",
    "    rolling_predictions.append(pred[0])\n",
    "\n",
    "    # our prediction is going to be the most common letter\n",
    "    # in our rolling predictions\n",
    "    prediction_output = f'The predicted letter is {most_common(rolling_predictions)}'\n",
    "\n",
    "    # show predicted text\n",
    "    cv2.putText(frame, prediction_output, (10, 350), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "    # show the frame\n",
    "    cv2.imshow('frame', frame)\n",
    "    # save the frames to out file\n",
    "    out.write(frame)\n",
    "\n",
    "\n",
    "    # press `q` to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqLNFzrnTOIB"
   },
   "source": [
    "This is a long piece of code so lets break it down bit by bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6il2O-KTUXQ"
   },
   "outputs": [],
   "source": [
    "# read video\n",
    "while True:\n",
    "    # capture each frame of the video\n",
    "    _ , frame = cap.read()\n",
    "\n",
    "    # flip frame to feel more 'natural' to webcam\n",
    "    frame = cv2.flip(frame, flipCode = 1)\n",
    "    \n",
    "\n",
    "\n",
    "    # ......\n",
    "    # truncated code here\n",
    "    # ......\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # show the frame\n",
    "    cv2.imshow('frame', frame)\n",
    "    # save the frames to out file\n",
    "    out.write(frame)\n",
    "\n",
    "\n",
    "    # press `q` to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CL9RLI82Tmy9"
   },
   "source": [
    "We create a infinite While loop that will always be running, until the user presses the 'q' letter on the keyboard, as defined by our last if statement at the very bottom of the loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSOZkAuST6vz"
   },
   "source": [
    "After that, we use the reader we created earlier and call `cap.read()` on it which returns the current frame of the video, and another variable that we are not going to use.\n",
    "\n",
    "A little intuition how videos works. A frame is somewhat equivalent to just one static image. Think of it as that. So for a video what usually happens it these single frames are played one after the other quickly, like 30-60 times faster hence creating the illusion of a continious video.\n",
    "\n",
    "So for our App, we are going to get each frame, and run it through our model (which expects the input to be an image, so this will work) and return the current prediction. This is also why we decided to use rolling average predictions and not the just the current prediction. To reduce the flickering that may occur by passing a different frame each second.\n",
    "\n",
    "Next:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "    frame = cv2.flip(frame, flipCode = 1)\n",
    "```\n",
    "\n",
    "This flips our frame to make it feel more natural. What I mean is, without flipping, the output image felt reversed, where if I raised my left arm it seemed like I was raising my right. Try running the App with this part commented out and you'll get what I mean.\n",
    "\n",
    "This shows the frames one after the other and the out writes it to disk\n",
    "\n",
    "```\n",
    "    cv2.imshow('frame', frame)\n",
    "    # save the frames to out file\n",
    "    out.write(frame)\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uyKxn_kTVuAS"
   },
   "outputs": [],
   "source": [
    "\n",
    "# read video\n",
    "while True:\n",
    "    # ......\n",
    "    # truncated code here\n",
    "    # ......\n",
    "\n",
    "    # draw a blue rectangle where to place hand\n",
    "    cv2.rectangle(frame, (50, 50), (324, 324), (255, 0, 0), 2)\n",
    "\n",
    "    # get the image\n",
    "    inference_image = hand_area(frame)\n",
    "\n",
    "    # get the current prediction on the hand\n",
    "    pred = inf_model.predict(inference_image)\n",
    "    # append the current prediction to our rolling predictions\n",
    "    rolling_predictions.append(pred[0])\n",
    "\n",
    "    # our prediction is going to be the most common letter\n",
    "    # in our rolling predictions\n",
    "    prediction_output = f'The predicted letter is {most_common(rolling_predictions)}'\n",
    "\n",
    "    # show predicted text\n",
    "    cv2.putText(frame, prediction_output, (10, 350), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "\n",
    "\n",
    "    # ......\n",
    "    # truncated code here\n",
    "    # ......\n",
    "\n",
    "\n",
    "    # press `q` to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGGvMlAdWFIB"
   },
   "source": [
    "Next, we draw a blue rectangle where the user should place the hand. The first parameter is where we want to draw the rectangle and we tell opencv to draw it on our current frame. The next two parameter describe the area where we want our rectangle to be. Note that this dimensions are exactly the same as those in the `hand_area` function we created earlier. This is to make sure we are running inference on the correct area. Lastly we pass in the color of the rectangle (in BGR formart) and the thickness of the line (2).\n",
    "\n",
    "```\n",
    "cv2.rectangle(frame, (50, 50), (324, 324), (255, 0, 0), 2)\n",
    "```\n",
    "\n",
    "Next, from our whole frame, we just extract the hand area and store it. This is the image we are going to pass to our model\n",
    "\n",
    "```\n",
    "inference_image = hand_area(frame)\n",
    "```\n",
    "\n",
    "Next, we pass our extracted image to our inference model and get the predictions and append that prediction to our rolling predictions deque. Remember that this deque will only hold the most recent 10 predictions and discard everything else\n",
    "\n",
    "```\n",
    "pred = inf_model.predict(inference_image)\n",
    "\n",
    "rolling_predictions.append(pred[0])\n",
    "```\n",
    "\n",
    "We get the most common Letter predicted in our Deque and use opencv to write that letter to the video. The parameters are almost similar to the rectangle code, with a slight variation since here we have to pass in the font(hershey simplex) and font size (0.9)\n",
    "\n",
    "```\n",
    "prediction_output = f'The predicted letter is {most_common(rolling_predictions)}'\n",
    "\n",
    "cv2.putText(frame, prediction_output, (10, 350), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7p9F5QiZRcy"
   },
   "source": [
    "The final part of the code just releases the resources we had acquired initially: the Video reader, the Video Writer and then destroys all windows created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o2RomBJmZOp8"
   },
   "outputs": [],
   "source": [
    "# release VideoCapture()\n",
    "cap.release()\n",
    "# release out file\n",
    "out.release()\n",
    "# close all frames and video windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3EQnt0nZghG"
   },
   "source": [
    "And that's all in this project. Hope you enjoyed it.\n",
    "\n",
    "In future, I am going to look for ways to improve this system and how to actually make it useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Sign Language Classification - Part B.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
