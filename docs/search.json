[
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "notes",
    "section": "",
    "text": "my notes from podcasts, YouTube videos, talks, online MOOCs, etc.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/sign-language/sign-language-classification.html",
    "href": "posts/sign-language/sign-language-classification.html",
    "title": "sign language classification",
    "section": "",
    "text": "Introduction\nI am going to attempt to use Deep Learning to create a model that can learn the American Sign Language. For this part, we will focus on model training and for the second part, we are going to create an application from our model that we get here.\nWe are going to utilize Transfer Learning for this project, which is an important part of Deep Learning.\nWhile I do not claim that this will be the best application out there for this particular problem, this small project could serve as motivation and can be expanded in future to create products that help the affected people who must use sign language to communicate.\n\n\nImporting Packages\nWe are going to be using fastai so let’s import it:\n\nfrom fastai.vision.all import *\n\n\n\nThe Data\nThe dataset we are going to be using is American Sign Language Dataset from Kaggle. It contains 87,000 images each of 200x200 pixels. It has 29 classes: 26 for the letters A-Z and 3 classes for space, delete and nothing. We are going to use the Kaggle API to get the data.\n\n!kaggle datasets download -d grassknoted/asl-alphabet\n\nDownloading asl-alphabet.zip to /content\n100% 1.03G/1.03G [00:05<00:00, 222MB/s]\n\n\n\nLet’s unzip the data and get rid of the zip file:\n\n!unzip *zip -d data && rm -rf *zip\n\nWe create a Pathlib object pointing to our data folder and look inside to see what it contains:\n\npath = Path('data')\nPath.BASE_PATH = path\n\n\npath.ls()\n\n(#2) [Path('asl_alphabet_test'),Path('asl_alphabet_train')]\n\n\nLet’s peek into one of those folders:\n\n(path/'asl_alphabet_train').ls()\n\n(#29) [Path('asl_alphabet_train/X'),Path('asl_alphabet_train/G'),Path('asl_alphabet_train/V'),Path('asl_alphabet_train/I'),Path('asl_alphabet_train/space'),Path('asl_alphabet_train/N'),Path('asl_alphabet_train/W'),Path('asl_alphabet_train/P'),Path('asl_alphabet_train/H'),Path('asl_alphabet_train/Z')...]\n\n\nWe have 29 folders, as explained earlier.\n\n\nData Preprocessing\nNow we are ready to create a DataBlock blueprint to hold our data. We use the fastai DataBlock API which is a convenient way to define how to handle our data.\nSince we don’t have validation data provided, we will split 20% of the training images and use it as our validation data.\nWe then create a DataLoaders object from the DataBlock, we will use a batch-size of 64.\n\nsigns = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    get_y=parent_label,\n    splitter=RandomSplitter(seed=42, valid_pct=0.2),\n    item_tfms=Resize(200),\n    batch_tfms=aug_transforms()\n)\n\ndls = signs.dataloaders(path/'asl_alphabet_train', bs=64)\n\nLet’s look into one batch of the data:\n\ndls.show_batch()\n\n\n\n\nThis is the number of steps we are going to take in an epoch:\n\nlen(dls.train)\n\n1087\n\n\n\n\nUsing Transfer Learning to Create A Model\nNow we can create a model and use Transfer Learning to train it on our data. Transfer Learning is important since it enables us to get good results with less training and data.\nFor those who wish to replicate this experiment, we use: resnet18 architecture, Cross Entropy Loss since this is a Classification Task, and for our optimizer, we select the Adam Optimizer. We will output error rate and accuracy as our metrics to help as analyze how our model is doing.\nWe use the Learning Rate Finder provided by fastai, using insights from Leslie Smith’s work, that enable us to find us a good learning rate, in short time instead of us trying a couple of learning rates experimentally and seeing what works.\nIf you are interested in reading more about the Learning Rate Finder, read this paper.\nFor our tast, it looks like a learning rate of 1x10-2 will work, so we fine-tine (transfer learn) for 4 epochs.\n\nlearn = cnn_learner(dls, resnet18, loss_func=CrossEntropyLossFlat(),\n                    metrics=[error_rate, accuracy], opt_func=Adam)\n\nlearn.lr_find()\n\nDownloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n\n\n\n\n\n\n\n\n\n\n\nSuggestedLRs(lr_min=0.017378008365631102, lr_steep=0.015848932787775993)\n\n\n\n\n\n\nlearn = cnn_learner(dls, resnet18, loss_func=CrossEntropyLossFlat(),\n                    metrics=[error_rate, accuracy], opt_func=Adam)\n\nlearn.fine_tune(4, base_lr=1e-2)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.230984\n      0.065030\n      0.019310\n      0.980690\n      04:11\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.160366\n      0.875754\n      0.117989\n      0.882011\n      05:24\n    \n    \n      1\n      0.038482\n      0.008822\n      0.002529\n      0.997471\n      05:26\n    \n    \n      2\n      0.008333\n      0.000855\n      0.000230\n      0.999770\n      05:25\n    \n    \n      3\n      0.002372\n      0.000346\n      0.000057\n      0.999943\n      05:25\n    \n  \n\n\n\nWe get a very good accuracy only after 4 epochs.\nNow we can tackle the small test set that comes with this dataset, although we will scale up to a better test dataset in a few moments.\n\n\nTesting Our Model\n\ntest_images = (path/'asl_alphabet_test').ls()\ntest_images\n\n(#28) [Path('asl_alphabet_test/U_test.jpg'),Path('asl_alphabet_test/space_test.jpg'),Path('asl_alphabet_test/N_test.jpg'),Path('asl_alphabet_test/R_test.jpg'),Path('asl_alphabet_test/H_test.jpg'),Path('asl_alphabet_test/P_test.jpg'),Path('asl_alphabet_test/T_test.jpg'),Path('asl_alphabet_test/C_test.jpg'),Path('asl_alphabet_test/X_test.jpg'),Path('asl_alphabet_test/V_test.jpg')...]\n\n\nWe have 28 images in this test set, each for the classes of data we have.\nLet’s take the first two images and predict them using our model.\n\nU = (path/'asl_alphabet_test'/'U_test.jpg')\nspace = (path/'asl_alphabet_test'/'space_test.jpg')\n\n\nlearn.predict(U)[0]\n\n\n\n\n'U'\n\n\n\nlearn.predict(space)[0]\n\n\n\n\n'space'\n\n\nThat looks like its working well. To predict on all the images in the test set, we are going to need a way to get the labels of the images, so as to compare with our prediction.\nLet’s work with one image first:\n\nu_test = test_images[0]\nu_test\n\nPath('asl_alphabet_test/U_test.jpg')\n\n\nAs you can see, the label of the test images is contained in the filename. So we are going to use regular expressions to extract the label from the filenames.\nHere is a simple regular expression that does the job:\n\nre.findall('(.+)_test.jpg$', u_test.name)[0]\n\n'U'\n\n\nAnd our prediction on that image:\n\nlearn.predict(u_test)[0]\n\n\n\n\n'U'\n\n\nAnd now, a way to compare our prediction, to the true label of the test set:\n\nre.findall('(.+)_test.jpg$', u_test.name)[0] == learn.predict(u_test)[0]\n\n\n\n\nTrue\n\n\nLet us write a function that is going to extract the labels, and store them in a list:\n\ndef get_test_names(images):\n  labels = []\n\n  for i in images:\n    label = re.findall('(.+)_test.jpg$', i.name)[0]\n    labels.append(label)\n  \n  return labels\n\nWe can now get all the labels for the 28 images in our test set:\n\ntest_labels = get_test_names(test_images)\n\n\nlen(test_labels)\n\n28\n\n\nNow we need a function to run inference on the images. It is going to take in the images, our model and the labels we just got as parameters and output the mean accuracy of our predictions:\n\ndef run_inference(images, model, labels):\n  corrects = []\n  # get the number of images to inference\n  num_images = len(images)\n\n  for i in range(num_images):\n    # get the inference for an image\n    prediction = model.predict(images[i])[0]\n\n    # compare with the label for that image\n    is_equal = (prediction==labels[i])\n\n    # append result to the list\n    corrects.append(is_equal)\n  \n  # convert the list of inferences to float Tensor\n  corrects = torch.Tensor(corrects).float()\n  \n  # return the mean accuracy\n  return corrects.mean().item()\n\nWe can use that function to get the mean accuracy of our model on the small test dataset.\n\ntest_accuracy = run_inference(test_images, learn, test_labels)\n\ntest_accuracy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.0\n\n\nWe get 100% accuracy. That’s fishy, you should always be worried when your model achieves a high accuracy like this. There could be data leaking. To add to that, this is a small dataset.\nLuckily, there is another dataset recommended to be used as a test set for this dataset. It contains 870 images, 30 images for each category.\nLet us use the Kaggle API again to get this new dataset:\n\n!kaggle datasets download -d danrasband/asl-alphabet-test\n\nDownloading asl-alphabet-test.zip to /content\n 70% 17.0M/24.3M [00:00<00:00, 24.5MB/s]\n100% 24.3M/24.3M [00:00<00:00, 33.3MB/s]\n\n\nAnd unzip it to a test folder:\n\n!unzip *zip -d test && rm -rf *zip\n\n\ntest_path = Path('test')\ntest_path.ls()\n\n(#29) [Path('test/X'),Path('test/G'),Path('test/V'),Path('test/I'),Path('test/space'),Path('test/N'),Path('test/W'),Path('test/P'),Path('test/H'),Path('test/Z')...]\n\n\nWe use the get_image_files to recursively get images from the newly-created test path. We get 870 images, so that seems to be working fine.\n\ntest_files = get_image_files(test_path)\n\nlen(test_files)\n\n870\n\n\nTo run inference, we are required to perform the same data preprocessing we perfomed on the training images. To make this easier, fastai suggest we use a test_dl that is created using the following syntax:\n\ntest_dl = learn.dls.test_dl(test_files, with_label=True)\n\ntest_dl.show_batch()\n\n\n\n\nWe can now get the predicitons on all the test images easily using the get_preds function and store it in a test_preds variable.\n\ntest_preds = learn.get_preds(dl=test_dl)  \n\n\n\n\nThis are our current vocabs of our data:\n\nlearn.dls.vocab\n\n['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n\n\nTo extract the true labels from the test images, we are again going to turn to regular expressions. But this time, we are required to write a regular expression robust enough to handle this three cases that represent how the rest of the images are named:\n\nexample_name = test_dl.items[370].name\nexample_name\n\n'del0002_test.jpg'\n\n\n\nre.findall(r'([A-Za-z]+)\\d+_test.jpg$', example_name)[0] \n\n'del'\n\n\n\nexample_name_2 = test_dl.items[690].name\nexample_name_2\n\n'nothing0013_test.jpg'\n\n\n\nre.findall(r'([A-Za-z]+)\\d+_test.jpg$', example_name_2)[0]\n\n'nothing'\n\n\n\nexample_name_3 = test_dl.items[0].name\nexample_name_3\n\n'X0023_test.jpg'\n\n\n\nre.findall(r'([A-Za-z]+)\\d+_test.jpg$', example_name_3)[0]\n\n'X'\n\n\nNow that we have that robust expression, we can proceed to check the accuracy of our prediction that we calculated:\nWe create a list to hold the result of our comparisons, from the predictions and the true labels, which we are going to use to calculate the final accuracy.\nWe also create a category_corrects dictionary, to tally for us, for each category, how many we predicted correct, so that we can see how our model performs on each category individually.\n\n# create a list to hold True or False when comparing\ncorrects = []\n\n# count how many predictions we get correct per category\ncategory_corrects = dict.fromkeys(learn.dls.vocab, 0)\n\n# for each enumerated predictions\nfor index, item in enumerate(test_preds[0]):\n  # get the predicted vocab\n  prediction = learn.dls.categorize.decode(np.argmax(item))\n  # get the confidence of the prediction\n  confidence = max(item) \n  confidence_percent = float(confidence)\n  # get the true label for the image we are predicting\n  image_name = test_dl.items[index].name\n  label = re.findall(r'([A-Za-z]+)\\d+_test.jpg$', image_name)[0]\n  # get the comparison and append it to our corrects list\n  is_correct = (prediction==label)\n  corrects.append(is_correct)\n\n  # if we got the prediction correct for that category,\n  # increase the count by one\n  if is_correct:\n    category_corrects[prediction] += 1\n\n\n# convert the list of inferences to float Tensor\ncorrects = torch.Tensor(corrects).float()\n\n# print the mean accuracy\nprint(f'Accuracy on the test set: {corrects.mean().item():.4f}')\n\nAccuracy on the test set: 0.6195\n\n\nAs you can see, using this better test set, we can see that the accuracy reduces.\nSince this is out of domain data, my intuition is that a better dataset that varies more could be collected and used in future. However for now, let’s work with these results.\nRemember, the test set is used as a final measure, and we shouldn’t use it to improve our model\nLet us check on the per-category prediction:\n\ncategory_corrects\n\n{'A': 15,\n 'B': 30,\n 'C': 17,\n 'D': 25,\n 'E': 19,\n 'F': 13,\n 'G': 13,\n 'H': 30,\n 'I': 29,\n 'J': 22,\n 'K': 21,\n 'L': 29,\n 'M': 22,\n 'N': 14,\n 'O': 19,\n 'P': 30,\n 'Q': 29,\n 'R': 17,\n 'S': 7,\n 'T': 0,\n 'U': 11,\n 'V': 0,\n 'W': 27,\n 'X': 4,\n 'Y': 28,\n 'Z': 11,\n 'del': 23,\n 'nothing': 14,\n 'space': 20}\n\n\nA plot would be better to analyze the information:\n\nfig, ax = plt.subplots(figsize=(20, 10))\nax.bar(*zip(*category_corrects.items()))\nplt.show()\n\n\n\n\nOur model performs really poorly on the letters T, V, and X!! We will see if this is going to be a problem in our Application that we create in part 2.\n\n\nMaking our Inference Model More Robust\nSince I plan on using this model to create a Computer Vision Model, I deciced to retrain my model, adding the new dataset in order to make it more robust, since that data varied more.\nNOTE: This is not how it should be done, you should never use your test set to train the model. I only combined the two datasets into one in order to get a better model since good Sign Language Data is hard to come by and the test set we used isn’t the official test set, just a recommended dataset that could have been used for training a different model. Also, I didn’t use my new model to get a better prediction on the test set.\n\nsigns = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    get_y=parent_label,\n    splitter=RandomSplitter(seed=42),\n    item_tfms=Resize(200),\n    batch_tfms=aug_transforms()\n)\n\ndls = signs.dataloaders(path/'asl_alphabet_train', bs=64)\n\n\nlen(dls.train)\n\n1098\n\n\n\nlearn = cnn_learner(dls, resnet18, loss_func=CrossEntropyLossFlat(),\n                    metrics=[error_rate, accuracy], opt_func=Adam)\n\nlearn.fine_tune(4, base_lr=1e-2)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.248037\n      0.073583\n      0.020769\n      0.979231\n      04:13\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.121224\n      0.061025\n      0.016502\n      0.983498\n      05:27\n    \n    \n      1\n      0.036989\n      0.008055\n      0.001878\n      0.998122\n      05:27\n    \n    \n      2\n      0.007163\n      0.003641\n      0.001081\n      0.998919\n      05:27\n    \n    \n      3\n      0.003785\n      0.002509\n      0.000569\n      0.999431\n      05:27\n    \n  \n\n\n\nI can now export my model, which I will use in the next Part, to create a Computer Vision Model to predict new data. Stay tuned!\n\nlearn.export('sign_language.pkl')\n\nHere is the link to the second part"
  },
  {
    "objectID": "posts/sign-language/sign-language-inference.html",
    "href": "posts/sign-language/sign-language-inference.html",
    "title": "sign language inference",
    "section": "",
    "text": "After training our model in Part A, we are now going to develop an application to run inference with for new data.\nI am going to be utilizing opencv to get live video from my webcam, then run our model against each frame in the video and get the prediction of what Sign Language Letter I am holding up.\nHere is an example of what the output will look like:\n\nThe whole code + training notebooks from Part A can be found in this github repo.\nThis tutorial assumes some basic understanding of the cv2 library and general understanding of how to run inference using a model."
  },
  {
    "objectID": "posts/sign-language/sign-language-inference.html#imports",
    "href": "posts/sign-language/sign-language-inference.html#imports",
    "title": "sign language inference",
    "section": "Imports",
    "text": "Imports\nInstall fastai and opencv-python.\nNext, this are the packages I utilize for this App. fastai is going to be used to run Inference with, cv2 is going to handle all the WebCam functionality and we are going to utilize deque and Counter from collections to apply a nifty trick I am going to show you.\n\nfrom collections import deque, Counter\n\nimport cv2\nfrom fastai.vision.all import *"
  },
  {
    "objectID": "posts/sign-language/sign-language-inference.html#loading-our-inference-model",
    "href": "posts/sign-language/sign-language-inference.html#loading-our-inference-model",
    "title": "sign language inference",
    "section": "Loading our Inference Model",
    "text": "Loading our Inference Model\n\nprint('Loading our Inference model...')\n# load our inference model\ninf_model = load_learner('model/sign_language.pkl')\nprint('Model Loaded')\n\nThe next part of our code loads the model we pickled in Part A and prints some useful information."
  },
  {
    "objectID": "posts/sign-language/sign-language-inference.html#rolling-average-predictions",
    "href": "posts/sign-language/sign-language-inference.html#rolling-average-predictions",
    "title": "sign language inference",
    "section": "Rolling Average Predictions",
    "text": "Rolling Average Predictions\nWhen I first made the App, I noticed one problem when using it. A slight movement of my hand was changing the predictions. This is known as flickering. The video below shows how flickering affects our App:\n{{< video https://youtu.be/aPAG39MjN68>}}\nThe Video you saw in the beginning shows how ‘stable’ our model is after using rolling predictions.\n\n# define a deque to get rolling average of predictions\n# I go with the last 10 predictions\nrolling_predictions = deque([], maxlen=10)\n\n# get the most common item in the deque\ndef most_common(D):\n    data = Counter(D)\n    return data.most_common(1)[0][0]\n\nTo solve this, a utilized the deque from Collections. I used 10 as the maxlength of the deque since I wanted the App, when running inference, to output the most common prediction out of the last 10 predictions. This makes it more stable than when we are using only the current one.\nThe function most_common will return the most common item in our deque."
  },
  {
    "objectID": "posts/sign-language/sign-language-inference.html#hand-area",
    "href": "posts/sign-language/sign-language-inference.html#hand-area",
    "title": "sign language inference",
    "section": "Hand Area",
    "text": "Hand Area\n\ndef hand_area(img):\n    # specify where hand should go\n    hand = img[50:324, 50:324]\n    # the images in the model were trainind on 200x200 pixels\n    hand = cv2.resize(hand, (200,200))\n    return hand\n\nNext, we define a function that tells our model which part of the video to run inference on. We do not want to run inference on the whole video which will include our face! We will eventually draw a blue rectangle in this area so that you’ll know where to place your hand."
  },
  {
    "objectID": "posts/sign-language/sign-language-inference.html#capture-video-on-the-webcam-and-define-our-writer",
    "href": "posts/sign-language/sign-language-inference.html#capture-video-on-the-webcam-and-define-our-writer",
    "title": "sign language inference",
    "section": "Capture Video on the WebCam and Define Our Writer",
    "text": "Capture Video on the WebCam and Define Our Writer\n\n# capture video on the webcam\ncap = cv2.VideoCapture(0)\n\n# get the dimensions on the frame\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\n\n# define codec and create our VideoWriter to save the video\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter('sign-language.mp4', fourcc, 12, (frame_width, frame_height))\n\nHere, we define a VideoCapture that will record our video. The parameter 0 means capture on the first WebCam it finds. If you have multiple WebCams, this is the parameter you want to play around with until you find the correct one.\nNext, we get the dimensions of the frame being recorded by the VideoCapture. We are going to use this dimensions when writing (outputting) the recorded video\nFinally, we create a VideoWriter that we are going to use to output the video and write it to our Hard Disk. To do that, opencv requires us to define a codec to use, and so we create a VideoWriter_fourcc exactly for that purpose and we use ‘mp4v’ with it.\nIn our writer, we first pass the name we want for the output file, here I use ‘sign-language.mp4’ which will be written in the current directory. You can change this location if you wish to. Next we pass in the codec. After that you pass in your fps (frame rate per second). I found that 12 worked best with my configuration but you probably want to play around with that until you get the best one for you. Finally, we pass in the frame sizes, which we had gotten earlier."
  },
  {
    "objectID": "posts/sign-language/sign-language-inference.html#the-main-video-loop",
    "href": "posts/sign-language/sign-language-inference.html#the-main-video-loop",
    "title": "sign language inference",
    "section": "The Main Video Loop",
    "text": "The Main Video Loop\n\n# read video\nwhile True:\n    # capture each frame of the video\n    ret, frame = cap.read()\n\n    # flip frame to feel more 'natural' to webcam\n    frame = cv2.flip(frame, flipCode = 1)\n\n    # draw a blue rectangle where to place hand\n    cv2.rectangle(frame, (50, 50), (324, 324), (255, 0, 0), 2)\n\n    # get the image\n    inference_image = hand_area(frame)\n\n    # get the current prediction on the hand\n    pred = inf_model.predict(inference_image)\n    # append the current prediction to our rolling predictions\n    rolling_predictions.append(pred[0])\n\n    # our prediction is going to be the most common letter\n    # in our rolling predictions\n    prediction_output = f'The predicted letter is {most_common(rolling_predictions)}'\n\n    # show predicted text\n    cv2.putText(frame, prediction_output, (10, 350), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n    # show the frame\n    cv2.imshow('frame', frame)\n    # save the frames to out file\n    out.write(frame)\n\n\n    # press `q` to exit\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\nThis is a long piece of code so lets break it down bit by bit:\n\n# read video\nwhile True:\n    # capture each frame of the video\n    _ , frame = cap.read()\n\n    # flip frame to feel more 'natural' to webcam\n    frame = cv2.flip(frame, flipCode = 1)\n    \n\n\n    # ......\n    # truncated code here\n    # ......\n\n\n\n    \n    # show the frame\n    cv2.imshow('frame', frame)\n    # save the frames to out file\n    out.write(frame)\n\n\n    # press `q` to exit\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\nWe create a infinite While loop that will always be running, until the user presses the ‘q’ letter on the keyboard, as defined by our last if statement at the very bottom of the loop.\nAfter that, we use the reader we created earlier and call cap.read() on it which returns the current frame of the video, and another variable that we are not going to use.\nA little intuition how videos works. A frame is somewhat equivalent to just one static image. Think of it as that. So for a video what usually happens it these single frames are played one after the other quickly, like 30-60 times faster hence creating the illusion of a continious video.\nSo for our App, we are going to get each frame, and run it through our model (which expects the input to be an image, so this will work) and return the current prediction. This is also why we decided to use rolling average predictions and not the just the current prediction. To reduce the flickering that may occur by passing a different frame each second.\nNext:\n    frame = cv2.flip(frame, flipCode = 1)\nThis flips our frame to make it feel more natural. What I mean is, without flipping, the output image felt reversed, where if I raised my left arm it seemed like I was raising my right. Try running the App with this part commented out and you’ll get what I mean.\nThis shows the frames one after the other and the out writes it to disk\n    cv2.imshow('frame', frame)\n    # save the frames to out file\n    out.write(frame)\n\n\n# read video\nwhile True:\n    # ......\n    # truncated code here\n    # ......\n\n    # draw a blue rectangle where to place hand\n    cv2.rectangle(frame, (50, 50), (324, 324), (255, 0, 0), 2)\n\n    # get the image\n    inference_image = hand_area(frame)\n\n    # get the current prediction on the hand\n    pred = inf_model.predict(inference_image)\n    # append the current prediction to our rolling predictions\n    rolling_predictions.append(pred[0])\n\n    # our prediction is going to be the most common letter\n    # in our rolling predictions\n    prediction_output = f'The predicted letter is {most_common(rolling_predictions)}'\n\n    # show predicted text\n    cv2.putText(frame, prediction_output, (10, 350), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n\n\n    # ......\n    # truncated code here\n    # ......\n\n\n    # press `q` to exit\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\nNext, we draw a blue rectangle where the user should place the hand. The first parameter is where we want to draw the rectangle and we tell opencv to draw it on our current frame. The next two parameter describe the area where we want our rectangle to be. Note that this dimensions are exactly the same as those in the hand_area function we created earlier. This is to make sure we are running inference on the correct area. Lastly we pass in the color of the rectangle (in BGR formart) and the thickness of the line (2).\ncv2.rectangle(frame, (50, 50), (324, 324), (255, 0, 0), 2)\nNext, from our whole frame, we just extract the hand area and store it. This is the image we are going to pass to our model\ninference_image = hand_area(frame)\nNext, we pass our extracted image to our inference model and get the predictions and append that prediction to our rolling predictions deque. Remember that this deque will only hold the most recent 10 predictions and discard everything else\npred = inf_model.predict(inference_image)\n\nrolling_predictions.append(pred[0])\nWe get the most common Letter predicted in our Deque and use opencv to write that letter to the video. The parameters are almost similar to the rectangle code, with a slight variation since here we have to pass in the font(hershey simplex) and font size (0.9)\nprediction_output = f'The predicted letter is {most_common(rolling_predictions)}'\n\ncv2.putText(frame, prediction_output, (10, 350), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\nThe final part of the code just releases the resources we had acquired initially: the Video reader, the Video Writer and then destroys all windows created.\n\n# release VideoCapture()\ncap.release()\n# release out file\nout.release()\n# close all frames and video windows\ncv2.destroyAllWindows()\n\nAnd that’s all in this project. Hope you enjoyed it.\nIn future, I am going to look for ways to improve this system and how to actually make it useful."
  },
  {
    "objectID": "posts/pneumonia-classification/handling-imbalance-medical-datasets.html",
    "href": "posts/pneumonia-classification/handling-imbalance-medical-datasets.html",
    "title": "handling imbalance in medical datasets",
    "section": "",
    "text": "Introduction\nRecently, I have been widely interested in the overlap between Deep Learning and Biology and decided to start learning about it. I came across an interesting challenge, where I try to build a Pneumonia Binary Classification Computer Vision model that predicts whether a chest X-ray has Pneumonia or not. I also learned a nifty approach to deal with a problem that is common in Medical Datasets, that I will show you here.\nI am going to be using fastai and PyTorch for this tutorial. I want to extend my thanks to the author of this dataset from Kaggle that we are going to be using today.\nLet’s get the packages that we will need:\n\nfrom fastai.vision.all import *\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nHow does our data look like?\n\npath = Path('data/chest_xray')\n\n\npath.ls()\n\n(#5) [Path('__MACOSX'),Path('chest_xray'),Path('val'),Path('train'),Path('test')]\n\n\nIt is already separated for us in the relevant folders. Awesome! Let’s check inside one of the folders:\n\n(path/'train').ls()\n\n(#2) [Path('train/PNEUMONIA'),Path('train/NORMAL')]\n\n\nThe folders are also separated into their respective classes. How many images do we have per category?\n\ntrain = get_image_files(path/'train')\nval = get_image_files(path/'val')\ntest = get_image_files(path/'test')\n\nprint(f\"Train: {len(train)}, Valid: {len(val)}, Test: {len(test)}\")\n\nTrain: 5216, Valid: 16, Test: 624\n\n\nOur validation set has only 16 images! That won’t be a good measurement of how our model is performing but we will tackle that later on.\nLet us check the distribution of images between the two classes:\n\nnormal = get_image_files(path/'train'/'NORMAL')\npneumonia = get_image_files(path/'train'/'PNEUMONIA')\n\nprint(f\"Normal Images: {len(normal)}. Pneumonia Images: {len(pneumonia)}\")\n\nNormal Images: 1341. Pneumonia Images: 3875\n\n\n\ndata = [['Normal', len(normal)], ['Pneumonia', len(pneumonia)]]\ndf = pd.DataFrame(data, columns=['Class', 'Count'])\n\nsns.barplot(x=df['Class'], y=df['Count']);\n\n\n\n\nRemember the problem common to Medical Datasets I was talking about? We see that our dataset is imbalanced. Our negative class (Normal) is 3 times less than our positive class. This is a problem. How do we solve it?\nFirst, we will utilize some Data Augmentations. This is artificially growing our dataset by introducing some transforms on the images.\nSecond, we will use some lessons that I read from a wonderful paper: Mateusz Buda, Atsuto Maki, and Maciej A Mazurowski. A systematic study of the class imbalance problem in convolutional neural networks that studies the problem of class imbalances and offers a way to solve it. I highly recommend reading the paper.\nHowever, we need to build a Baseline model that we can later improve on.\nFor the Data Augmentations, we have to be careful to pick the ones that make sense for our X-Ray data. I picked Rotate and Zoom. If you think about it, transforms like flipping the image won’t be useful since our body parts are in specific locations. e.g our liver is one the right, and flipping the X-Ray would take it to the opposite side.\nI also utilize a nifty trick called Presizing from the fastai team. The basic idea behind this approach is this: We first resize the image to a bigger size, bigger than what we want for the final image. For instance, here, I resize the image to 460x460 first, then later on, resize it to 224x224 and at the same time, apply all the augmentaions at once. That is the most important point, applying the final resize and the transforms at the same time, preferably as a batch transform on the GPU. This helps in a higher quality image than insted, let’s say, applying them one by one, which may degrade the data. To learn more about presizing, check out his notebook.\n\naugs = [RandomResizedCropGPU(size=224, min_scale=0.75), Rotate(), Zoom()]\n\n\ndblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    get_y=parent_label,\n    splitter=GrandparentSplitter(train_name='train', valid_name='val'),\n    item_tfms=Resize(460),\n    batch_tfms=augs\n)\n\nLet us collect the data in a dataloaders object and show one batch.\n\ndls = dblock.dataloaders(path)\n\n\ndls.show_batch()\n\n\n\n\nWe are going to utilize transfer learning on the resnet18 architecture. Our metrics to guide us are going to be error rate and accuracy.\n\nlearn = cnn_learner(dls, resnet18, metrics=[error_rate, accuracy])\n\nDownloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n\n\n\n\n\n\n\n\nWe use another great tool by fastai that helps us get the optimal learning rate to use. Something around 1x10-2 will work okay according to the plot. (The bottom scale is Logarithmic)\n\nlearn.lr_find()\n\n\n\n\nSuggestedLRs(lr_min=0.012022644281387329, lr_steep=0.0008317637839354575)\n\n\n\n\n\nLet me explain, what happens in the next few cells. In Transfer Learning, we need to retain the knowledge learned by the pretrained model. So what happens is, we freeze all the earlier layers and chop off the last classification layer and replace it with a layer with random weights and the correct number outputs, two in this case(it is done by default in fastai when creating the learner through the cnn_learner method).\nSo, first we train the final layer (with random weights) for 3 epochs, with the one cycle training policy. Then we unfreeze the whole model, find the new suitable learning rate (because we are now updating all the weights) and train for a further 3 epochs.\n\nlearn.fit_one_cycle(3, lr_max=1e-2)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.168814\n      0.368405\n      0.187500\n      0.812500\n      03:13\n    \n    \n      1\n      0.099094\n      0.734940\n      0.312500\n      0.687500\n      03:15\n    \n    \n      2\n      0.063019\n      0.439835\n      0.187500\n      0.812500\n      03:17\n    \n  \n\n\n\n\nlearn.unfreeze()\n\n\nlearn.lr_find()\n\n\n\n\nSuggestedLRs(lr_min=6.309573450380412e-08, lr_steep=2.2908675418875646e-06)\n\n\n\n\n\nThis plot looks different from the other one, since we are now updating all the weights, not just the final random ones, and the first layers don’t need too much learning. 4x10-6 is the suggested learning rate.\nLet’s train the whole model with the new learning rate:\n\nlearn.fit_one_cycle(3, lr_max=4.4e-6)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.053950\n      0.435164\n      0.187500\n      0.812500\n      03:24\n    \n    \n      1\n      0.053677\n      0.220901\n      0.062500\n      0.937500\n      03:23\n    \n    \n      2\n      0.047155\n      0.361383\n      0.125000\n      0.875000\n      03:23\n    \n  \n\n\n\n87.5% accuracy. Not bad for a start, but we will try ways to improve it.\nLet us see how our model is doing by inspecting the Confusion Matrix.\n\ninterp = ClassificationInterpretation.from_learner(learn)\n\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\nFour ‘NORMAL’ images are being classified as ‘PNEUMONIA’. Can this be caused because our model doesn’t have enough examples of the ‘NORMAL’ class to learn about? Let us investigate.\n\n\nSolving the Imbalance Problem\nThe solution to the problem, as with many solutions to problems in Deep Learning, is simple and something that can be implemented easily. Quoting from their conclusions in the paper: * The method of addressing class imbalance that emerged as dominant in almost all analyzed scenarios was oversampling. * Oversampling should be applied to the level that completely eliminates the imbalance. * Oversampling does not cause over\f tting of convolutional neural networks.\nBasically, Oversampling is artificially making the minority class bigger by replicating it a couple of times. The paper recommends we replicate it until is completely eliminates the imbalance, therefore, our new oversampled ‘NORMAL’ class is going to be the original images, repeated three times. And we don’t have to worry about overfitting of our model too!\n\nos_normal = get_image_files(path/'train'/'NORMAL') * 3\npneumonia = get_image_files(path/'train'/'PNEUMONIA')\n\nprint(f\"Normal Images: {len(os_normal)}. Pneumonia Images: {len(pneumonia)}\")\n\nNormal Images: 4023. Pneumonia Images: 3875\n\n\n\ndata = [['Normal', len(os_normal)], ['Pneumonia', len(pneumonia)]]\nos_df = pd.DataFrame(data, columns=['Class', 'Count'])\n\nsns.barplot(x=os_df['Class'], y=os_df['Count']);\n\n\n\n\nAfter the Oversampling, the distribution between the classes is almost at per. Now our dataset it balanced and we can train a new model on this Balanced Data.\nNow we need a new way to split our dataset when loading it to a DataLoader. Our new Oversampled Path is going to be the Oversampled ‘NOMARL’ class, the original ‘PNEUMONIA’ and the validation data.\nThen we create two variables, train_idx and val_idx, that represent the indexes of the respective category of the images, whether train or validation.\n\nos_path = os_normal + pneumonia + val\n\n\ntrain_idx = [i for i, fname in enumerate(os_path) if 'train' in str(fname)]\nval_idx = [i for i, fname in enumerate(os_path) if 'val' in str(fname)]\n\n\nL(train_idx), L(val_idx)\n\n((#7898) [0,1,2,3,4,5,6,7,8,9...],\n (#16) [7898,7899,7900,7901,7902,7903,7904,7905,7906,7907...])\n\n\nNow we have 7898 images in the Train instead of the original 5216, and we still have 16 Validation images. We load them up in a dataloaders object, which our learner expects, find the new optimal learning rate and train the model:\n\ndblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    get_y=parent_label,\n    splitter=lambda x: [train_idx, val_idx],\n    item_tfms=Resize(460),\n    batch_tfms=augs\n)\n\n\ndls = dblock.dataloaders(path)\n\nlearn = cnn_learner(dls, resnet18, metrics=[error_rate, accuracy])\n\nlearn.lr_find()\n\n\n\n\nSuggestedLRs(lr_min=0.025118863582611083, lr_steep=0.0030199517495930195)\n\n\n\n\n\n\nlearn.fit_one_cycle(3, lr_max=2.5e-2)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.346110\n      0.308598\n      0.125000\n      0.875000\n      02:18\n    \n    \n      1\n      0.196921\n      0.040522\n      0.000000\n      1.000000\n      02:17\n    \n    \n      2\n      0.107927\n      0.038475\n      0.000000\n      1.000000\n      02:19\n    \n  \n\n\n\nAfter just three epochs, we get 100% accuracy on the Validation Set. The Oversampling Solution worked well for us.\nHowever, as I mentioned before, we only have 16 images on the Validation Set, so its not a good measure on how well our model generalizes.\nSo I combined the Validation and Test Set into one, and used that as my Validation Set to test how well my model generalizes.\n\nmerged_path = os_normal + pneumonia + val + test\n\ntrain_idx = [i for i, fname in enumerate(merged_path) if 'train' in str(fname)]\nval_idx = [i for i, fname in enumerate(merged_path) if 'train' not in str(fname)]\n\n\nL(train_idx), L(val_idx)\n\n((#7898) [0,1,2,3,4,5,6,7,8,9...],\n (#640) [7898,7899,7900,7901,7902,7903,7904,7905,7906,7907...])\n\n\nWe now have 640 images as our validation. How does our model perform with this new data?\n\nlearn.fit_one_cycle(5, lr_max=1e-4)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.087615\n      0.043269\n      0.014062\n      0.985937\n      02:30\n    \n    \n      1\n      0.079633\n      0.041382\n      0.015625\n      0.984375\n      02:30\n    \n    \n      2\n      0.067601\n      0.054884\n      0.018750\n      0.981250\n      02:31\n    \n    \n      3\n      0.053627\n      0.027576\n      0.006250\n      0.993750\n      02:30\n    \n    \n      4\n      0.037838\n      0.025329\n      0.004688\n      0.995313\n      02:27\n    \n  \n\n\n\n99.5% accuracy after 5 epochs looks good, looks like our model generalizes well.\nSee you next time!"
  },
  {
    "objectID": "posts/transfer-learning-beginner/transfer-learning.html",
    "href": "posts/transfer-learning-beginner/transfer-learning.html",
    "title": "training state of the art models with little data and compute using transfer learning",
    "section": "",
    "text": "Introduction\nToday, we are going to have a beginner’s look into an interesting approach in Deep Learning, called Transfer Learning. We are also going to build a simple Computer Vision model to demonstrate a working example of Transfer Learning. Our model will be a classifier that differentiates between Millipedes, Centipedes and Spiders.\nWe are going to be using fastai and PyTorch for this tutorial. More specifically, we are going to be using Google’s free service Colab that gives us free GPU to do Deep Learning.\n\n\nWhat is Transfer Learning?\nSo what really is it Transfer Learning?\nIn simple terms, Transfer Learning is the approach of transferring knowledge from one Deep Learning Model to another. In more technical terms, Transfer Learning is the approach of using an already pretained model, and adapting it to a new problem.\nThis simple approach helps developers get state of the art results with little data and little compute.\nTraining a model from scratch requires a lot of compute and a lot of data. For example the pretrained model we are going to use was trained on ImageNet database which contains 1.2 million images with 1000 categories. In practice, very few people train the enitre Network from scratch, we often leverage the knowledge gained from these pretrained models and adapt them to our specific dataset.\nTo learn more about Transfer Learning, you can use these notes.\n\n\nA Little Intuition Before We Begin.\nDeep Learning models consists of many layers inside them, each learning its own unique features. In 2013, two researches published a paper called Visualizing and Understanding Convolutional Networks that helped visualize what is going on inside the layers and what they actually learn. In the interest of keeping this post beginner friendly, I won’t go much into the technical details of the paper but, here are some images showing what the layers in the neural network learn.\nIn the first layer, the two reasearchers showed that the network learns general features like diagonal, vertical and horizontal edges. These are the building blocks.\n\n\n\nFirst layer of the CNN (courtesy of Matthew D. Zeiler and Rob Fergus)\n\n\nIn the second layer, the Network starts to learn simple patterns that are also general to any Computer Vision Data like circles, etc. \nAnd it keeps on improving layer by layer, building from the building blocks.\n\n\n\nThird layer of the CNN (courtesy of Matthew D. Zeiler and Rob Fergus)\n\n\nSo as you can see, the first layers of a Convolutional Network learn general patterns that are common to all images. This is why we don’t want to discard these knowledge because it can be used for any dataset.\nWhat actually happens in transfer learning, specifically for Computer Vision Tasks is the first layers are freezed (no learning goes on) and the final layer (the one that actually does the classification e.g dog from cat) is chopped of and replaced with a classification layer that is specific to the dataset, i.e our final layer will be trained to specifically distinguish Millipedes, Centipedes and Spiders.\nLet’s get straight into the practical example, shall we?\n\n\nTraining A Computer Vision Model Using Transfer Learning\nLet’s start by handling our imports:\n\nfrom fastai.vision.all import *\n\nFor the DataSet, we are going to scrap the internet for centipedes, millipedes and spiders. We are going to use a very handy tool, jmd_imagescraper, that uses DuckDuckGo for the image scraping and returns some nice images. The developer also provides a nice ImageCleaner that we are going to use later to clean up the dataset. Let’s import them too.\n\nfrom jmd_imagescraper.core import *\nfrom jmd_imagescraper.imagecleaner import *\n\nWe create a directory called ‘data’. We then use the image scrapper to get the images of the three classes we are interested in and save them each to their specific directories inside the ‘data’ directory, i.e. the centipedes will be stored inside a directory called ‘Centipedes’ and the millipedes will be stored inside the ‘Millipede’ directory and likewise for the spiders (This arrangement is going to prove useful later!). We download 150 images for each.\n\nroot = Path().cwd()/\"data\"\n\nduckduckgo_search(root, 'Centipede', 'centipede', max_results=150)\nduckduckgo_search(root, 'Millipede', 'millipede', max_results=150)\nduckduckgo_search(root, 'Spider', 'spider', max_results=150)\n\nLet us see how our ‘data’ directory looks after the downloading completes:\n\npath = Path('data')\n\n\npath.ls()\n\n(#3) [Path('Spider'),Path('Centipede'),Path('Millipede')]\n\n\nAs you can see, we have three directories inside, each corresponding to the images it containes. If we look inside a specific directory, e.g. Centipede, we see the individual images downloaded, and the total number of images downloaded (150) prefixed before the list:\n\n(path/'Centipede').ls()\n\n(#150) [Path('Centipede/135_dd009ed0.jpg'),Path('Centipede/084_69e0099b.jpg'),Path('Centipede/129_5409e84e.jpg'),Path('Centipede/077_cc3b3dd9.jpg'),Path('Centipede/097_cdfd1abf.jpg'),Path('Centipede/030_55b8c176.jpg'),Path('Centipede/090_ef7667e5.jpg'),Path('Centipede/028_5b5b8f46.jpg'),Path('Centipede/052_ec993151.jpg'),Path('Centipede/056_86c51270.jpg')...]\n\n\nOkay, now that we have got our images ready, we can begin the next step which is processing them. We use a handy function provided by fastai called get_image_files, which simply recursively goes through the directory and gets all the images inside them.\n\nfns = get_image_files(path)\nfns\n\n(#450) [Path('Spider/019_ffed6440.jpg'),Path('Spider/100_59bd4277.jpg'),Path('Spider/056_21ce5818.jpg'),Path('Spider/114_33c06a31.jpg'),Path('Spider/001_f7a867bc.jpg'),Path('Spider/139_3d7b9ec9.jpg'),Path('Spider/007_f8419240.jpg'),Path('Spider/113_3082658a.jpg'),Path('Spider/135_347f4e6e.jpg'),Path('Spider/144_e94c648a.jpg')...]\n\n\nWe have 450 images, which makes sense. Did any image get corrupted during downloading? Let us verify the images.\n\nfailed = verify_images(fns)\nfailed\n\n\n\n\n(#0) []\n\n\nLuckily, no image got corrupted. Good, now let’s go on.\nLet us open one and see how it looks:\n\nim = Image.open(fns[2])\nim.to_thumb(128, 128)\n\n\n\n\nSo far everything is looking good!\nSince we have gotten the images, now we can start processing them in a format that our learner expects. We are going to use the DataBlock API from fastai.\nI am going to give a brief explanation of what is going on, but I highly recommend going through their documentaion about the DataBlock API, where they explain everything in detail.\nLet us first see how the code looks like:\n\nimages = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    get_y=parent_label,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    item_tfms=RandomResizedCrop(224, min_scale=0.3),\n    batch_tfms=aug_transforms()\n)\n\nLet us go step by step:\n\nblocks=(ImageBlock, CategoryBlock)\n\nThis simply tells the dataloader the format of the data it is receiving, i.e. here, our independent variable is going to be and Image, hence the ImageBlock, and the labels or the dependent variables are going to be a category (either ‘Centipede’, ‘Millipede’, or ‘Spider’)\n\nget_items=get_image_files\n\nThis tells our dataloader how to get the items, using the get_image_files we used before\n\nget_y=parent_label\n\nThis instructs our dataloader on how to get the labels of the images, by getting the parent name of the directory the image is in (That’s why we arranged the pictures in images in their repsective directories).\n\nsplitter=RandomSplitter(valid_pct=0.2, seed=42)\n\nThis provides a way of splitting the dataset into a training and a validation set. Here we split the validation set into 20% of the total data. The seed option is there to ensure we get the same validation set every time.\n\nitem_tfms=RandomResizedCrop(224, min_scale=0.3)\n\nThis is simply a transformation done on every image individually. Here we resize the images to 224 x 224. Images should be the same size when fed into the Neural Network. We go an extra step of randomly picking a different crop of the images every time, i.e. a minimum of 30% of the total image every time. Randomly picking a different section of the image every time helps the Network generalize well to new data.\nAnd finally this,\n\nbatch_tfms=aug_transforms()\n\nperforms data augmentation on the images. Data Augmentation deserves a whole post by itself to explain, but for intuition on why we do this, let me give a brief explanation. When using our model in the real world, people will provide images in very different formats, taken from different angles, some from cameras with low pixel capturing capabilities which provides somewhat blurry images. But we still need the model to generalize well to all of these cases! Hence data augmentation. Data Augmentation transforms the images to different versions, flipping it, rotating it, darkening it and many other transforms, to help the model generalize well in the real world. We use a batch transform here that applies the transforms in batches in the GPU which is way faster.\nLet us load the images into a dataloader, which is what the learner expects, and show one batch of the images.\n\ndls = images.dataloaders(path)\n\n\ndls.show_batch(max_n=9)\n\n\n\n\nAs you can see, the images are being Randomly Resized, cropping every time to 30% of the image.\nLet us see what the augmentation transforms did to our data, by adding the unique parameter:\n\ndls.train.show_batch(max_n=8, nrows=2, unique=True)\n\n\n\n\nAs you can see, these all are the same images but transformed differently.\nNow we are ready to create the model.\n\n\nTraining The Model\nRemember all the talk of using a pretrained model? Well, here is where we apply it.\nWe are using the resnet18 pretrained model from PyTorch and fine tuning it for 5 epochs for our specific dataset. The ‘18’ suffix simply means it has 18 layers, which is going to be sufficient for our simple model. However, there are deeper models like resnet34, resnet50, resnet101 and resnet152 with the respective number of layers. Deeper models take more time to train, and often produce better results but not always! As a rule of thumb, start simple then upgrade if need be.\nWe load our dataloaders (dls) created earlier and we are going to output ‘error_rate’ and ‘accuracy’ as our metrics, to guide us on how well our model is performing.\nWe are going to use a cnn_learner which is simply a Convolutional Neural Network Learner which is the type of Neural Network widely used for Computer Visions tasks.\n\nlearn = cnn_learner(dls, resnet18, metrics=[error_rate, accuracy])\n\nlearn.fine_tune(5)\n\nDownloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n\n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.545470\n      0.389558\n      0.155556\n      0.844444\n      00:03\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.454676\n      0.267906\n      0.100000\n      0.900000\n      00:03\n    \n    \n      1\n      0.342936\n      0.232648\n      0.066667\n      0.933333\n      00:03\n    \n    \n      2\n      0.291200\n      0.193626\n      0.077778\n      0.922222\n      00:03\n    \n    \n      3\n      0.237957\n      0.190752\n      0.066667\n      0.933333\n      00:03\n    \n    \n      4\n      0.205695\n      0.206321\n      0.066667\n      0.933333\n      00:03\n    \n  \n\n\n\nAfter 5 epochs, we get an error rate of 6.7% which corresponds to an accuracy of 93.3%. That is really good considering our small dataset and the time we used to train this model, approximately 20 seconds, but as you will see, we can improve this.\nYou may be asking yourself why we didn’t clean the dataset first before training. It is good to train your model as soon as possible to provide you with a baseline which you can start improving from. And we will clean the dataset later, with the help of the training results and then retrain with a clean dataaset and see if it improves.\nLet us inspect what errors our initial model is making. The Classification Confusion Matrix can aid in displaying this in a good format.\n\ninterp = ClassificationInterpretation.from_learner(learn)\n\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\nThe dark colors on that diagonal indicate that the model is perfoming well. But it is still making mistakes, for example it classified Centipedes as Millipedes 4 times and Spiders as Centipedes twice.\nLet us see the specific images it is getting wrong and try to understand why it is confusing them by plotting the top losses of our model.\n\ninterp.plot_top_losses(5, nrows=5)\n\n\n\n\nStraight away we can see that some of the mistakes it is making is because of unclean data. For example the 2nd and 4th images have nothing to do with our data.\nThis is why we need to clean the data. As you can see, training the model first helps us with the cleaning process.\nWe are going to use the ImageCleaner provides by the jmd_imagescrapper developer.\n\ndisplay_image_cleaner(root)\n\n\n\n\nCleaner\n\n\nI did deleted a few of the images from the datasets that didn’t fit the criteria and we were left with 394 images (but useful ones!).\n\nfns = get_image_files(path)\nfns\n\n(#394) [Path('Spider/019_ffed6440.jpg'),Path('Spider/100_59bd4277.jpg'),Path('Spider/056_21ce5818.jpg'),Path('Spider/114_33c06a31.jpg'),Path('Spider/001_f7a867bc.jpg'),Path('Spider/139_3d7b9ec9.jpg'),Path('Spider/007_f8419240.jpg'),Path('Spider/113_3082658a.jpg'),Path('Spider/135_347f4e6e.jpg'),Path('Spider/144_e94c648a.jpg')...]\n\n\nOkay, now we create a new dataloader with the clean images.\n\ndls = images.dataloaders(path)\n\nWill training with only clean data help improve our model? Lets train a new model and see. We are going to use the exact details we used before, but I am fine-tuning for 10 epochs this time.\n\nlearn = cnn_learner(dls, resnet18, metrics=[error_rate, accuracy])\n\nlearn.fine_tune(10)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      2.070883\n      0.254761\n      0.076923\n      0.923077\n      00:02\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.478606\n      0.169040\n      0.038462\n      0.961538\n      00:02\n    \n    \n      1\n      0.413722\n      0.118356\n      0.038462\n      0.961538\n      00:02\n    \n    \n      2\n      0.333819\n      0.103353\n      0.038462\n      0.961538\n      00:02\n    \n    \n      3\n      0.260725\n      0.119273\n      0.025641\n      0.974359\n      00:02\n    \n    \n      4\n      0.213143\n      0.118922\n      0.025641\n      0.974359\n      00:02\n    \n    \n      5\n      0.185268\n      0.092165\n      0.025641\n      0.974359\n      00:02\n    \n    \n      6\n      0.156762\n      0.087852\n      0.012821\n      0.987179\n      00:02\n    \n    \n      7\n      0.138017\n      0.083028\n      0.025641\n      0.974359\n      00:02\n    \n    \n      8\n      0.118409\n      0.083742\n      0.025641\n      0.974359\n      00:02\n    \n    \n      9\n      0.111713\n      0.082776\n      0.025641\n      0.974359\n      00:02\n    \n  \n\n\n\nWe went upto an error rate of just 2.6% which means that our model is correct 97.4% of the time!\nAs you have seen practically, Transfer Learning is a very important technique in Deep Learning that can go a long way. We only used 394 images here and trained for approximately for 20 seconds and got a model which is pretty accurate.\nStay tuned for more."
  },
  {
    "objectID": "posts/transfer-learning-intermediate/making-transfer-learning-work.html",
    "href": "posts/transfer-learning-intermediate/making-transfer-learning-work.html",
    "title": "making transfer learning work in plain PyTorch",
    "section": "",
    "text": "Practical Deep Learning for Coders - Lesson 5\nPractical Deep Learning for Coders - Lesson 15\nTransfer Learning in fast.ai forum post\nJeremy Howard talking about Transfer Learning\nAndrew Ng’s DeepLearningAi video about Transfer Learning"
  },
  {
    "objectID": "posts/transfer-learning-intermediate/making-transfer-learning-work.html#imports",
    "href": "posts/transfer-learning-intermediate/making-transfer-learning-work.html#imports",
    "title": "making transfer learning work in plain PyTorch",
    "section": "Imports",
    "text": "Imports\n\n\nImports\nimport os\nimport random\nimport urllib\nimport urllib.request\nimport shutil\nfrom glob import glob\nfrom PIL import Image\nimport cv2\nfrom collections import defaultdict\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom tqdm import tqdm\n\nimport timm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport torch.optim as optim\nimport torchvision\nfrom torchvision.datasets import ImageFolder\n\nfrom fastcore.all import *\n\nimport wandb"
  },
  {
    "objectID": "posts/transfer-learning-intermediate/making-transfer-learning-work.html#utility-functions",
    "href": "posts/transfer-learning-intermediate/making-transfer-learning-work.html#utility-functions",
    "title": "making transfer learning work in plain PyTorch",
    "section": "Utility Functions",
    "text": "Utility Functions\nDefining useful stuff we use in this Notebook:\n\n\nSome utility functions for this notebook\n# Data Handling stuff\ndef get_data(URL, FILE, FOLDER):\n  # This is a function that downloads and extracts the data\n  # then returns a pathlib object containing the location of the data\n\n  # Downloading\n  if not os.path.isfile(FILE):\n    print(f'Downloading {URL} and saving it as {FILE}')\n    print('-'*120)\n    urllib.request.urlretrieve(URL, FILE)\n    print('Finished Downloading')\n  else:\n    print(f'{FILE} already exists')\n  \n  # Extracting\n  print('\\n')\n  print(f'Extracting Files into {FOLDER}')\n  shutil.unpack_archive(FILE, FOLDER)\n  return Path(FOLDER)\n\ndef get_loaders(bs):\n  trainloader = DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=2,\n                           pin_memory=True)\n  valoader = DataLoader(val_ds, batch_size=bs, shuffle=False, num_workers=2,\n                           pin_memory=True)\n  return trainloader, valoader\n\n\n# Training Helper Functions\ndef calculate_accuracy(preds, target):\n  correct = 0\n  total = 0\n\n  predicted = torch.argmax(preds, axis=1)\n  total += target.shape[0]\n  correct += int((predicted==target).sum())\n\n  return correct / total\n\nclass MetricMonitor:\n    def __init__(self, float_precision=3):\n        self.float_precision = float_precision\n        self.reset()\n\n    def reset(self):\n        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n\n    def update(self, metric_name, val):\n        metric = self.metrics[metric_name]\n\n        metric[\"val\"] += val\n        metric[\"count\"] += 1\n        metric[\"avg\"] = metric[\"val\"] / metric[\"count\"]\n\n    def __str__(self):\n        return \" | \".join(\n            [\n                f\"{metric_name}: {metric['avg']:.{self.float_precision}f}\"\n                for (metric_name, metric) in self.metrics.items()\n            ]\n        )\n\ndef init_weights(m):\n  if type(m) == nn.Linear:\n    nn.init.kaiming_normal_(m.weight)"
  },
  {
    "objectID": "posts/transfer-learning-intermediate/making-transfer-learning-work.html#a-quick-note-about-fastai-and-transfer-learning",
    "href": "posts/transfer-learning-intermediate/making-transfer-learning-work.html#a-quick-note-about-fastai-and-transfer-learning",
    "title": "making transfer learning work in plain PyTorch",
    "section": "A Quick Note about fastai and Transfer Learning",
    "text": "A Quick Note about fastai and Transfer Learning\nTranfer Learning is an important technique in making Deep Learning accessible to everyone. fastai is a leading research company in the area of Transfer Learning (and almost all other Deep Learning areas too!). Out of the box, you get state of the art results with carefully chosen defaults and training methods after lots of research.\n\n\n\nfastai magic.png\n\n\nThese is an example post in the forums where the user tried to match the results gotten by fastai in Tensorflow. The good thing about fastai though is that they have a completely free MOOC where they teach everything and how to get the good results. In this post, we will be taking insights from the course and try to implement them in PyTorch.\nSome of the techniques covered here are:\n\nSplitting a Pretrained Network into body and head.\nCreating a better custom head.\nNot freezing the BatchNorm Layer and the intuition why.\nFreezing the body and just training the head.\nUnfreezing the whole network and training it further to get better results.\nUsing different learning rates for the body and the head, or as it is known as, Discriminative Learning Rates.\n\nWe will be getting our pretrained models from Ross Wightman’s library which makes availabe *almost all vision models.\nWe will be running our experiments on the Imagenette dataset created by Jeremy Howard."
  },
  {
    "objectID": "posts/transfer-learning-intermediate/making-transfer-learning-work.html#configs",
    "href": "posts/transfer-learning-intermediate/making-transfer-learning-work.html#configs",
    "title": "making transfer learning work in plain PyTorch",
    "section": "Configs",
    "text": "Configs\nWe store our configurations in a dictionary for easy access and update:\n\nhead_lr=1e-5\nbody_lr = head_lr / 2.6\n\n\nconfigs = dict(\n    init_epochs=5,\n    num_epochs=10,\n    num_classes=10,\n    batch_size=64,\n    lr=3e-3,\n    head_lr=head_lr,\n    body_lr=body_lr,\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n    dataset=\"Imagenette\",\n    architecture=\"resnet34\")"
  },
  {
    "objectID": "posts/transfer-learning-intermediate/making-transfer-learning-work.html#getting-the-data",
    "href": "posts/transfer-learning-intermediate/making-transfer-learning-work.html#getting-the-data",
    "title": "making transfer learning work in plain PyTorch",
    "section": "Getting the Data",
    "text": "Getting the Data\nUsing a small utility function I created, we can get the data and return a Pathlib object:\n\nURL = 'https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz'\nFILE = 'imagenette2-320.tgz'\nFOLDER = 'data'\n\n\npath = get_data(URL, FILE, FOLDER)\npath.ls()\n\nDownloading https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz and saving it as imagenette2-320.tgz\n------------------------------------------------------------------------------------------------------------------------\nFinished Downloading\n\n\nExtracting Files into data\n\n\n(#1) [Path('data/imagenette2-320')]\n\n\n\npath.ls()\n\n(#1) [Path('data/imagenette2-320')]\n\n\nWe can recursively get all the JPEG files from our path.\n\nfiles = L(glob(f'{path}/**/*.JPEG', recursive=True)).map(Path)\nfiles[0]\n\nPath('data/imagenette2-320/train/n03394916/n03394916_15417.JPEG')\n\n\n\nfiles\n\n(#13394) [Path('data/imagenette2-320/train/n03394916/n03394916_15417.JPEG'),Path('data/imagenette2-320/train/n03394916/n03394916_41638.JPEG'),Path('data/imagenette2-320/train/n03394916/n03394916_40833.JPEG'),Path('data/imagenette2-320/train/n03394916/n03394916_23145.JPEG'),Path('data/imagenette2-320/train/n03394916/n03394916_16274.JPEG'),Path('data/imagenette2-320/train/n03394916/n03394916_34078.JPEG'),Path('data/imagenette2-320/train/n03394916/ILSVRC2012_val_00022204.JPEG'),Path('data/imagenette2-320/train/n03394916/n03394916_24385.JPEG'),Path('data/imagenette2-320/train/n03394916/n03394916_42289.JPEG'),Path('data/imagenette2-320/train/n03394916/n03394916_34364.JPEG')...]\n\n\nWe have 13394 images in total. To verify they are working correctly, we can have a look at one of the images:\n\nim = Image.open(files[2])\nim\n\n\n\n\nFor modelling purposes, we will need the names of the images, which we can get this way:\n\nfiles[0].parent.name\n\n'n03394916'\n\n\nWe can get all the unique labels from our dataset and create a dictionary converting them to numerical format which is what our network expects:\n\nlbls = files.map(Self.parent.name()).unique(); lbls\n\n(#10) ['n03394916','n03000684','n01440764','n03417042','n03425413','n02979186','n02102040','n03445777','n03028079','n03888257']\n\n\n\nv2i = lbls.val2idx(); v2i\n\n{'n01440764': 2,\n 'n02102040': 6,\n 'n02979186': 5,\n 'n03000684': 1,\n 'n03028079': 8,\n 'n03394916': 0,\n 'n03417042': 3,\n 'n03425413': 4,\n 'n03445777': 7,\n 'n03888257': 9}\n\n\nThe following snippets of code create the Dataset for our code and creates the DataLoaders using appropriate augmentations from Albumentations:\n\nclass Imagenette(Dataset):\n  def __init__(self, files, v2i, transform=None):\n    self.files = files\n    self.v2i = v2i\n    self.transform = transform\n  \n  def __len__(self):\n    return len(self.files)\n  \n  def __getitem__(self, idx):\n    image_filepath = self.files[idx]\n    image = cv2.imread(f'{image_filepath}')\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    label = self.v2i[self.files[idx].parent.name]\n\n    # transform if available\n    if self.transform is not None:\n      image = self.transform(image=image)[\"image\"]\n    \n    return image, label\n\n\ntrain_transform = A.Compose(\n    [\n        A.SmallestMaxSize(max_size=160),\n        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n        A.RandomCrop(height=128, width=128),\n        A.Rotate(limit=40, p=0.9, border_mode=cv2.BORDER_CONSTANT),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.1),\n        A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n        A.RandomBrightnessContrast(p=0.5),\n        A.Normalize(\n            mean=[0, 0, 0],\n            std=[1, 1, 1],\n            max_pixel_value=255,\n        ),\n        ToTensorV2(),\n    ]\n)\n\nval_transform = A.Compose(\n    [\n        A.SmallestMaxSize(max_size=160),\n        A.CenterCrop(height=128, width=128),\n        A.Normalize(\n            mean=[0, 0, 0],\n            std=[1, 1, 1],\n            max_pixel_value=255,\n        ),\n        ToTensorV2(),\n    ]\n)\n\n\ntrain_filt = L(o.parent.parent.name=='train' for o in files)\ntrain,valid = files[train_filt],files[~train_filt]\nlen(train),len(valid)\n\n(9469, 3925)\n\n\n\ntrain_ds = Imagenette(train, v2i, train_transform)\nval_ds = Imagenette(valid, v2i, val_transform)\nx,y = train_ds[1000]\nx.shape,y\n\n(torch.Size([3, 128, 128]), 1)\n\n\n\nlen(train_ds), len(val_ds)\n\n(9469, 3925)\n\n\n\ntrainloader, valoader = get_loaders(configs['batch_size'])"
  },
  {
    "objectID": "posts/transfer-learning-intermediate/making-transfer-learning-work.html#training-helper-functions",
    "href": "posts/transfer-learning-intermediate/making-transfer-learning-work.html#training-helper-functions",
    "title": "making transfer learning work in plain PyTorch",
    "section": "Training Helper Functions",
    "text": "Training Helper Functions\nNext we define our basic training loops that we are going to use in this notebook:\nNote: We are going to use Wandb’s (Weights and Biaises) experiment tracking tool for logging and plotting our metrics:\n\ndef train(train_loader, model, criterion, optimizer, epoch, scheduler=None):\n    metric_monitor = MetricMonitor()\n    model.train()\n    stream = tqdm(train_loader)\n    for i, (images, target) in enumerate(stream, start=1):\n        images = images.to(configs['device'])\n        target = target.to(configs['device'])\n        output = model(images)\n        loss = criterion(output, target)\n        accuracy = calculate_accuracy(output, target)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        if scheduler:\n          scheduler.step()\n        \n        metric_monitor.update(\"Loss\", loss.item())\n        metric_monitor.update(\"Accuracy\", accuracy)\n        stream.set_description(\n            f\"Epoch: {epoch}. Train       {metric_monitor}\"\n        )\n    wandb.log({'Train Loss': metric_monitor.metrics['Loss']['avg'], \n               'Train Accuracy':metric_monitor.metrics['Accuracy']['avg']})\n\n\ndef validate(val_loader, model, criterion, epoch):\n    metric_monitor = MetricMonitor()\n    model.eval()\n    stream = tqdm(val_loader)\n    with torch.no_grad():\n        for i, (images, target) in enumerate(stream, start=1):\n            images = images.to(configs['device'])\n            target = target.to(configs['device'])\n            output = model(images)\n            loss = criterion(output, target)\n            accuracy = calculate_accuracy(output, target)\n\n            metric_monitor.update(\"Loss\", loss.item())\n            metric_monitor.update(\"Accuracy\", accuracy)\n            stream.set_description(\n                f\"Epoch: {epoch}. Validation  {metric_monitor}\"\n            )\n\n        wandb.log({'Val Loss': metric_monitor.metrics['Loss']['avg'], \n                  'Val Accuracy':metric_monitor.metrics['Accuracy']['avg']})\n\n\ndef loss_func(out, targ):\n    return F.cross_entropy(out, targ.long())\n\ndef get_optim(model, lr):\n  return optim.AdamW(model.parameters(), lr=lr)\n\ndef get_scheduler(optimizer, max_lr, epochs):\n  return optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr,\n                                          steps_per_epoch=int(len(trainloader)),\n                                          epochs=epochs,\n                                          anneal_strategy='cos')\n\ndef fit(epochs):\n  for epoch in range(1, epochs + 1):\n    train(trainloader, model, loss_func, optimizer, epoch, scheduler)\n    validate(valoader, model, loss_func, epoch)\n    print()"
  },
  {
    "objectID": "posts/transfer-learning-intermediate/making-transfer-learning-work.html#baseline---training-from-scratch",
    "href": "posts/transfer-learning-intermediate/making-transfer-learning-work.html#baseline---training-from-scratch",
    "title": "making transfer learning work in plain PyTorch",
    "section": "Baseline - Training from Scratch",
    "text": "Baseline - Training from Scratch\nIn all Machine Learning tasks, you are better off starting with a baseline model that you can use to compare the performance of the models you are going to train.\nTo get some sort of baseline on how well our Transfer Learning is working, we will first get a Resnet 34 architecture and train it from scratch without any transfer learning:\n\nwith wandb.init(project='Transfer-Learning-Pytorch', config=configs, \n                name='training from scratch'):\n  config = wandb.config\n\n  # get the model from timm\n  model = timm.create_model(f'{config.architecture}', pretrained=False)\n  # Change the last linear to match our number of classes\n  model.fc = nn.Linear(512, config.num_classes)\n  # Initialize the weights\n  model.fc.apply(init_weights)\n  model = model.to(config.device)\n  wandb.watch(model, log=\"all\")\n\n  optimizer = get_optim(model, config.lr)\n  scheduler = get_scheduler(optimizer, config.lr, config.num_epochs+5)\n\n  fit(config.num_epochs+5)\n\n\n                    Syncing run training from scratch to Weights & Biases (docs).\n\n                \n\n\nEpoch: 1. Train       Loss: 1.974 | Accuracy: 0.306: 100%|██████████| 148/148 [00:54<00:00,  2.70it/s]\nEpoch: 1. Validation  Loss: 1.794 | Accuracy: 0.381: 100%|██████████| 62/62 [00:11<00:00,  5.34it/s]\n\n\n\n\n\nEpoch: 2. Train       Loss: 1.736 | Accuracy: 0.399: 100%|██████████| 148/148 [00:54<00:00,  2.72it/s]\nEpoch: 2. Validation  Loss: 1.855 | Accuracy: 0.392: 100%|██████████| 62/62 [00:11<00:00,  5.52it/s]\n\n\n\n\n\nEpoch: 3. Train       Loss: 1.629 | Accuracy: 0.452: 100%|██████████| 148/148 [00:54<00:00,  2.71it/s]\nEpoch: 3. Validation  Loss: 1.904 | Accuracy: 0.423: 100%|██████████| 62/62 [00:11<00:00,  5.45it/s]\n\n\n\n\n\nEpoch: 4. Train       Loss: 1.532 | Accuracy: 0.490: 100%|██████████| 148/148 [00:54<00:00,  2.71it/s]\nEpoch: 4. Validation  Loss: 1.576 | Accuracy: 0.490: 100%|██████████| 62/62 [00:11<00:00,  5.47it/s]\n\n\n\n\n\nEpoch: 5. Train       Loss: 1.409 | Accuracy: 0.532: 100%|██████████| 148/148 [00:54<00:00,  2.72it/s]\nEpoch: 5. Validation  Loss: 1.464 | Accuracy: 0.538: 100%|██████████| 62/62 [00:11<00:00,  5.18it/s]\n\n\n\n\n\nEpoch: 6. Train       Loss: 1.329 | Accuracy: 0.560: 100%|██████████| 148/148 [00:54<00:00,  2.71it/s]\nEpoch: 6. Validation  Loss: 1.147 | Accuracy: 0.623: 100%|██████████| 62/62 [00:11<00:00,  5.48it/s]\n\n\n\n\n\nEpoch: 7. Train       Loss: 1.223 | Accuracy: 0.594: 100%|██████████| 148/148 [00:54<00:00,  2.71it/s]\nEpoch: 7. Validation  Loss: 1.048 | Accuracy: 0.657: 100%|██████████| 62/62 [00:11<00:00,  5.44it/s]\n\n\n\n\n\nEpoch: 8. Train       Loss: 1.135 | Accuracy: 0.623: 100%|██████████| 148/148 [00:54<00:00,  2.70it/s]\nEpoch: 8. Validation  Loss: 1.019 | Accuracy: 0.669: 100%|██████████| 62/62 [00:11<00:00,  5.44it/s]\n\n\n\n\n\nEpoch: 9. Train       Loss: 1.089 | Accuracy: 0.641: 100%|██████████| 148/148 [00:55<00:00,  2.69it/s]\nEpoch: 9. Validation  Loss: 1.162 | Accuracy: 0.640: 100%|██████████| 62/62 [00:11<00:00,  5.23it/s]\n\n\n\n\n\nEpoch: 10. Train       Loss: 0.989 | Accuracy: 0.671: 100%|██████████| 148/148 [00:55<00:00,  2.65it/s]\nEpoch: 10. Validation  Loss: 1.118 | Accuracy: 0.656: 100%|██████████| 62/62 [00:11<00:00,  5.44it/s]\n\n\n\n\n\nEpoch: 11. Train       Loss: 0.940 | Accuracy: 0.693: 100%|██████████| 148/148 [00:54<00:00,  2.69it/s]\nEpoch: 11. Validation  Loss: 0.883 | Accuracy: 0.716: 100%|██████████| 62/62 [00:11<00:00,  5.47it/s]\n\n\n\n\n\nEpoch: 12. Train       Loss: 0.874 | Accuracy: 0.711: 100%|██████████| 148/148 [00:54<00:00,  2.69it/s]\nEpoch: 12. Validation  Loss: 0.796 | Accuracy: 0.741: 100%|██████████| 62/62 [00:11<00:00,  5.41it/s]\n\n\n\n\n\nEpoch: 13. Train       Loss: 0.828 | Accuracy: 0.729: 100%|██████████| 148/148 [00:54<00:00,  2.71it/s]\nEpoch: 13. Validation  Loss: 0.665 | Accuracy: 0.787: 100%|██████████| 62/62 [00:11<00:00,  5.32it/s]\n\n\n\n\n\nEpoch: 14. Train       Loss: 0.781 | Accuracy: 0.743: 100%|██████████| 148/148 [00:54<00:00,  2.69it/s]\nEpoch: 14. Validation  Loss: 0.648 | Accuracy: 0.790: 100%|██████████| 62/62 [00:11<00:00,  5.20it/s]\n\n\n\n\n\nEpoch: 15. Train       Loss: 0.753 | Accuracy: 0.754: 100%|██████████| 148/148 [00:55<00:00,  2.65it/s]\nEpoch: 15. Validation  Loss: 0.646 | Accuracy: 0.792: 100%|██████████| 62/62 [00:11<00:00,  5.46it/s]\n\n\n\n\n\nWaiting for W&B process to finish, PID 233... (success).\n\n\n\n\n\n\n\n\nRun history:Train Accuracy▁▂▃▄▅▅▅▆▆▇▇▇███Train Loss█▇▆▅▅▄▄▃▃▂▂▂▁▁▁Val Accuracy▁▁▂▃▄▅▆▆▅▆▇▇███Val Loss▇██▆▆▄▃▃▄▄▂▂▁▁▁\nRun summary:Train Accuracy0.75394Train Loss0.75312Val Accuracy0.7923Val Loss0.64564\n\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\nSynced training from scratch: https://wandb.ai/jimmiemunyi/Transfer-Learning-Pytorch/runs/2zy2cc88\nFind logs at: ./wandb/run-20211017_151239-2zy2cc88/logs\n\n\nAfter 15 epochs, we get a train loss of 0.735 and a validation loss of 0.646, which an accuracy of 79%. This is the first baseline we are going to try and beat."
  },
  {
    "objectID": "posts/transfer-learning-intermediate/making-transfer-learning-work.html#baseline---basic-transfer-learning",
    "href": "posts/transfer-learning-intermediate/making-transfer-learning-work.html#baseline---basic-transfer-learning",
    "title": "making transfer learning work in plain PyTorch",
    "section": "Baseline - Basic Transfer Learning",
    "text": "Baseline - Basic Transfer Learning\nFor our second baseline, we are going to do what most tutorials on Transfer Learning do, and that is:\n\n\n\nBaseline Transfer Learning.jpg\n\n\n\nwith wandb.init(project='Transfer-Learning-Pytorch', config=configs, \n                name='basic transfer learning'):\n  config = wandb.config\n\n  # get the model from timm\n  model = timm.create_model(f'{config.architecture}', pretrained=True)\n  # Freeze all parameters\n  for param in model.parameters():\n      param.requires_grad = False\n  # Change the last linear to match our number of classes\n  model.fc = nn.Linear(512, config.num_classes)\n  # Initialize the weights\n  model.fc.apply(init_weights)\n  model = model.to(config.device)\n  wandb.watch(model, log=\"all\")\n\n  optimizer = get_optim(model, config.lr)\n  scheduler = get_scheduler(optimizer, config.lr, config.num_epochs+5)\n\n  fit(config.num_epochs+5)\n\n\n                    Syncing run basic transfer learning to Weights & Biases (docs).\n\n                \n\n\nEpoch: 1. Train       Loss: 2.645 | Accuracy: 0.213: 100%|██████████| 148/148 [00:31<00:00,  4.64it/s]\nEpoch: 1. Validation  Loss: 1.493 | Accuracy: 0.520: 100%|██████████| 62/62 [00:11<00:00,  5.54it/s]\n\n\n\n\n\nEpoch: 2. Train       Loss: 1.218 | Accuracy: 0.603: 100%|██████████| 148/148 [00:31<00:00,  4.63it/s]\nEpoch: 2. Validation  Loss: 0.550 | Accuracy: 0.834: 100%|██████████| 62/62 [00:11<00:00,  5.52it/s]\n\n\n\n\n\nEpoch: 3. Train       Loss: 0.780 | Accuracy: 0.749: 100%|██████████| 148/148 [00:31<00:00,  4.64it/s]\nEpoch: 3. Validation  Loss: 0.399 | Accuracy: 0.876: 100%|██████████| 62/62 [00:11<00:00,  5.40it/s]\n\n\n\n\n\nEpoch: 4. Train       Loss: 0.684 | Accuracy: 0.783: 100%|██████████| 148/148 [00:32<00:00,  4.62it/s]\nEpoch: 4. Validation  Loss: 0.352 | Accuracy: 0.898: 100%|██████████| 62/62 [00:11<00:00,  5.55it/s]\n\n\n\n\n\nEpoch: 5. Train       Loss: 0.636 | Accuracy: 0.798: 100%|██████████| 148/148 [00:31<00:00,  4.63it/s]\nEpoch: 5. Validation  Loss: 0.354 | Accuracy: 0.895: 100%|██████████| 62/62 [00:11<00:00,  5.25it/s]\n\n\n\n\n\nEpoch: 6. Train       Loss: 0.604 | Accuracy: 0.809: 100%|██████████| 148/148 [00:31<00:00,  4.64it/s]\nEpoch: 6. Validation  Loss: 0.336 | Accuracy: 0.903: 100%|██████████| 62/62 [00:11<00:00,  5.55it/s]\n\n\n\n\n\nEpoch: 7. Train       Loss: 0.593 | Accuracy: 0.815: 100%|██████████| 148/148 [00:32<00:00,  4.59it/s]\nEpoch: 7. Validation  Loss: 0.333 | Accuracy: 0.899: 100%|██████████| 62/62 [00:11<00:00,  5.47it/s]\n\n\n\n\n\nEpoch: 8. Train       Loss: 0.567 | Accuracy: 0.816: 100%|██████████| 148/148 [00:32<00:00,  4.61it/s]\nEpoch: 8. Validation  Loss: 0.324 | Accuracy: 0.907: 100%|██████████| 62/62 [00:11<00:00,  5.54it/s]\n\n\n\n\n\nEpoch: 9. Train       Loss: 0.568 | Accuracy: 0.819: 100%|██████████| 148/148 [00:32<00:00,  4.60it/s]\nEpoch: 9. Validation  Loss: 0.327 | Accuracy: 0.904: 100%|██████████| 62/62 [00:11<00:00,  5.48it/s]\n\n\n\n\n\nEpoch: 10. Train       Loss: 0.543 | Accuracy: 0.823: 100%|██████████| 148/148 [00:32<00:00,  4.53it/s]\nEpoch: 10. Validation  Loss: 0.317 | Accuracy: 0.908: 100%|██████████| 62/62 [00:11<00:00,  5.50it/s]\n\n\n\n\n\nEpoch: 11. Train       Loss: 0.540 | Accuracy: 0.832: 100%|██████████| 148/148 [00:32<00:00,  4.60it/s]\nEpoch: 11. Validation  Loss: 0.316 | Accuracy: 0.910: 100%|██████████| 62/62 [00:11<00:00,  5.40it/s]\n\n\n\n\n\nEpoch: 12. Train       Loss: 0.518 | Accuracy: 0.832: 100%|██████████| 148/148 [00:32<00:00,  4.59it/s]\nEpoch: 12. Validation  Loss: 0.310 | Accuracy: 0.911: 100%|██████████| 62/62 [00:11<00:00,  5.50it/s]\n\n\n\n\n\nEpoch: 13. Train       Loss: 0.519 | Accuracy: 0.832: 100%|██████████| 148/148 [00:32<00:00,  4.58it/s]\nEpoch: 13. Validation  Loss: 0.299 | Accuracy: 0.916: 100%|██████████| 62/62 [00:11<00:00,  5.57it/s]\n\n\n\n\n\nEpoch: 14. Train       Loss: 0.532 | Accuracy: 0.828: 100%|██████████| 148/148 [00:31<00:00,  4.63it/s]\nEpoch: 14. Validation  Loss: 0.308 | Accuracy: 0.913: 100%|██████████| 62/62 [00:11<00:00,  5.58it/s]\n\n\n\n\n\nEpoch: 15. Train       Loss: 0.519 | Accuracy: 0.836: 100%|██████████| 148/148 [00:32<00:00,  4.56it/s]\nEpoch: 15. Validation  Loss: 0.308 | Accuracy: 0.911: 100%|██████████| 62/62 [00:11<00:00,  5.56it/s]\n\n\n\n\n\nWaiting for W&B process to finish, PID 789... (success).\n\n\n\n\n\n\n\n\nRun history:Train Accuracy▁▅▇▇███████████Train Loss█▃▂▂▁▁▁▁▁▁▁▁▁▁▁Val Accuracy▁▇▇████████████Val Loss█▂▂▁▁▁▁▁▁▁▁▁▁▁▁\nRun summary:Train Accuracy0.83642Train Loss0.51881Val Accuracy0.91103Val Loss0.30831\n\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\nSynced basic transfer learning: https://wandb.ai/jimmiemunyi/Transfer-Learning-Pytorch/runs/12bk94ij\nFind logs at: ./wandb/run-20211017_153023-12bk94ij/logs\n\n\nFor our second baseline, we get a train loss of 0.519 and a validation loss of 0.308, which an accuracy of 91%. This is the second baseline we are going to try and beat.\nWith those two Base line results, we can now start using a few tweaks and tricks to make our fine-tuning work."
  },
  {
    "objectID": "posts/transfer-learning-intermediate/making-transfer-learning-work.html#custom-head",
    "href": "posts/transfer-learning-intermediate/making-transfer-learning-work.html#custom-head",
    "title": "making transfer learning work in plain PyTorch",
    "section": "Custom Head",
    "text": "Custom Head\nThe first modification we are going to make to our transfer learning process is using a better custom head.\nWhat I mean by this is, instead of just changing the final linear classifier in our pretrained model, we are going to chop our model into two:\n\nA Body which will act as our feature extractor (Already pretrained)\nA custom head that will be our classifier.\n\n\n\n\nBody-Head Chop.jpg\n\n\nThe reason we split the model right before the pooling operation is, in Deep Learning, we can fine-tune a model pretrained for classification and use it for other tasks like segmentation or object detection. Different applications might require different pooling layers or even none.\nThis is going to be our body of our pretrained model:\n\ndef make_body(model):\n  layers = list((model.children()))[:-2]\n  body =  nn.Sequential(*layers)\n\n  # freeze the body\n  for param in body.parameters():\n    param.requires_grad = False\n  \n  return body\n\nOur head is going to be a Sequential Layer that follows the following pattern:\n\nOur New Pooling operation followed by flattening the output.\nA Sequence of BatchNorm -> Dropout -> Linear Layer. You can add as many of these as your task at hand requires, just remember to use and activation between two such sequences.\n\nFor our task at hand, we are going to use the following configuration:\nNote, the input to our BatchNorm is 512 since that’s what our head receives from the body, then pools and flattens it. We won’t require a ReLU for our last linear layer and its out_features is going to be the number of classes we have in our dataset:\n\ndef make_head(in_features, out_features):\n  head = nn.Sequential(\n      nn.AdaptiveAvgPool2d(output_size=1),\n      nn.Flatten(),\n      nn.BatchNorm1d(num_features=in_features),\n      nn.Dropout(p=0.25),\n      nn.Linear(in_features=in_features, out_features=512, bias=False),\n      nn.ReLU(inplace=True),\n      nn.BatchNorm1d(num_features=512),\n      nn.Dropout(p=0.5),\n      nn.Linear(in_features=512, out_features=out_features, bias=False)\n  )\n  head.apply(init_weights)\n  return head\n\n\nmake_head(512, len(lbls))\n\nSequential(\n  (0): AdaptiveAvgPool2d(output_size=1)\n  (1): Flatten(start_dim=1, end_dim=-1)\n  (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (3): Dropout(p=0.25, inplace=False)\n  (4): Linear(in_features=512, out_features=512, bias=False)\n  (5): ReLU(inplace=True)\n  (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (7): Dropout(p=0.5, inplace=False)\n  (8): Linear(in_features=512, out_features=10, bias=False)\n)\n\n\nLet us create a handy function that creates our model for us:\n\ndef create_model(arch):\n  # get pretrained model\n  model = timm.create_model(f'{arch}', pretrained=True)\n\n  # create the body and head\n  body = make_body(model)\n  head = make_head(512, len(lbls))\n\n  # fine tune the model\n  model = nn.Sequential(body, head)\n\n  return model\n\nAnd then now we can do the fine-tuning:\n\nwith wandb.init(project='Transfer-Learning-Pytorch', config=configs, \n                name='transfer learning custom head'):\n  config = wandb.config\n\n  # create model\n  model = create_model(config.architecture)\n  model = model.to(config.device)\n  wandb.watch(model, log=\"all\")\n\n  optimizer = get_optim(model, config.lr)\n  scheduler = get_scheduler(optimizer, config.lr, config.num_epochs+5)\n\n  fit(config.num_epochs+5)\n\n\n                    Syncing run transfer learning custom head to Weights & Biases (docs).\n\n                \n\n\nEpoch: 1. Train       Loss: 2.295 | Accuracy: 0.358: 100%|██████████| 148/148 [00:31<00:00,  4.68it/s]\nEpoch: 1. Validation  Loss: 0.526 | Accuracy: 0.851: 100%|██████████| 62/62 [00:11<00:00,  5.63it/s]\n\n\n\n\n\nEpoch: 2. Train       Loss: 1.125 | Accuracy: 0.666: 100%|██████████| 148/148 [00:31<00:00,  4.72it/s]\nEpoch: 2. Validation  Loss: 0.371 | Accuracy: 0.891: 100%|██████████| 62/62 [00:11<00:00,  5.59it/s]\n\n\n\n\n\nEpoch: 3. Train       Loss: 0.897 | Accuracy: 0.725: 100%|██████████| 148/148 [00:31<00:00,  4.67it/s]\nEpoch: 3. Validation  Loss: 0.359 | Accuracy: 0.894: 100%|██████████| 62/62 [00:10<00:00,  5.71it/s]\n\n\n\n\n\nEpoch: 4. Train       Loss: 0.822 | Accuracy: 0.743: 100%|██████████| 148/148 [00:31<00:00,  4.69it/s]\nEpoch: 4. Validation  Loss: 0.331 | Accuracy: 0.899: 100%|██████████| 62/62 [00:11<00:00,  5.50it/s]\n\n\n\n\n\nEpoch: 5. Train       Loss: 0.770 | Accuracy: 0.758: 100%|██████████| 148/148 [00:31<00:00,  4.63it/s]\nEpoch: 5. Validation  Loss: 0.318 | Accuracy: 0.904: 100%|██████████| 62/62 [00:11<00:00,  5.22it/s]\n\n\n\n\n\nEpoch: 6. Train       Loss: 0.731 | Accuracy: 0.763: 100%|██████████| 148/148 [00:31<00:00,  4.67it/s]\nEpoch: 6. Validation  Loss: 0.311 | Accuracy: 0.905: 100%|██████████| 62/62 [00:10<00:00,  5.64it/s]\n\n\n\n\n\nEpoch: 7. Train       Loss: 0.696 | Accuracy: 0.777: 100%|██████████| 148/148 [00:31<00:00,  4.65it/s]\nEpoch: 7. Validation  Loss: 0.308 | Accuracy: 0.908: 100%|██████████| 62/62 [00:11<00:00,  5.54it/s]\n\n\n\n\n\nEpoch: 8. Train       Loss: 0.673 | Accuracy: 0.778: 100%|██████████| 148/148 [00:31<00:00,  4.69it/s]\nEpoch: 8. Validation  Loss: 0.314 | Accuracy: 0.907: 100%|██████████| 62/62 [00:11<00:00,  5.63it/s]\n\n\n\n\n\nEpoch: 9. Train       Loss: 0.667 | Accuracy: 0.786: 100%|██████████| 148/148 [00:31<00:00,  4.70it/s]\nEpoch: 9. Validation  Loss: 0.300 | Accuracy: 0.907: 100%|██████████| 62/62 [00:10<00:00,  5.68it/s]\n\n\n\n\n\nEpoch: 10. Train       Loss: 0.657 | Accuracy: 0.785: 100%|██████████| 148/148 [00:31<00:00,  4.65it/s]\nEpoch: 10. Validation  Loss: 0.290 | Accuracy: 0.910: 100%|██████████| 62/62 [00:10<00:00,  5.68it/s]\n\n\n\n\n\nEpoch: 11. Train       Loss: 0.655 | Accuracy: 0.787: 100%|██████████| 148/148 [00:31<00:00,  4.72it/s]\nEpoch: 11. Validation  Loss: 0.288 | Accuracy: 0.914: 100%|██████████| 62/62 [00:10<00:00,  5.67it/s]\n\n\n\n\n\nEpoch: 12. Train       Loss: 0.639 | Accuracy: 0.793: 100%|██████████| 148/148 [00:31<00:00,  4.72it/s]\nEpoch: 12. Validation  Loss: 0.285 | Accuracy: 0.914: 100%|██████████| 62/62 [00:11<00:00,  5.54it/s]\n\n\n\n\n\nEpoch: 13. Train       Loss: 0.632 | Accuracy: 0.796: 100%|██████████| 148/148 [00:31<00:00,  4.63it/s]\nEpoch: 13. Validation  Loss: 0.282 | Accuracy: 0.912: 100%|██████████| 62/62 [00:11<00:00,  5.57it/s]\n\n\n\n\n\nEpoch: 14. Train       Loss: 0.620 | Accuracy: 0.798: 100%|██████████| 148/148 [00:32<00:00,  4.59it/s]\nEpoch: 14. Validation  Loss: 0.282 | Accuracy: 0.914: 100%|██████████| 62/62 [00:11<00:00,  5.46it/s]\n\n\n\n\n\nEpoch: 15. Train       Loss: 0.605 | Accuracy: 0.797: 100%|██████████| 148/148 [00:32<00:00,  4.50it/s]\nEpoch: 15. Validation  Loss: 0.279 | Accuracy: 0.913: 100%|██████████| 62/62 [00:11<00:00,  5.46it/s]\n\n\n\n\n\nWaiting for W&B process to finish, PID 1238... (success).\n\n\n\n\n\n\n\n\nRun history:Train Accuracy▁▆▇▇▇▇█████████Train Loss█▃▂▂▂▂▁▁▁▁▁▁▁▁▁Val Accuracy▁▅▆▆▇▇▇▇▇▇█████Val Loss█▄▃▂▂▂▂▂▂▁▁▁▁▁▁\nRun summary:Train Accuracy0.79691Train Loss0.60476Val Accuracy0.91305Val Loss0.27871\n\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\nSynced transfer learning custom head: https://wandb.ai/jimmiemunyi/Transfer-Learning-Pytorch/runs/36k0wp38\nFind logs at: ./wandb/run-20211017_154149-36k0wp38/logs\n\n\nWe get a train loss of 0.605 and a validation loss of 0.279, which an accuracy of 91%. It is a slight improvement on our fine-tuning which doesn’t look like it does much, but now we have a better design of the head than just changing the classes of the last linear layer. Our custom head can be adopted to different training requirements by changing the configuration of the dropouts and batchnorms."
  },
  {
    "objectID": "posts/transfer-learning-intermediate/making-transfer-learning-work.html#dont-freeze-batchnorm",
    "href": "posts/transfer-learning-intermediate/making-transfer-learning-work.html#dont-freeze-batchnorm",
    "title": "making transfer learning work in plain PyTorch",
    "section": "Don’t Freeze BatchNorm",
    "text": "Don’t Freeze BatchNorm\nAnother tweak from fastai’s MOOC is about how to handle the BatchNorm layers while fine-tuning. To understand this, we will need to go back to how BatchNorm works.\nBatchNorm works differently in training vs validation. During validation, BatchNorm uses a running mean of the statistics calculated during training. Therefore, when we get a pretrained layer, our BatchNorm’s present have statistics of its previous training (commonly on Imagenet). But during our fine-tuning, we want our model to adapt to the statistics of our current training statistics, and that is why we should not freeze them.\nTo do this, we are just going to slightly change our get_body function, and not freeze BatchNorm layers while freezing the rest of the layers\n\ndef make_body(model):\n  layers = list((model.children()))[:-2]\n  body =  nn.Sequential(*layers)\n\n  # Loop through the model and don't freeze BatchNorm\n  for module in body.modules():\n    if isinstance(module, torch.nn.BatchNorm2d):\n      for param in module.parameters():\n        param.requires_grad = True\n    else:\n      for param in module.parameters():\n        param.requires_grad = False\n\n  return body\n\nLet run the experiment and see if that improves our fine-tuning:\n\nwith wandb.init(project='Transfer-Learning-Pytorch', config=configs, \n                name='transfer learning unfrozen batchnorm'):\n  config = wandb.config\n\n  # create model\n  model = create_model(config.architecture)\n  model = model.to(config.device)\n  wandb.watch(model, log=\"all\")\n\n  optimizer = get_optim(model, config.lr)\n  scheduler = get_scheduler(optimizer, config.lr, config.num_epochs+5)\n\n  fit(config.num_epochs+5)\n\n\n                    Syncing run transfer learning unfrozen batchnorm to Weights & Biases (docs).\n\n                \n\n\nEpoch: 1. Train       Loss: 2.287 | Accuracy: 0.360: 100%|██████████| 148/148 [00:38<00:00,  3.87it/s]\nEpoch: 1. Validation  Loss: 0.521 | Accuracy: 0.847: 100%|██████████| 62/62 [00:11<00:00,  5.54it/s]\n\n\n\n\n\nEpoch: 2. Train       Loss: 0.994 | Accuracy: 0.700: 100%|██████████| 148/148 [00:38<00:00,  3.88it/s]\nEpoch: 2. Validation  Loss: 0.317 | Accuracy: 0.906: 100%|██████████| 62/62 [00:11<00:00,  5.54it/s]\n\n\n\n\n\nEpoch: 3. Train       Loss: 0.712 | Accuracy: 0.779: 100%|██████████| 148/148 [00:38<00:00,  3.86it/s]\nEpoch: 3. Validation  Loss: 0.241 | Accuracy: 0.926: 100%|██████████| 62/62 [00:11<00:00,  5.53it/s]\n\n\n\n\n\nEpoch: 4. Train       Loss: 0.567 | Accuracy: 0.826: 100%|██████████| 148/148 [00:38<00:00,  3.87it/s]\nEpoch: 4. Validation  Loss: 0.183 | Accuracy: 0.944: 100%|██████████| 62/62 [00:11<00:00,  5.54it/s]\n\n\n\n\n\nEpoch: 5. Train       Loss: 0.476 | Accuracy: 0.853: 100%|██████████| 148/148 [00:38<00:00,  3.87it/s]\nEpoch: 5. Validation  Loss: 0.151 | Accuracy: 0.954: 100%|██████████| 62/62 [00:11<00:00,  5.23it/s]\n\n\n\n\n\nEpoch: 6. Train       Loss: 0.409 | Accuracy: 0.872: 100%|██████████| 148/148 [00:37<00:00,  3.90it/s]\nEpoch: 6. Validation  Loss: 0.144 | Accuracy: 0.953: 100%|██████████| 62/62 [00:11<00:00,  5.54it/s]\n\n\n\n\n\nEpoch: 7. Train       Loss: 0.376 | Accuracy: 0.881: 100%|██████████| 148/148 [00:37<00:00,  3.90it/s]\nEpoch: 7. Validation  Loss: 0.136 | Accuracy: 0.958: 100%|██████████| 62/62 [00:11<00:00,  5.61it/s]\n\n\n\n\n\nEpoch: 8. Train       Loss: 0.362 | Accuracy: 0.884: 100%|██████████| 148/148 [00:37<00:00,  3.93it/s]\nEpoch: 8. Validation  Loss: 0.139 | Accuracy: 0.955: 100%|██████████| 62/62 [00:10<00:00,  5.70it/s]\n\n\n\n\n\nEpoch: 9. Train       Loss: 0.342 | Accuracy: 0.892: 100%|██████████| 148/148 [00:37<00:00,  3.94it/s]\nEpoch: 9. Validation  Loss: 0.123 | Accuracy: 0.961: 100%|██████████| 62/62 [00:10<00:00,  5.66it/s]\n\n\n\n\n\nEpoch: 10. Train       Loss: 0.331 | Accuracy: 0.895: 100%|██████████| 148/148 [00:38<00:00,  3.83it/s]\nEpoch: 10. Validation  Loss: 0.120 | Accuracy: 0.963: 100%|██████████| 62/62 [00:11<00:00,  5.61it/s]\n\n\n\n\n\nEpoch: 11. Train       Loss: 0.323 | Accuracy: 0.897: 100%|██████████| 148/148 [00:37<00:00,  3.94it/s]\nEpoch: 11. Validation  Loss: 0.118 | Accuracy: 0.963: 100%|██████████| 62/62 [00:10<00:00,  5.65it/s]\n\n\n\n\n\nEpoch: 12. Train       Loss: 0.296 | Accuracy: 0.905: 100%|██████████| 148/148 [00:37<00:00,  3.94it/s]\nEpoch: 12. Validation  Loss: 0.112 | Accuracy: 0.964: 100%|██████████| 62/62 [00:10<00:00,  5.66it/s]\n\n\n\n\n\nEpoch: 13. Train       Loss: 0.283 | Accuracy: 0.908: 100%|██████████| 148/148 [00:37<00:00,  3.97it/s]\nEpoch: 13. Validation  Loss: 0.113 | Accuracy: 0.963: 100%|██████████| 62/62 [00:10<00:00,  5.68it/s]\n\n\n\n\n\nEpoch: 14. Train       Loss: 0.274 | Accuracy: 0.910: 100%|██████████| 148/148 [00:36<00:00,  4.01it/s]\nEpoch: 14. Validation  Loss: 0.114 | Accuracy: 0.964: 100%|██████████| 62/62 [00:10<00:00,  5.75it/s]\n\n\n\n\n\nEpoch: 15. Train       Loss: 0.278 | Accuracy: 0.909: 100%|██████████| 148/148 [00:38<00:00,  3.88it/s]\nEpoch: 15. Validation  Loss: 0.110 | Accuracy: 0.965: 100%|██████████| 62/62 [00:10<00:00,  5.70it/s]\n\n\n\n\n\nWaiting for W&B process to finish, PID 1689... (success).\n\n\n\n\n\n\n\n\nRun history:Train Accuracy▁▅▆▇▇██████████Train Loss█▄▃▂▂▁▁▁▁▁▁▁▁▁▁Val Accuracy▁▅▆▇▇▇█▇███████Val Loss█▅▃▂▂▂▁▁▁▁▁▁▁▁▁\nRun summary:Train Accuracy0.90886Train Loss0.27814Val Accuracy0.96472Val Loss0.10962\n\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\nSynced transfer learning unfrozen batchnorm: https://wandb.ai/jimmiemunyi/Transfer-Learning-Pytorch/runs/p4szuadu\nFind logs at: ./wandb/run-20211017_155246-p4szuadu/logs\n\n\nJust that slight tweak improved our model to an accuracy of 96.5%. Our train and validation also decreased to 0.278 and 0.110 respectively. This confirms the theory why not freezing the BatchNorm is important."
  },
  {
    "objectID": "posts/transfer-learning-intermediate/making-transfer-learning-work.html#unfreezing",
    "href": "posts/transfer-learning-intermediate/making-transfer-learning-work.html#unfreezing",
    "title": "making transfer learning work in plain PyTorch",
    "section": "Unfreezing",
    "text": "Unfreezing\nSo far, all we have been doing is keeping the body of our pretrained model frozen and fine-tuning the newly created head and it worked out fine. But we can also try something else. How about unfreezing our whole model after some epochs and training it to see if it improves our metrics?\nTherefore, the next experiment we are going to try is unfreezing our model after a few epochs and fine tuning the whole model on our new dataset.\n\n\n\nUnfreezing.jpg\n\n\n\nwith wandb.init(project='Transfer-Learning-Pytorch', config=configs, \n                name='transfer learning with unfreezing'):\n  config = wandb.config\n\n  # create model\n  model = create_model(config.architecture)\n  model = model.to(config.device)\n  wandb.watch(model, log=\"all\")\n\n  optimizer = get_optim(model, config.lr)\n  scheduler = get_scheduler(optimizer, config.lr, config.init_epochs)\n\n  # Training while Frozen\n  print('_'*40)\n  print('Training with Frozen Body')\n  print('_'*40)\n  print()\n  fit(config.init_epochs)\n\n  # Unfreeze the model and train further\n  print('_'*40)\n  print('Unfreezing the body')\n  print('_'*40)\n  print()\n  for param in model.parameters():\n      param.requires_grad = True\n  \n  optimizer = get_optim(model, lr=1e-5)\n  scheduler = get_scheduler(optimizer, 1e-5, config.num_epochs)\n\n  print('_'*40)\n  print('Training with Unfrozen Body')\n  print('_'*40)\n  print()\n  fit(config.num_epochs)\n\n\n                    Syncing run transfer learning with unfreezing to Weights & Biases (docs).\n\n                \n\n\n________________________________________\nTraining with Frozen Body\n________________________________________\n\n\n\nEpoch: 1. Train       Loss: 1.833 | Accuracy: 0.493: 100%|██████████| 148/148 [00:37<00:00,  4.00it/s]\nEpoch: 1. Validation  Loss: 0.345 | Accuracy: 0.897: 100%|██████████| 62/62 [00:10<00:00,  5.86it/s]\n\n\n\n\n\nEpoch: 2. Train       Loss: 0.765 | Accuracy: 0.774: 100%|██████████| 148/148 [00:37<00:00,  3.98it/s]\nEpoch: 2. Validation  Loss: 0.227 | Accuracy: 0.929: 100%|██████████| 62/62 [00:11<00:00,  5.62it/s]\n\n\n\n\n\nEpoch: 3. Train       Loss: 0.544 | Accuracy: 0.831: 100%|██████████| 148/148 [00:37<00:00,  3.91it/s]\nEpoch: 3. Validation  Loss: 0.179 | Accuracy: 0.943: 100%|██████████| 62/62 [00:11<00:00,  5.64it/s]\n\n\n\n\n\nEpoch: 4. Train       Loss: 0.439 | Accuracy: 0.862: 100%|██████████| 148/148 [00:37<00:00,  3.93it/s]\nEpoch: 4. Validation  Loss: 0.168 | Accuracy: 0.949: 100%|██████████| 62/62 [00:11<00:00,  5.57it/s]\n\n\n\n\n\nEpoch: 5. Train       Loss: 0.446 | Accuracy: 0.866: 100%|██████████| 148/148 [00:38<00:00,  3.89it/s]\nEpoch: 5. Validation  Loss: 0.148 | Accuracy: 0.954: 100%|██████████| 62/62 [00:11<00:00,  5.41it/s]\n\n\n\n________________________________________\nUnfreezing the body\n________________________________________\n\n________________________________________\nTraining with Unfrozen Body\n________________________________________\n\n\n\nEpoch: 1. Train       Loss: 0.417 | Accuracy: 0.867: 100%|██████████| 148/148 [00:53<00:00,  2.76it/s]\nEpoch: 1. Validation  Loss: 0.154 | Accuracy: 0.953: 100%|██████████| 62/62 [00:11<00:00,  5.56it/s]\n\n\n\n\n\nEpoch: 2. Train       Loss: 0.409 | Accuracy: 0.866: 100%|██████████| 148/148 [00:53<00:00,  2.76it/s]\nEpoch: 2. Validation  Loss: 0.144 | Accuracy: 0.953: 100%|██████████| 62/62 [00:11<00:00,  5.64it/s]\n\n\n\n\n\nEpoch: 3. Train       Loss: 0.387 | Accuracy: 0.876: 100%|██████████| 148/148 [00:53<00:00,  2.76it/s]\nEpoch: 3. Validation  Loss: 0.145 | Accuracy: 0.953: 100%|██████████| 62/62 [00:10<00:00,  5.65it/s]\n\n\n\n\n\nEpoch: 4. Train       Loss: 0.359 | Accuracy: 0.886: 100%|██████████| 148/148 [00:53<00:00,  2.75it/s]\nEpoch: 4. Validation  Loss: 0.137 | Accuracy: 0.957: 100%|██████████| 62/62 [00:10<00:00,  5.64it/s]\n\n\n\n\n\nEpoch: 5. Train       Loss: 0.337 | Accuracy: 0.896: 100%|██████████| 148/148 [00:54<00:00,  2.71it/s]\nEpoch: 5. Validation  Loss: 0.129 | Accuracy: 0.960: 100%|██████████| 62/62 [00:11<00:00,  5.51it/s]\n\n\n\n\n\nEpoch: 6. Train       Loss: 0.328 | Accuracy: 0.892: 100%|██████████| 148/148 [00:53<00:00,  2.75it/s]\nEpoch: 6. Validation  Loss: 0.136 | Accuracy: 0.957: 100%|██████████| 62/62 [00:11<00:00,  5.63it/s]\n\n\n\n\n\nEpoch: 7. Train       Loss: 0.311 | Accuracy: 0.900: 100%|██████████| 148/148 [00:53<00:00,  2.74it/s]\nEpoch: 7. Validation  Loss: 0.126 | Accuracy: 0.960: 100%|██████████| 62/62 [00:11<00:00,  5.48it/s]\n\n\n\n\n\nEpoch: 8. Train       Loss: 0.312 | Accuracy: 0.899: 100%|██████████| 148/148 [00:53<00:00,  2.74it/s]\nEpoch: 8. Validation  Loss: 0.124 | Accuracy: 0.960: 100%|██████████| 62/62 [00:11<00:00,  5.48it/s]\n\n\n\n\n\nEpoch: 9. Train       Loss: 0.298 | Accuracy: 0.904: 100%|██████████| 148/148 [00:53<00:00,  2.74it/s]\nEpoch: 9. Validation  Loss: 0.129 | Accuracy: 0.959: 100%|██████████| 62/62 [00:11<00:00,  5.52it/s]\n\n\n\n\n\nEpoch: 10. Train       Loss: 0.304 | Accuracy: 0.902: 100%|██████████| 148/148 [00:54<00:00,  2.71it/s]\nEpoch: 10. Validation  Loss: 0.127 | Accuracy: 0.960: 100%|██████████| 62/62 [00:10<00:00,  5.72it/s]\n\n\n\n\n\nWaiting for W&B process to finish, PID 2214... (success).\n\n\n\n\n\n\n\n\nRun history:Train Accuracy▁▆▇▇▇▇▇████████Train Loss█▃▂▂▂▂▂▁▁▁▁▁▁▁▁Val Accuracy▁▅▆▇▇▇▇▇███████Val Loss█▄▃▂▂▂▂▂▁▁▁▁▁▁▁\nRun summary:Train Accuracy0.90187Train Loss0.30449Val Accuracy0.96043Val Loss0.12706\n\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\nSynced transfer learning with unfreezing: https://wandb.ai/jimmiemunyi/Transfer-Learning-Pytorch/runs/3un2fp34\nFind logs at: ./wandb/run-20211017_161036-3un2fp34/logs\n\n\nOur metrics keep on improving with the changes. This means we are headed to the right direction.\nBut if we think about it, our body and our head are two very different group of layers. On one hand, we have a body that has important weights learned from its previous task that it was pretrained on. And on the other hand, we have a head that has been initialized randomly and has no important weights until we start fine-tuning it.\nWe do not want to change the weights of the body too much as they have already learned important features that generalize well to a lot of vision tasks, while at the same time we want to make our head useful to our current task. How do we achieve both of these conditions? Enters Discriminative Learning Rates."
  },
  {
    "objectID": "posts/transfer-learning-intermediate/making-transfer-learning-work.html#discriminative-learning-rates",
    "href": "posts/transfer-learning-intermediate/making-transfer-learning-work.html#discriminative-learning-rates",
    "title": "making transfer learning work in plain PyTorch",
    "section": "Discriminative Learning Rates",
    "text": "Discriminative Learning Rates\nDiscriminative Fine Tuning was introduced by Jeremy Howard et al. in the paper Universal Language Model Fine-Tuning for Text Classification or ULMFiT in short.\nThe main intuition is that: even after we unfreeze, we stil care a lot about the quality of the pretrained weights. The best learning rates for the pretrained parameters should not be as high as for the randomly added parameters because the pretrained weights have been trained for hundreds of epochs, on millions of images.\nTherefore, after unfreezing our whole model, we are going to train the head and the body using two different learning rates.\nA good rule of thumb is: the body learning rate should be 2.6 smaller than the head learning rate. Therefore, we are going to divide the head learning rate by 2.6 to get the body learning rate.\n\nclass MyModel(nn.Module):\n  def __init__(self, body, head):\n    super(MyModel, self).__init__()\n    self.body = body\n    self.head = head\n  \n  def forward(self, x):\n    out = self.body(x)\n    return self.head(out)\n\n\nwith wandb.init(project='Transfer-Learning-Pytorch', config=configs, \n                name='discriminative learning rates'):\n  config = wandb.config\n\n  # create model\n  body = create_model(config.architecture)[0]\n  head = create_model(config.architecture)[1]\n  model = MyModel(body, head)\n\n  model = model.to(config.device)\n  wandb.watch(model, log=\"all\")\n\n  optimizer = get_optim(model, config.lr)\n  scheduler = get_scheduler(optimizer, config.lr,config.init_epochs)\n\n  # Training while Frozen\n  print('_'*40)\n  print('Training with Frozen Body')\n  print('_'*40)\n  print()\n  fit(config.init_epochs)\n\n  # Unfreeze the model and train further\n  print('_'*40)\n  print('Unfreezing the body')\n  print('_'*40)\n  print()\n  for param in model.parameters():\n      param.requires_grad = True\n  \n\n  # update the learning rates of each param group\n  head_params = []\n  body_params = []\n\n  for name, param in model.body.named_parameters():\n    body_params.append(param)\n\n  for name, param in model.head.named_parameters():\n    head_params.append(param)\n\n\n  optimizer = optim.AdamW([{'params':body_params}, {'params':head_params}], \n                          lr=1e-5)\n  scheduler = get_scheduler(optimizer, config.head_lr, config.num_epochs)\n  optimizer.param_groups[0]['lr'] = config.body_lr\n  optimizer.param_groups[1]['lr'] = config.head_lr\n\n  # discriminative learning rates\n  print('_'*40)\n  print('Training with Unfrozen Body')\n  print('_'*40)\n  print()\n  fit(config.num_epochs)\n\n\n                    Syncing run discriminative learning rates to Weights & Biases (docs).\n\n                \n\n\n________________________________________\nTraining with Frozen Body\n________________________________________\n\n\n\nEpoch: 1. Train       Loss: 1.788 | Accuracy: 0.502: 100%|██████████| 148/148 [00:37<00:00,  3.92it/s]\nEpoch: 1. Validation  Loss: 0.354 | Accuracy: 0.896: 100%|██████████| 62/62 [00:10<00:00,  5.75it/s]\n\n\n\n\n\nEpoch: 2. Train       Loss: 0.708 | Accuracy: 0.783: 100%|██████████| 148/148 [00:36<00:00,  4.01it/s]\nEpoch: 2. Validation  Loss: 0.226 | Accuracy: 0.928: 100%|██████████| 62/62 [00:10<00:00,  5.64it/s]\n\n\n\n\n\nEpoch: 3. Train       Loss: 0.532 | Accuracy: 0.838: 100%|██████████| 148/148 [00:37<00:00,  3.97it/s]\nEpoch: 3. Validation  Loss: 0.168 | Accuracy: 0.948: 100%|██████████| 62/62 [00:10<00:00,  5.69it/s]\n\n\n\n\n\nEpoch: 4. Train       Loss: 0.455 | Accuracy: 0.855: 100%|██████████| 148/148 [00:37<00:00,  3.99it/s]\nEpoch: 4. Validation  Loss: 0.161 | Accuracy: 0.948: 100%|██████████| 62/62 [00:10<00:00,  5.73it/s]\n\n\n\n\n\nEpoch: 5. Train       Loss: 0.411 | Accuracy: 0.869: 100%|██████████| 148/148 [00:37<00:00,  3.97it/s]\nEpoch: 5. Validation  Loss: 0.155 | Accuracy: 0.949: 100%|██████████| 62/62 [00:11<00:00,  5.27it/s]\n\n\n\n________________________________________\nUnfreezing the body\n________________________________________\n\n________________________________________\nTraining with Unfrozen Body\n________________________________________\n\n\n\nEpoch: 1. Train       Loss: 0.405 | Accuracy: 0.870: 100%|██████████| 148/148 [00:53<00:00,  2.76it/s]\nEpoch: 1. Validation  Loss: 0.152 | Accuracy: 0.952: 100%|██████████| 62/62 [00:10<00:00,  5.68it/s]\n\n\n\n\n\nEpoch: 2. Train       Loss: 0.408 | Accuracy: 0.870: 100%|██████████| 148/148 [00:53<00:00,  2.76it/s]\nEpoch: 2. Validation  Loss: 0.140 | Accuracy: 0.954: 100%|██████████| 62/62 [00:10<00:00,  5.75it/s]\n\n\n\n\n\nEpoch: 3. Train       Loss: 0.382 | Accuracy: 0.881: 100%|██████████| 148/148 [00:53<00:00,  2.76it/s]\nEpoch: 3. Validation  Loss: 0.137 | Accuracy: 0.957: 100%|██████████| 62/62 [00:10<00:00,  5.70it/s]\n\n\n\n\n\nEpoch: 4. Train       Loss: 0.374 | Accuracy: 0.883: 100%|██████████| 148/148 [00:53<00:00,  2.76it/s]\nEpoch: 4. Validation  Loss: 0.132 | Accuracy: 0.959: 100%|██████████| 62/62 [00:10<00:00,  5.67it/s]\n\n\n\n\n\nEpoch: 5. Train       Loss: 0.336 | Accuracy: 0.891: 100%|██████████| 148/148 [00:54<00:00,  2.73it/s]\nEpoch: 5. Validation  Loss: 0.128 | Accuracy: 0.960: 100%|██████████| 62/62 [00:11<00:00,  5.62it/s]\n\n\n\n\n\nEpoch: 6. Train       Loss: 0.318 | Accuracy: 0.899: 100%|██████████| 148/148 [00:53<00:00,  2.76it/s]\nEpoch: 6. Validation  Loss: 0.128 | Accuracy: 0.960: 100%|██████████| 62/62 [00:10<00:00,  5.71it/s]\n\n\n\n\n\nEpoch: 7. Train       Loss: 0.313 | Accuracy: 0.904: 100%|██████████| 148/148 [00:53<00:00,  2.76it/s]\nEpoch: 7. Validation  Loss: 0.126 | Accuracy: 0.960: 100%|██████████| 62/62 [00:10<00:00,  5.77it/s]\n\n\n\n\n\nEpoch: 8. Train       Loss: 0.300 | Accuracy: 0.907: 100%|██████████| 148/148 [00:53<00:00,  2.76it/s]\nEpoch: 8. Validation  Loss: 0.125 | Accuracy: 0.960: 100%|██████████| 62/62 [00:11<00:00,  5.62it/s]\n\n\n\n\n\nEpoch: 9. Train       Loss: 0.301 | Accuracy: 0.906: 100%|██████████| 148/148 [00:53<00:00,  2.76it/s]\nEpoch: 9. Validation  Loss: 0.122 | Accuracy: 0.962: 100%|██████████| 62/62 [00:10<00:00,  5.70it/s]\n\n\n\n\n\nEpoch: 10. Train       Loss: 0.317 | Accuracy: 0.899: 100%|██████████| 148/148 [00:54<00:00,  2.73it/s]\nEpoch: 10. Validation  Loss: 0.125 | Accuracy: 0.960: 100%|██████████| 62/62 [00:10<00:00,  5.67it/s]\n\n\n\n\n\nWaiting for W&B process to finish, PID 2713... (success).\n\n\n\n\n\n\n\n\nRun history:Train Accuracy▁▆▇▇▇▇▇████████Train Loss█▃▂▂▂▁▂▁▁▁▁▁▁▁▁Val Accuracy▁▄▆▆▇▇▇▇███████Val Loss█▄▂▂▂▂▂▁▁▁▁▁▁▁▁\nRun summary:Train Accuracy0.89927Train Loss0.31676Val Accuracy0.95968Val Loss0.12464\n\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\nSynced discriminative learning rates: https://wandb.ai/jimmiemunyi/Transfer-Learning-Pytorch/runs/340psirj\nFind logs at: ./wandb/run-20211017_162543-340psirj/logs\n\n\nThat brings us to the end of this blog post. You have seen some tweaks that you can use to make transfer learning work for your particular task.\nHere is a plot depicting how our specific changes impacted our accuracy.\n\n\n\nTransfer Learning Val Accuracy.png\n\n\nFollow me on Twitter for more updates."
  },
  {
    "objectID": "posts/thoughts/not-into-computers/not-into-computers.html",
    "href": "posts/thoughts/not-into-computers/not-into-computers.html",
    "title": "there’s no such thing as not a computer person",
    "section": "",
    "text": "image generated by stable diffusion\nMy coding path has always left me inspired to talk others into learning to code themselves. It may be a little selfish, but it is with the best intentions. Programming is a beautiful thing. Having the opportunity to bring ideas to life is magic. It creates great opportunities for our future and even our current lives.\n“I’m not that into computers anyway”, is the response I get most, which is okay, I understand. In this post, I try and respond to that, from my point of view."
  },
  {
    "objectID": "posts/thoughts/not-into-computers/not-into-computers.html#relating-coding-to-a-spoken-language",
    "href": "posts/thoughts/not-into-computers/not-into-computers.html#relating-coding-to-a-spoken-language",
    "title": "there’s no such thing as not a computer person",
    "section": "relating coding to a spoken language",
    "text": "relating coding to a spoken language\nLet us start by relating coding to a commonly spoken language globally like English, although this analogy extends to any other language of your choice. This sounds weird at first but we will get to that.\nMost people learn English (again, or any other spoken Language), irrespective of their field of specialization. It helps them communicate with each other and share their ideas. Whether you are an international ambassador, a teacher, a doctor or even a security guard, no one is stopping you from learning how to speak in English. You don’t have to switch careers or anything.\nYou pick up the vocabulary relating to your area of interest and share your ideas using the specific vocabulary. For example, a doctor uses words such as cardiac arrest and pulmonary artery to speak to his patients and workmates. A teacher uses vocabulary that makes sense to his students. The same applies to lawyers or secretaries. We are all taking the general English Language and specifying how to use it in our particular domain."
  },
  {
    "objectID": "posts/thoughts/not-into-computers/not-into-computers.html#study-of-a-language-as-a-specific-domain",
    "href": "posts/thoughts/not-into-computers/not-into-computers.html#study-of-a-language-as-a-specific-domain",
    "title": "there’s no such thing as not a computer person",
    "section": "study of a language as a specific domain",
    "text": "study of a language as a specific domain\nAt the same time, some people study the English Language itself as a domain, and they are responsible for its advancement and development and make sure we use it correctly. They are concerned with creating and updating tools like dictionaries and English wikis and tutorials."
  },
  {
    "objectID": "posts/thoughts/not-into-computers/not-into-computers.html#how-it-relates-to-programming",
    "href": "posts/thoughts/not-into-computers/not-into-computers.html#how-it-relates-to-programming",
    "title": "there’s no such thing as not a computer person",
    "section": "how it relates to programming",
    "text": "how it relates to programming\nHow does this relate to programming? Well, computer code is just a language by itself, a language for computers.\nWe start with the people generally concerned with the programming domain. They are better known as software engineers. These are the scary tech people who stare at colored screens with scrolling text as they jam into their keyboards. (We honestly don’t do this, our work is much more chill and exciting.)\nThese people, in their way, ensure the continuity of programming. Some of them write the programming languages themselves, and others create tutorials and wiki’s for newbies to use to ease into programming. Some of them are creating the hardware we use in our daily lives. Most of them are developing specific software that solves particular problems. Recently, we have even gotten software 2.0 developers who do amazing stuff in Artificial Intelligence.\nThese are also the people who are considered into computers by society. And people feel like, if they aren’t these people, then they are just not into computers. Such as Rachel Thomas argues that  there’s no such thing as not a math person (which is where I got my title btw), I want to argue the same applies to computers and programming too.\nThe Coding Language isn’t limited to the nerds and software engineers, just as the English Language isn’t limited to the ‘English guys’.\nAnyone can (and should) learn how to code, because only you know how to best implement it into your domain. Only a doctor can know how to implement ideas about ‘cardiac arrest’ and ‘pulmonary arteries’ with the help of code. Who knows, you could create a program that helps you to effectively differentiate a malignant tumor from a benign tumor, to help curb cancer. A lawyer can develop a data mining system to learn all about the guy he or she is representing and learn all about the case. If you are into charity, you can develop a system where people can donate to your society from anywhere in your world and can spread knowledge to others. The security guard can develop a system to curb crime in the estate his guarding. One where people can communicate with each other during attacks, and inform the police.\nWhat I am trying to convey is, you don’t have to switch careers or ‘be that into computers’ to learn how to code. You can learn how to code and solve problems in your domain and at the same time, keep your career."
  },
  {
    "objectID": "posts/thoughts/not-into-computers/not-into-computers.html#learning-to-code",
    "href": "posts/thoughts/not-into-computers/not-into-computers.html#learning-to-code",
    "title": "there’s no such thing as not a computer person",
    "section": "learning to code",
    "text": "learning to code\nI would be lying if I told you that programming was a breeze. It has its challenges. You have to pick a language according to what you eventually want to achieve and learn its syntax and how to express ideas with it. For example, if you want to develop mobile applications, you are better off learning Kotlin than Python. On a similar note, if you want to do machine learning, then Python is currently the best language to learn.\nTherefore do your research. Do not get into coding wars. You will see them everywhere in your journey. People online arguing that one language is better than the other, that one development tool is better than the other one, and that people who use a certain IDE aren’t true programmers. Filter out the noise and focus on building cool things that change the world, or at least make the current one more habitable."
  },
  {
    "objectID": "posts/learning-from-sound/respiratory-sounds.html",
    "href": "posts/learning-from-sound/respiratory-sounds.html",
    "title": "learning from sound",
    "section": "",
    "text": "Some utility functions for this notebook\n# To be used with torchaudio\ndef print_stats(waveform, sample_rate=None, src=None):\n  if src:\n    print(\"-\" * 10)\n    print(\"Source:\", src)\n    print(\"-\" * 10)\n  if sample_rate:\n    print(\"Sample Rate:\", sample_rate)\n  print(\"Shape:\", tuple(waveform.shape))\n  print(\"Dtype:\", waveform.dtype)\n  print(f\" - Max:     {waveform.max().item():6.3f}\")\n  print(f\" - Min:     {waveform.min().item():6.3f}\")\n  print(f\" - Mean:    {waveform.mean().item():6.3f}\")\n  print(f\" - Std Dev: {waveform.std().item():6.3f}\")\n  print()\n  print(waveform)\n  print()\n\ndef plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n  waveform = waveform.numpy()\n\n  num_channels, num_frames = waveform.shape\n  time_axis = torch.arange(0, num_frames) / sample_rate\n\n  figure, axes = plt.subplots(num_channels, 1)\n  if num_channels == 1:\n    axes = [axes]\n  for c in range(num_channels):\n    axes[c].plot(time_axis, waveform[c], linewidth=1)\n    axes[c].grid(True)\n    if num_channels > 1:\n      axes[c].set_ylabel(f'Channel {c+1}')\n    if xlim:\n      axes[c].set_xlim(xlim)\n    if ylim:\n      axes[c].set_ylim(ylim)\n  figure.suptitle(title)\n  plt.show(block=False)\n\ndef plot_specgram(waveform, sample_rate, title=\"Spectrogram\", xlim=None):\n  waveform = waveform.numpy()\n\n  num_channels, num_frames = waveform.shape\n  time_axis = torch.arange(0, num_frames) / sample_rate\n\n  figure, axes = plt.subplots(num_channels, 1)\n  if num_channels == 1:\n    axes = [axes]\n  for c in range(num_channels):\n    axes[c].specgram(waveform[c], Fs=sample_rate)\n    if num_channels > 1:\n      axes[c].set_ylabel(f'Channel {c+1}')\n    if xlim:\n      axes[c].set_xlim(xlim)\n  figure.suptitle(title)\n  plt.show(block=False)\n\ndef play_audio(waveform, sample_rate):\n  waveform = waveform.numpy()\n\n  num_channels, num_frames = waveform.shape\n  if num_channels == 1:\n    display(Audio(waveform[0], rate=sample_rate))\n  elif num_channels == 2:\n    display(Audio((waveform[0], waveform[1]), rate=sample_rate))\n  else:\n    raise ValueError(\"Waveform with more than 2 channels are not supported.\")\n\ndef inspect_file(path):\n  print(\"-\" * 10)\n  print(\"Source:\", path)\n  print(\"-\" * 10)\n  print(f\" - File size: {os.path.getsize(path)} bytes\")\n  print(f\" - {torchaudio.info(path)}\")\n\ndef plot_spectrogram(spec, title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n  fig, axs = plt.subplots(1, 1)\n  axs.set_title(title or 'Spectrogram (db)')\n  axs.set_ylabel(ylabel)\n  axs.set_xlabel('frame')\n  im = axs.imshow(librosa.power_to_db(spec), origin='lower', aspect=aspect)\n  if xmax:\n    axs.set_xlim((0, xmax))\n  fig.colorbar(im, ax=axs)\n  plt.show(block=False)\nThe experimental version of this notebook can be found in this repo: Learning from Sound - Experimental\nThis notebook assumes basic knowledge about training neural networks, what a CNN is and other deep learning knowledge such as batchnorm and basic knowledge of sound represented in digital format.\nTo learn about the latter, you can go through this 6-part blog series that goes from the beginning explaining about the issue. (The first four posts will be sufficient for this notebook)"
  },
  {
    "objectID": "posts/learning-from-sound/respiratory-sounds.html#introduction",
    "href": "posts/learning-from-sound/respiratory-sounds.html#introduction",
    "title": "learning from sound",
    "section": "Introduction",
    "text": "Introduction\nRespiratory sounds are important indicators of respiratory health and respiratory disorders. The sound emitted when a person breathes is directly related to air movement, changes within lung tissue and the position of secretions within the lung. A wheezing sound, for example, is a common sign that a patient has an obstructive airway disease like asthma or chronic obstructive pulmonary disease (COPD).\nThese sounds can be recorded using digital stethoscopes and other recording techniques. This digital data opens up the possibility of using machine learning to automatically diagnose respiratory disorders like asthma, pneumonia and bronchiolitis, to name a few.\nIn this notebook, we are going to try and create a Convolutional Neural Network that can distinguish and classify different respiratory sounds and make a diagnosis. In the process, we are going to learn about how sound is represented in digital format, converting the audio files into spectrograms, which the CNN can use to learn from and a few other random things about training neural networks.\nI learned a lot from other people while making this notebook and I reference all of them at the bottom."
  },
  {
    "objectID": "posts/learning-from-sound/respiratory-sounds.html#getting-the-data",
    "href": "posts/learning-from-sound/respiratory-sounds.html#getting-the-data",
    "title": "learning from sound",
    "section": "Getting the Data",
    "text": "Getting the Data\nLuckily for us, two research teams in Portugal and Greece already prepared a suitable dataset that can be found on Kaggle. It includes 920 annotated recordings of varying length - 10s to 90s. These recordings were taken from 126 patients. There are a total of 5.5 hours of recordings containing 6898 respiratory cycles.\nWe can download the dataset using the kaggle command.\n\n!kaggle datasets download -d vbookshelf/respiratory-sound-database\n\nDownloading respiratory-sound-database.zip to /content\n100% 3.68G/3.69G [01:38<00:00, 24.1MB/s]\n100% 3.69G/3.69G [01:38<00:00, 40.2MB/s]"
  },
  {
    "objectID": "posts/learning-from-sound/respiratory-sounds.html#working-with-torchaudio",
    "href": "posts/learning-from-sound/respiratory-sounds.html#working-with-torchaudio",
    "title": "learning from sound",
    "section": "Working with torchaudio",
    "text": "Working with torchaudio\nWe are going to be using PyTorch and torchaudio in this notebook.\nLet’s create a pathlib object pointing to where our data is located:\n\ndata_path = Path('data/respiratory_sound_database/Respiratory_Sound_Database')\n\nWe can see what files are present in out data_path\n\ndata_path.ls()\n\n(#4) [Path('patient_diagnosis.csv'),Path('audio_and_txt_files'),Path('filename_format.txt'),Path('filename_differences.txt')]\n\n\nAnd get one file to use our example:\n\n(data_path/'audio_and_txt_files').ls(file_exts='.wav')[0]\n\nPath('audio_and_txt_files/138_1p2_Ar_mc_AKGC417L.wav')\n\n\n\nAUDIO_FILE = (data_path/'audio_and_txt_files').ls(file_exts='.wav')[0]\n\nLet us load that audio file using torchaudio. It returns a tuple containing the waveform and its sample rate.\n\nwaveform, sample_rate = torchaudio.load(AUDIO_FILE)\n\n\nwaveform.shape, sample_rate\n\n(torch.Size([1, 882000]), 44100)\n\n\nOur example audio file has a shape of [1, 882000] and a sample rate of 44100 kHz which is pretty common.\nOther info about the audio file can be seen using the following handy utility function:\n\nprint_stats(waveform)\n\nShape: (1, 882000)\nDtype: torch.float32\n - Max:      0.899\n - Min:     -0.623\n - Mean:     0.000\n - Std Dev:  0.112\n\ntensor([[0.0000, 0.0000, 0.0000,  ..., 0.0847, 0.0853, 0.0724]])\n\n\n\nWe can plot the waveform of the audio file:\n\nplot_waveform(waveform, sample_rate);\n\n\n\n\nAs you can see, the waveform is still a signal. A CNN expects an image-like input. So we need a way to convert the above signal to an image. A Spectrogram is a visual representation of spectrum of frequencies of a signal as it varies with time\nHere is a spectrogram of the example audio above:\n\nplot_specgram(waveform, sample_rate);\n\n/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py:7592: RuntimeWarning: divide by zero encountered in log10\n  Z = 10. * np.log10(spec)\n\n\n\n\n\nAs you can see, just and ordinary spectrogram won’t give our CNN much to learn from. Mel Spectrograms work better in this case. Converting a Spectrogram to a Mel spectogram is easy in PyTorch.\n\nn_fft = 1024\nwin_length = None\nhop_length = 512\nn_mels = 128\n\nmel_spectrogram = T.MelSpectrogram(\n    sample_rate=sample_rate,\n    n_fft=n_fft,\n    win_length=win_length,\n    hop_length=hop_length,\n    center=True,\n    pad_mode=\"reflect\",\n    power=2.0,\n    norm='slaney',\n    onesided=True,\n    n_mels=n_mels,\n    mel_scale=\"htk\",\n)\n\nmelspec = mel_spectrogram(waveform)\nplot_spectrogram(\n    melspec[0], title=\"MelSpectrogram\", ylabel='mel freq');\n\n/usr/local/lib/python3.7/dist-packages/torchaudio/functional/functional.py:433: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (513) may be set too low.\n  \"At least one mel filterbank has all zero values. \"\n\n\n\n\n\nThis is a better visual representation than ordinary spectrograms and gives our neural network something to work with.\nFinally, we can play the audio and hear the respiratory recording.\n\nplay_audio(waveform, sample_rate);\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/learning-from-sound/respiratory-sounds.html#audio-data-augmentation-specaugment-by-google",
    "href": "posts/learning-from-sound/respiratory-sounds.html#audio-data-augmentation-specaugment-by-google",
    "title": "learning from sound",
    "section": "Audio Data Augmentation (SpecAugment by Google)",
    "text": "Audio Data Augmentation (SpecAugment by Google)\nTo make training robust in Deep Learning, we usually utilize Data Augmentation which is artificially creating new data from the data we have. This also helps with regularizing the model to curb overfitting.\nBut for our data, we can’t just use the normal augmentations we use on images, like flip and rotate. Google came up with a nice augmentation specifically called SpecAugment. This involves maksing our Mel Spectograms on either the time axis (Time Masking) or along the frequency axis (Frequency Masking).\ntorchaudio makes this easier for us to implement the following transforms. Here are the corresponding outputs when we apply the specific masking:\nTime Masking\n\ntime_masking = T.TimeMasking(time_mask_param=80)\nspec = time_masking(melspec)\n\nplot_spectrogram(spec[0], title=\"Masked along time axis\")\n\n\n\n\nFrequency Masking\n\nfreq_masking = T.FrequencyMasking(freq_mask_param=80)\nspec = freq_masking(melspec)\n\nplot_spectrogram(spec[0], title=\"Masked along frequency axis\")\n\n\n\n\nFor our specific task, we are going to be utilizing both of the two transform simultaneously.\n\nspec_augment = nn.Sequential(\n    time_masking,\n    freq_masking)\n\nspec = spec_augment(melspec)\n\nplot_spectrogram(spec[0], title=\"SpecAugment\")"
  },
  {
    "objectID": "posts/learning-from-sound/respiratory-sounds.html#getting-samples-and-the-corresponding-labels",
    "href": "posts/learning-from-sound/respiratory-sounds.html#getting-samples-and-the-corresponding-labels",
    "title": "learning from sound",
    "section": "Getting samples and the corresponding labels",
    "text": "Getting samples and the corresponding labels\nNow that we know how to load our audio files and perform augmentations on them, we need a way to get the labels for each audio file.\nThe creators of the dataset provided us with a csv file that we can pop into a pandas dataframe and get the labels of each file:\n\ndf = pd.read_csv(data_path/'patient_diagnosis.csv', \n                 names=['Patient number', 'Diagnosis'])\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Patient number\n      Diagnosis\n    \n  \n  \n    \n      0\n      101\n      URTI\n    \n    \n      1\n      102\n      Healthy\n    \n    \n      2\n      103\n      Asthma\n    \n    \n      3\n      104\n      COPD\n    \n    \n      4\n      105\n      URTI\n    \n  \n\n\n\n\nHere is a distribution of the Diagnosis column:\n\nplt.figure(figsize=(10,5))\nsns.countplot(df['Diagnosis']);\n\n/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n  FutureWarning\n\n\n\n\n\n\ndf['Diagnosis'].value_counts()\n\nCOPD              64\nHealthy           26\nURTI              14\nBronchiectasis     7\nBronchiolitis      6\nPneumonia          6\nLRTI               2\nAsthma             1\nName: Diagnosis, dtype: int64\n\n\nAs with many medical datasets, we can already see massive imbalance in the data. This is a disadvantage since our model can just learn to predict the most common class and it will correct a high amount of times.We will need to fix that later before feeding this data into our model.\nTo extract the diagnosis from our audio files, we need to look again closely at the structure of our filename.\n\nAUDIO_FILE\n\nPath('audio_and_txt_files/138_1p2_Ar_mc_AKGC417L.wav')\n\n\nIt contains a lot of cryptic information that can best be understood from the description of the dataset given by the dataset creators.\nIt reads:\n\nEach audio file name is divided into 5 elements, separated with underscores (_).\n\n\nPatient number (101,102,…,226)\nRecording index\nChest location\n…\n\nWe are mostly interested in the first part of the filename, which is the Patient Number, which we can then cross-check from the data frame and get corresponding diagnosis.\nSince we know the patient number is always three digits, we can use the following code to get the diagnosis:\n\ndf[df['Patient number'] == int(AUDIO_FILE.stem[:3])]['Diagnosis'].item()\n\n'COPD'\n\n\nLet’s pop that into a function since that functionality is vital to creating our dataset.\n\ndef get_y(path):\n  return df[df['Patient number'] == int(path.stem[:3])]['Diagnosis'].item()\n\nget_y(AUDIO_FILE)\n\n'COPD'\n\n\nNow that we can get the labels, let us revisit the unbalanced dataset problem and see how bad it actually is. You see, in this dataset, we have multiple recordings for the same patient, corresponding to different chest locations. So we need to get all audio files, label them and get the exact count and distribution of our data.\nLet us create a function that takes in a list, and returns a dictionary containing the frequency of all the unique items in that list:\n\ndef CountFrequency(my_list):\n    # Creating an empty dictionary\n    freq = {}\n    for item in my_list:\n        if (item in freq):\n            freq[item] += 1\n        else:\n            freq[item] = 1\n    return freq\n\nNext, we loop through all the audio files in the list, get their labels and append the label in a list\n\ndiagnosis_list = []\nfor recording in (data_path/'audio_and_txt_files').ls(file_exts='.wav'):\n  diagnosis = df[df['Patient number'] == int(recording.stem[:3])]['Diagnosis'].item()\n  diagnosis_list.append(diagnosis)\n\n\nlen(diagnosis_list)\n\n920\n\n\nWe have a total of 920 labels, now let’s see the frequency of all the diagnosis:\n\nCountFrequency(diagnosis_list)\n\n{'Asthma': 1,\n 'Bronchiectasis': 16,\n 'Bronchiolitis': 13,\n 'COPD': 793,\n 'Healthy': 35,\n 'LRTI': 2,\n 'Pneumonia': 37,\n 'URTI': 23}\n\n\n\ncount = CountFrequency(diagnosis_list)\n\nplt.figure(figsize=(16, 6))\nsns.barplot(x=list(count.keys()), y=list(count.values()));\n\n\n\n\nCOPD is highly represented in terms of frequency, almost around 750 times Asthma and LRTI.\nTo solve this, we will need to oversample the small classes until they are at per with the highest class. We will do that while creating the Dataloader"
  },
  {
    "objectID": "posts/learning-from-sound/respiratory-sounds.html#creating-the-dataset",
    "href": "posts/learning-from-sound/respiratory-sounds.html#creating-the-dataset",
    "title": "learning from sound",
    "section": "Creating the Dataset",
    "text": "Creating the Dataset\nWe need to create a PyTorch Dataset before we deal with the imbalance problem.\nThe audio processing pipeline for Neural Networks involves a sequence of steps that can be represented as follows: * Load the audio file * Rechannel the waveform to be consistent, either as Mono (one channel) or Stereo (two channels) since our tensors have to have the same number of channels. We will convert all of them to Stereo audio files. * Resample the waveforms to a consistent sample rate since they can be sampled at different rates. We will resample them to 44100 kHz here. * Making the audio files the same size or duration. Some might be 20 seconds and others 15 seconds. This is because our model expects tensors of the same size. Resizing is accomplished by either padding smaller files or truncating longer files depending on their initial size and our target size. * Applying a Time Shift Data Augmentation of the waveform before it is converted into a visual representation. * And finally, converting the waveforms into their respective Mel Spectrogram respresentation.\nTo simplify the process, I have created the following audio utility class that does the above processes as static methods.\n\n# Audio utility function\nclass AudioUtil():\n  \"\"\"\n  This is a utility function for the following functions:\n  -------------------------------------------------------\n  * loading audio files\n  * rechanneling the audio files\n  * resampling the audio files\n  * padding or truncating the audio files\n  * Time Shift Data Augmentation\n  * Converting waveform into Mel Spectrogram\n  \"\"\"\n\n  # load audio and return signal as tensor and the sample rate\n  @staticmethod\n  def load(path):\n    waveform, sample_rate = torchaudio.load(path)\n    return (waveform, sample_rate)\n  \n  # conversion of channels (Mono to Stereo and vice versa)\n  @staticmethod\n  def rechannel(audio, new_channel):\n    waveform, sample_rate = audio\n\n    if (waveform.shape[0] == new_channel):\n      # no rechanneling needed\n      return audio\n    \n    if (new_channel==1):\n      # converting stereo to mono\n      # by selecting the first channel\n      new_waveform = waveform[:1,:]\n    elif (new_channel==2):\n      # converting mono to stereo\n      # by duplicating the first channel\n      new_waveform = torch.cat([waveform, waveform])\n    \n    return (new_waveform, sample_rate)\n  \n  # resampling\n  @staticmethod\n  def resample(audio, new_sr):\n    waveform, sr = audio\n\n    if (sr==new_sr):\n      # no resampling needed\n      return audio\n    \n    num_channels = waveform.shape[0]\n\n    # resample first channel\n    new_audio = torchaudio.transforms.Resample(sr, new_sr)(waveform[:1,:])\n    if (num_channels) > 1:\n      # resample second channel and merge the two\n      re_two = torchaudio.transforms.Resample(sr, new_sr)(waveform[1,:])\n      new_audio = torch.cat([new_audio, re_two])\n    \n    return (new_audio, new_sr)\n  \n  # resizing audio to same max length (max_ms) in milliseconds\n  @staticmethod\n  def pad_trunc(audio, max_ms):\n    waveform, sr = audio\n    num_channels, num_frames = waveform.shape\n    max_len = sr//1000 * max_ms\n\n    if (num_frames>max_len):\n      # truncate signal to given length\n      waveform = waveform[:,:max_len]\n    \n    if (num_frames<max_len):\n      # get padding lengths for beginning and end\n      begin_ln = random.randint(0, max_len-num_frames)\n      end_ln = max_len - num_frames - begin_ln\n\n      # pad the audio with zeros\n      pad_begin = torch.zeros((num_channels, begin_ln))\n      pad_end = torch.zeros((num_channels, end_ln))\n\n      waveform = torch.cat((pad_begin, waveform, pad_end), 1)\n    return (waveform, sr)\n\n  # time shift data augmentation\n  @staticmethod\n  def time_shift(audio, shift_limit):\n    waveform, sr = audio\n\n    _, num_frames = waveform.shape\n    shift_amt = int(random.random() * shift_limit * num_frames)\n    return (waveform.roll(shift_amt), sr)\n\n  # generating a Mel Spectrogram\n  @staticmethod\n  def melspectro(audio, n_mels=64, n_fft=1024, hop_len=None):\n    waveform, sr = audio\n    top_db = 80\n\n    # spec shape == (num_channels, n_mels, time)\n    spec = torchaudio.transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)(waveform)\n\n    # convert into db\n    spec = torchaudio.transforms.AmplitudeToDB(top_db=top_db)(spec)\n    \n    return spec\n\nWe also need a way to implement the SpecAugment from Google in Pytorch.\n\nclass SpecAugment(object):\n    \"\"\"Augment the spectograms based on SpecAugment from Google\n    https://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html\n\n    Args:\n        max_mask_pct: The percentange of the spectrogram to be augmented\n        n_freq_masks: The number of frequency masks to place in spectrogram\n        n_time_masks: The number of time masks to place in spectrogram\n    \"\"\"\n\n    def __init__(self, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n        self.max_mask_pct = max_mask_pct\n        self.n_freq_masks = n_freq_masks\n        self.n_time_masks = n_time_masks\n\n    def __call__(self, spec):\n        _, n_mels, n_steps = spec.shape\n        mask_value = spec.mean()\n        aug_spec = spec\n  \n        # apply the augmentation one after the other\n        # order: freq_aug -----> time_aug\n        freq_mask_param = self.max_mask_pct * n_mels\n        for _ in range(self.n_freq_masks):\n          aug_spec = torchaudio.transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n      \n        time_mask_param = self.max_mask_pct * n_steps\n        for _ in range(self.n_time_masks):\n          aug_spec = torchaudio.transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n      \n        return aug_spec\n\nNow that we have those two out of the way, we need to move on to the next step in creating our Dataset.\nOur model always expects data in numerical form, that means, we cannot just pass the labels as they are to our model. We need to represent them as numbers.\nOne solution is creating a list of all the possible labels and converting the list into a dictionary where the labels are enumerated to give them all a specific unique number:\n\nfiles = (data_path/'audio_and_txt_files/').ls(file_exts='.wav')\nlbls = files.map(get_y).unique()\nlbls\n\n(#8) ['COPD','Healthy','Bronchiectasis','Pneumonia','Bronchiolitis','URTI','Asthma','LRTI']\n\n\n\nv2i = {v:k for k,v in enumerate(lbls)}\nv2i\n\n{'Asthma': 6,\n 'Bronchiectasis': 2,\n 'Bronchiolitis': 4,\n 'COPD': 0,\n 'Healthy': 1,\n 'LRTI': 7,\n 'Pneumonia': 3,\n 'URTI': 5}\n\n\nNow we can create our Dataset. We will rechannel the audio to two channels, resmaple them to 44100 kHz, resize them to 20 seconds each or 20, 000 milliseconds and use a shift percentage of 40 percent on the Time Shift:\n\nclass RespiratoryDataset(Dataset):\n\n  def __init__(self, fns, v2i, transform):\n    self.fns = fns\n    self.v2i = v2i\n    self.duration = 20_000\n    self.sr = 44100\n    self.channel = 2\n    self.shift_pct=0.4\n    self.transform = transform\n  \n  def __len__(self):\n    return len(self.fns)\n  \n  def __getitem__(self, idx):\n    # get audio file\n    audio_file = self.fns[idx]\n    # get label\n    label = v2i[get_y(audio_file)]\n\n    # preprocess the audio file\n    # load -> resample -> rechannel -> resize -> time_shift -> convert into spec\n    # -> spec augment\n\n    aud = AudioUtil.load(audio_file)\n    resampled = AudioUtil.resample(aud, self.sr)\n    rechanneled = AudioUtil.rechannel(resampled, self.channel)\n    resized = AudioUtil.pad_trunc(rechanneled, self.duration)\n    shifted = AudioUtil.time_shift(resized, self.shift_pct)\n    sgram = AudioUtil.melspectro(shifted)\n\n    if self.transform:\n        sgram = self.transform(sgram)\n\n    return sgram, torch.tensor(label)\n\nAnd pass in our SpecAugment as our transform:\n\nspecaugment = SpecAugment(\n    max_mask_pct=0.1,\n    n_freq_masks=1,\n    n_time_masks=2\n)\n\n\ndset = RespiratoryDataset(files, v2i, transform=specaugment)\n\nWe can confirm our Dataset contains every file by checking its length:\n\nlen(dset)\n\n920\n\n\nLet’s take a sample random file from the Dataset and check its shape and the value of the label\n\nsample = dset[100]\nsample[0].shape, type(sample),sample[1]\n\n(torch.Size([2, 64, 1719]), tuple, tensor(0))\n\n\nThe shape shows that we have two channels, which makes sense since we rechanneled all our audio files to stereos which contain two channels. The other shapes result from resizing our files with either padding or truncating to 20, 000 milliseconds and the Time Shift applied\nAs for the label, we can convert it back into a readable form using the created dictionary.\n\nlbls[sample[1]]\n\n'COPD'\n\n\nWe need to split the data into training and validation sets. We randomly take the data and split 80% into the training set and the remaining 20% into the validation set.\n\nnum_items = len(dset)\nnum_train = round(num_items * 0.8)\nnum_val = num_items - num_train\ntrain_ds, val_ds = random_split(dset, [num_train, num_val])\n\n\nlen(train_ds), len(val_ds)\n\n(736, 184)"
  },
  {
    "objectID": "posts/learning-from-sound/respiratory-sounds.html#handling-the-imbalance-problem",
    "href": "posts/learning-from-sound/respiratory-sounds.html#handling-the-imbalance-problem",
    "title": "learning from sound",
    "section": "Handling the Imbalance Problem",
    "text": "Handling the Imbalance Problem\nNow we can handle the imbalance problem.\nLet has one more look on the frequencies of the diagnosis.\n\ncount = CountFrequency(diagnosis_list)\ncount\n\n{'Asthma': 1,\n 'Bronchiectasis': 16,\n 'Bronchiolitis': 13,\n 'COPD': 793,\n 'Healthy': 35,\n 'LRTI': 2,\n 'Pneumonia': 37,\n 'URTI': 23}\n\n\nWe can can convert that into a numpy array that will be better for creating our solution\n\ndata = list(count.values())\nclass_count = np.array(data)\nclass_count\n\narray([793,  35,  16,  37,  13,  23,   1,   2])\n\n\nTo solve the issue, PyTorch approaches such problems using the concepts of Samplers. Samplers provides a way for the Dataloader fetches data from the dataset by implementing __iter__ function.\nPyTorch comes with some in-built samplers and one of them will help us with our problem.\nThe WeightedRandomSampler fetches data randomly but in a weighted manner such that classes with lower frequencies are picked more frequently than classes with higher frequencies.\nTo create the sampler, we will need to pass in weights and number of samples.\nI struggled a little with implementing this Sampler but luckily found a solution in the PyTorch Forums\nTo create the weights, we will get the inverse of the class counts array, then get the labels for all the files in our training dataset and get their class weights and store them in a list\n\nclass_weights = 1./ torch.Tensor(class_count)\ntrain_targets = [sample[1] for sample in train_ds]\ntrain_samples_weight = [class_weights[class_id] for class_id in train_targets]\n\nAnd finally create the Sampler.\n\ntrain_sampler = WeightedRandomSampler(train_samples_weight, len(train_ds))"
  },
  {
    "objectID": "posts/learning-from-sound/respiratory-sounds.html#creating-the-dataloader",
    "href": "posts/learning-from-sound/respiratory-sounds.html#creating-the-dataloader",
    "title": "learning from sound",
    "section": "Creating the Dataloader",
    "text": "Creating the Dataloader\nWhen creating the training Dataloader, we will pass in the Sampler we created above and for the validation Dataloader, we will set shuffle=False which will make that Dataloader use the SequentialSampler that fetches data one after the other.\nWe create a function that returns both of this Dataloaders\n\ndef get_dls(bs, train_sampler=train_sampler):\n  train_dl = torch.utils.data.DataLoader(\n    train_ds, \n    batch_size=bs,\n    sampler=train_sampler,\n    num_workers=2,\n    pin_memory=True)\n\n  val_dl = torch.utils.data.DataLoader(\n    val_ds, \n    batch_size=bs, \n    shuffle=False,\n    num_workers=2,\n    pin_memory=True)\n  \n  return train_dl, val_dl\n\nWe can see the distribution of data in the train dataloader:\n\nfor i, (data,target) in enumerate(get_dls(32, train_sampler)[0]):\n  count=Counter(target.numpy())\n  print(f'batch {i}, {count}')\n\nbatch 0, Counter({2: 5, 1: 5, 4: 5, 3: 4, 6: 4, 0: 3, 5: 3, 7: 3})\nbatch 1, Counter({7: 6, 3: 5, 4: 4, 5: 4, 6: 4, 0: 3, 2: 3, 1: 3})\nbatch 2, Counter({1: 7, 5: 7, 2: 6, 6: 6, 0: 2, 3: 2, 4: 1, 7: 1})\nbatch 3, Counter({6: 5, 4: 5, 0: 5, 3: 4, 2: 4, 5: 3, 1: 3, 7: 3})\nbatch 4, Counter({6: 7, 7: 6, 2: 5, 3: 4, 1: 4, 4: 3, 5: 2, 0: 1})\nbatch 5, Counter({7: 7, 6: 6, 3: 4, 0: 4, 2: 3, 1: 3, 5: 3, 4: 2})\nbatch 6, Counter({6: 7, 1: 7, 0: 6, 3: 3, 5: 3, 4: 3, 7: 2, 2: 1})\nbatch 7, Counter({6: 8, 1: 7, 7: 4, 3: 4, 0: 4, 4: 3, 5: 1, 2: 1})\nbatch 8, Counter({5: 6, 1: 5, 3: 5, 7: 5, 4: 4, 6: 3, 2: 2, 0: 2})\nbatch 9, Counter({0: 7, 7: 5, 5: 5, 2: 5, 4: 3, 3: 3, 6: 2, 1: 2})\nbatch 10, Counter({1: 7, 6: 5, 0: 4, 4: 4, 5: 4, 3: 3, 2: 3, 7: 2})\nbatch 11, Counter({1: 10, 2: 6, 5: 4, 0: 3, 4: 3, 6: 2, 3: 2, 7: 2})\nbatch 12, Counter({1: 8, 5: 5, 3: 5, 0: 4, 6: 3, 2: 3, 7: 3, 4: 1})\nbatch 13, Counter({1: 6, 6: 6, 3: 5, 5: 4, 4: 3, 7: 3, 2: 3, 0: 2})\nbatch 14, Counter({4: 7, 7: 5, 3: 5, 5: 4, 6: 4, 2: 4, 1: 3})\nbatch 15, Counter({5: 7, 1: 5, 7: 4, 6: 4, 0: 4, 2: 4, 4: 2, 3: 2})\nbatch 16, Counter({7: 7, 6: 5, 0: 5, 5: 4, 1: 3, 4: 3, 3: 3, 2: 2})\nbatch 17, Counter({7: 5, 5: 5, 2: 5, 6: 4, 1: 4, 0: 4, 4: 3, 3: 2})\nbatch 18, Counter({2: 6, 3: 5, 6: 5, 7: 4, 0: 3, 1: 3, 5: 3, 4: 3})\nbatch 19, Counter({2: 8, 7: 7, 6: 5, 5: 4, 3: 3, 1: 2, 0: 2, 4: 1})\nbatch 20, Counter({3: 6, 0: 6, 7: 5, 5: 5, 6: 5, 2: 2, 4: 2, 1: 1})\nbatch 21, Counter({5: 8, 6: 6, 2: 5, 4: 4, 1: 3, 7: 3, 3: 3})\nbatch 22, Counter({4: 6, 3: 5, 7: 5, 0: 5, 2: 4, 1: 4, 6: 2, 5: 1})\n\n\nAs expected, classes with smaller frequencies get sampled more often.\nWe can also take one batch and inpsect the shapes as a sanity check\n\nbatch = next(iter(get_dls(32, train_sampler)[0]))\nbatch[0].shape\n\ntorch.Size([32, 2, 64, 1719])\n\n\nAnd plot one data item in the batch. We can see that this particular item has already been preprocessed and augmented and is ready to be passed in the model.\n\nplot_spectrogram(batch[0][0][0]);"
  },
  {
    "objectID": "posts/learning-from-sound/respiratory-sounds.html#creating-the-model",
    "href": "posts/learning-from-sound/respiratory-sounds.html#creating-the-model",
    "title": "learning from sound",
    "section": "Creating the Model",
    "text": "Creating the Model\nAs a final piece of our creation process, we can now create the model that will learn this data.\nI chose to design and train a model from scratch.\nBefore designing the model, we will create some useful functions that will make it easier to create layers in our model.\nThe conv funciton return sequential layers of a Convolutional Layer, an Activation Function (ReLU) and a Batch Normalization layer in that order.\nThe linear_classifier returns sequential layers of Batch Normalization layer, Dropout regularization layer and finally a Linear layer.\nThe AdaptiveConcatPool2d is a function I got from the fastai library that concats an Adaptive Pool and a Max Pool that gives us better results than just using one of them individually. The only catch is the result will be double of what the previous layer outputed, e.g., if our last conv block outputs 64 channels, the output from our concat pool will be 64*2=128. This is because the adaptive pool has an output of 64, and the max pool has its output of 64 and both are concated together.\n\ndef conv(ni,nf, ks=3, act=True):\n  layers = [ ]\n  layers.append(nn.Conv2d(ni, nf, kernel_size=ks, stride=2, padding=ks//2))\n  if act: layers.append(nn.ReLU())\n  layers.append(nn.BatchNorm2d(nf))\n  \n  return nn.Sequential(*layers)\n\ndef linear_classifier(nf, out):\n  layers = [ ]\n  layers.append(nn.BatchNorm1d(num_features=nf)),\n  layers.append(nn.Dropout(0.25)),\n  layers.append(nn.Linear(in_features=nf, out_features=out))\n  \n  return nn.Sequential(*layers)\n\nclass AdaptiveConcatPool2d(nn.Module):\n  \"Layer that concats `AdaptiveAvgPool2d` and `AdaptiveMaxPool2d`\"\n  def __init__(self, size=None):\n    super(AdaptiveConcatPool2d, self).__init__()\n    self.size = size or 1\n    self.ap = nn.AdaptiveAvgPool2d(self.size)\n    self.mp = nn.AdaptiveMaxPool2d(self.size)\n\n  def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1)\n\nNext, we define our custom model by subclassing the nn.Module class. We will have 4 conv blocks followed by our pooling block then three linear layers.\n\nclass Net(nn.Module):\n  \n  def __init__(self):\n    super(Net, self).__init__()\n\n    # Conv Layers\n    self.conv_layers = nn.Sequential(\n        conv(2, 8, ks=5),\n        conv(8, 16),\n        conv(16, 32),\n        conv(32, 64))\n    \n    # Adaptive Concat Pool\n    self.pool = nn.Sequential(\n        AdaptiveConcatPool2d(size=1),\n        nn.Flatten())\n    \n    # Linear Classifiers\n    # the first layer is is double the last conv layer output\n    # because our adaptive pool concats avg and max pool\n    self.lin = nn.Sequential(\n        linear_classifier(128, 256),\n        linear_classifier(256, 128),\n        linear_classifier(128, 8))\n  \n  def forward(self, x):\n    x = self.conv_layers(x)\n    x = self.pool(x)\n    x = self.lin(x)\n\n    return x\n\n\nnet = Net()\n\n\nnet\n\nNet(\n  (conv_layers): Sequential(\n    (0): Sequential(\n      (0): Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n      (1): ReLU()\n      (2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): Sequential(\n      (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (1): ReLU()\n      (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): Sequential(\n      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (1): ReLU()\n      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (3): Sequential(\n      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (1): ReLU()\n      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (pool): Sequential(\n    (0): AdaptiveConcatPool2d(\n      (ap): AdaptiveAvgPool2d(output_size=1)\n      (mp): AdaptiveMaxPool2d(output_size=1)\n    )\n    (1): Flatten(start_dim=1, end_dim=-1)\n  )\n  (lin): Sequential(\n    (0): Sequential(\n      (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (1): Dropout(p=0.25, inplace=False)\n      (2): Linear(in_features=128, out_features=256, bias=True)\n    )\n    (1): Sequential(\n      (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (1): Dropout(p=0.25, inplace=False)\n      (2): Linear(in_features=256, out_features=128, bias=True)\n    )\n    (2): Sequential(\n      (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (1): Dropout(p=0.25, inplace=False)\n      (2): Linear(in_features=128, out_features=8, bias=True)\n    )\n  )\n)"
  },
  {
    "objectID": "posts/learning-from-sound/respiratory-sounds.html#training-the-model",
    "href": "posts/learning-from-sound/respiratory-sounds.html#training-the-model",
    "title": "learning from sound",
    "section": "Training The Model",
    "text": "Training The Model\nFinally after all our pipilining and modelling process, we can train our model:\n\n#collapse\ndef train_epoch(model, optimizer, loss_fn, train_loader, scheduler=None):\n  # set the model to training mode\n  model.train()\n  running_loss = 0.0\n  for images, labels in train_loader:\n    # place the data in the GPU\n    images, labels = images.to(device), labels.to(device)\n    preds = model(images)\n    loss = loss_fn(preds, labels)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    if scheduler:\n      scheduler.step()\n\n    running_loss += loss.item()\n  \n  return running_loss / len(train_loader)\n\ndef validate_epoch(model, loss_fn, val_loader):\n  # set the model to validation mode\n  model.eval()\n  running_loss = 0.0\n  correct = 0\n  total = 0\n\n  with torch.no_grad():\n    for images, labels in val_loader:\n      # place the data in the GPU\n      images, labels = images.to(device), labels.to(device)\n      preds = model(images)\n      loss = loss_fn(preds, labels)\n\n      predicted = torch.argmax(preds, axis=1)\n      total += labels.shape[0]\n      correct += int((predicted==labels).sum())\n      running_loss += loss.item()\n  \n  return running_loss / len(val_loader), correct / total\n\n\n# The Main Training Loop\ndef training_loop(epochs,model, optimizer, loss_fn, train_loader, val_loader, \n                  scheduler=None):\n  # loop through the epochs\n  for epoch in range(1, epochs+1):\n    # forward pass + backpropagation\n    train_loss = train_epoch(model, optimizer, loss_fn, train_loader, \n                             scheduler=scheduler)\n\n    val_loss, acc = validate_epoch(model, loss_fn, val_loader)\n\n    if epoch == 1 or epoch % 2 == 0:\n      print('\\n')\n      print(f'Epoch {epoch}/{epochs}')\n      print('-' * 10)\n      print(f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n      print(f'Accuracy: {acc:.2}')\n\nWe use a batch size of 128 and train for 20 epochs\n\ntrainloader, validloader = get_dls(128)\n\nWe will use the AdamW as our optimiter and train using the one cycle learning rate method from Leslie Smith’s Superconvergence Paper that let’s us train neural networks at order of magnitude’s faster than ordinary methods. Since this is a classificaiton task, we use the Cross Entropy Loss function.\n\nnet = Net()\n\n# Initialize the weights\ndef init_weights(m):\n  if type(m) == nn.Linear or type(m) == nn.Conv2d:\n    nn.init.xavier_uniform_(m.weight)\n\nnet.apply(init_weights)\n\ndevice = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\nnet.to(device)\n\nepochs = 20\nlr = 0.001\noptimizer = optim.AdamW(net.parameters(), lr=lr)\nscheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr,\n                                          steps_per_epoch=int(len(trainloader)),\n                                          epochs=epochs,\n                                          anneal_strategy='cos')\nloss_fn = nn.CrossEntropyLoss()\n\n\ntraining_loop(\n    epochs=epochs,\n    model=net,\n    optimizer=optimizer,\n    loss_fn=loss_fn,\n    train_loader=trainloader,\n    val_loader=validloader,\n    scheduler=scheduler)\n\n\n\nEpoch 1/20\n----------\nTrain Loss: 2.7297, Val Loss: 2.3048\nAccuracy: 0.0054\n\n\nEpoch 2/20\n----------\nTrain Loss: 2.3913, Val Loss: 2.6318\nAccuracy: 0.06\n\n\nEpoch 4/20\n----------\nTrain Loss: 1.5233, Val Loss: 2.1714\nAccuracy: 0.43\n\n\nEpoch 6/20\n----------\nTrain Loss: 1.2452, Val Loss: 0.8995\nAccuracy: 0.76\n\n\nEpoch 8/20\n----------\nTrain Loss: 0.9807, Val Loss: 0.8484\nAccuracy: 0.74\n\n\nEpoch 10/20\n----------\nTrain Loss: 0.9345, Val Loss: 0.8602\nAccuracy: 0.74\n\n\nEpoch 12/20\n----------\nTrain Loss: 0.8758, Val Loss: 0.6720\nAccuracy: 0.79\n\n\nEpoch 14/20\n----------\nTrain Loss: 0.7463, Val Loss: 0.6032\nAccuracy: 0.81\n\n\nEpoch 16/20\n----------\nTrain Loss: 0.7504, Val Loss: 0.7792\nAccuracy: 0.78\n\n\nEpoch 18/20\n----------\nTrain Loss: 0.6423, Val Loss: 0.7427\nAccuracy: 0.78\n\n\nEpoch 20/20\n----------\nTrain Loss: 0.6852, Val Loss: 0.6396\nAccuracy: 0.82\n\n\nIn this notebook, we learned how to use Deep Learning to solve audio problems. It was a great learning experience for me and I am definitely going to delve more into this subfield."
  },
  {
    "objectID": "posts/learning-from-sound/respiratory-sounds.html#references",
    "href": "posts/learning-from-sound/respiratory-sounds.html#references",
    "title": "learning from sound",
    "section": "References",
    "text": "References\nUseful Kaggle Kernels:\n\nhttps://www.kaggle.com/dienhoa/healthy-lung-classification-spectrogram-fast-ai\nhttps://www.kaggle.com/craq21/pytorch-meets-audio\nhttps://www.kaggle.com/shivam316/part-1-preprocessing\nhttps://www.kaggle.com/shivam316/part-2-handel-imbalance-creating-spectrogram\nhttps://www.kaggle.com/shivam316/part-3-feature-extraction-modeling-95-acc\n\nUseful Blog Posts and Websites:\n\nAudio manipulation with torchaudio - https://pytorch.org/audio_preprocessing_tutorial\n6 part blog - https://towardsdatascience.com/audio-deep-learning-made-simple-part-1-state-of-the-art-techniques-da1d3dff2504\nfastaudio - https://fastaudio.github.io/Introduction%20to%20Audio.html"
  },
  {
    "objectID": "posts/book-recommendation/similar-books.html",
    "href": "posts/book-recommendation/similar-books.html",
    "title": "making a book recommendation system",
    "section": "",
    "text": "Turning this into an app that people can interact with:"
  },
  {
    "objectID": "posts/book-recommendation/similar-books.html#creating-the-dataloaders",
    "href": "posts/book-recommendation/similar-books.html#creating-the-dataloaders",
    "title": "making a book recommendation system",
    "section": "Creating the DataLoaders",
    "text": "Creating the DataLoaders\nSince that looks okay, we can go ahead and create our DataLoaders object that we will feed to the model. A DataLoaders is simply a convinient object that holds our training and validation data and will pass in the data to our model in mini-batches.\nWe can look up the documentation of a function by calling the doc function.\n\ndoc(CollabDataLoaders.from_df)\n\n\nCollabDataLoaders.from_df[source]CollabDataLoaders.from_df(ratings, valid_pct=0.2, user_name=None, item_name=None, rating_name=None, seed=None, path='.', bs=64, val_bs=None, shuffle_train=True, device=None)\n\nCreate a DataLoaders suitable for collaborative filtering from ratings.\nShow in docs\n\n\nThe above tells us what we will need to pass in the function in order to create our DataLoaders. Specifically we will need to pass in :\n\nThe DataFrame we are creating from (ratings)\nSize of the Validation split (20%)\nThe column that represents our user (user_id)\nThe column that represents our item (original_title)\nThe column that represents our rating (rating)\n\n\ndls = CollabDataLoaders.from_df(ratings=ratings, valid_pct=0.2, \n                                user_name='user_id', item_name='original_title',\n                                rating_name='rating')\n\nWe can look at one batch of data:\n\ndls.show_batch()\n\n\n\n  \n    \n      \n      user_id\n      original_title\n      rating\n    \n  \n  \n    \n      0\n      791\n      Sphere\n      3\n    \n    \n      1\n      543\n      Metamorphoses\n      5\n    \n    \n      2\n      1835\n      1491: New Revelations of the Americas Before Columbus\n      4\n    \n    \n      3\n      2688\n      Good Omens: The Nice and Accurate Prophecies of Agnes Nutter, Witch\n      5\n    \n    \n      4\n      264\n      Jonathan Livingston Seagull\n      5\n    \n    \n      5\n      2074\n      The Curious Incident of the Dog in the Night-Time\n      4\n    \n    \n      6\n      2136\n      The Lovely Bones\n      4\n    \n    \n      7\n      1089\n      Franny and Zooey\n      5\n    \n    \n      8\n      2334\n      Great Expectations\n      4\n    \n    \n      9\n      750\n      The History of the Hobbit, Part One: Mr. Baggins\n      2\n    \n  \n\n\n\nFor the latent factors of both our books, we will initialize a random Embedding with a size of all the books we have and the number of factors we want. The same goes for the latent factors of our users.\nThe values in these Embeddings will be updated by BackPropagation until they become meaningful.\nFor our small subset of data, we will use 50 factors. This simply means we are giving our model an allowance to represent each books in 50 different ways (e.g. action, classics etc).\nSo, we first should get the number of users and the number of books:\n\nn_books = len(dls.classes['original_title'])\nn_users = len(dls.classes['user_id'])\n\nFor learning purposes, we should also get into the habit of manually chekcking shapes of the data in our batches and see if we understand why it is so:\n\nxb, yb = dls.one_batch()\nxb.shape, yb.shape\n\n(torch.Size([64, 2]), torch.Size([64, 1]))\n\n\nWe are using a batch size of 64, so that makes sense. For our independent variable (X), we have two items there, which are the user and the book and for the dependent variable (y), we only have the rating of the book. That is why the above shapes are so."
  },
  {
    "objectID": "posts/book-recommendation/similar-books.html#creating-the-model",
    "href": "posts/book-recommendation/similar-books.html#creating-the-model",
    "title": "making a book recommendation system",
    "section": "Creating the Model",
    "text": "Creating the Model\nNext, we need to handle the model part.\n\nclass CollabFiltering(Module):\n\n  def __init__(self, n_users, n_books, n_factors, y_range=(0, 5.5)):\n    self.u_weights = Embedding(n_users, n_factors)\n    self.u_bias = Embedding(n_users, 1)\n    self.i_weights = Embedding(n_books, n_factors)\n    self.i_bias = Embedding(n_books, 1)\n    self.y_range = y_range\n  \n  def forward(self, x):\n    users = self.u_weights(x[:,0])\n    books = self.i_weights(x[:,1])\n    res = (users * books).sum(dim=1, keepdims=True)\n    res += self.u_bias(x[:,0]) + self.i_bias(x[:,1])\n    res = sigmoid_range(res, *self.y_range)\n    return res\n\nOur Model Inherits from Module which is what PyTorch expects:\nclass CollabFiltering(Module):\nThe model takes in the number of users, the number of books, the number of factors we want to use for modelling and a y_range that defaults to a tuple of (0, 5.5).\nThe purpose of the y_range is to squish our output between 0 and 5. (We choose 5.5 as the upper limit since we are going to be using sigmoid range to contain our output. And since values in sigmoid never reach the upper limit, to predict a value of 5, we are going to need to go higher).\nIt then creates the Embeddings of the users and the books. We are also going to create biases for each user and each book to capture the fact that some users generally more positive or negative in their recommendations than others and smoe books are just plain better or worse than others.\ndef __init__(self, n_users, n_books, n_factors, y_range=(0, 5.5)):\n    self.u_weights = Embedding(n_users, n_factors)\n    self.u_bias = Embedding(n_users, 1)\n    self.i_weights = Embedding(n_books, n_factors)\n    self.i_bias = Embedding(n_books, 1)\n    self.y_range = y_range\nThe forward method simply gets the factors from the user embedding and book embedding, performs dot product on them, adds the biases then uses sigmoid range to squish the output between 0 and 5 and returns the result.\ndef forward(self, x):\n    users = self.u_weights(x[:,0])\n    books = self.i_weights(x[:,1])\n    res = (users * books).sum(dim=1, keepdims=True)\n    res += self.u_bias(x[:,0]) + self.i_bias(x[:,1])\n    res = sigmoid_range(res, *self.y_range)\n    return res\nWe can now instantiate the model with 50 factors and create our Learner. We are going to use Mean Squared Error as our loss function since this is a regression problem.\n\nmodel = CollabFiltering(n_users, n_books, 50)\n\n\nlearn = Learner(dls, model, loss_func=MSELossFlat())\n\nLet us find a suitable Learning rate and train the model for 6 epochs. We use a weight decay of 0.2 to regularize our model\n\nlearn.lr_find()\n\n\n\n\nSuggestedLRs(lr_min=0.03019951581954956, lr_steep=7.585775892948732e-05)\n\n\n\n\n\n\nlearn.fit_one_cycle(6, 1e-2, wd=0.2)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.972651\n      0.988126\n      00:08\n    \n    \n      1\n      0.959149\n      0.947992\n      00:08\n    \n    \n      2\n      0.908314\n      0.913695\n      00:08\n    \n    \n      3\n      0.789891\n      0.863736\n      00:08\n    \n    \n      4\n      0.646984\n      0.828928\n      00:08\n    \n    \n      5\n      0.538308\n      0.826404\n      00:08\n    \n  \n\n\n\nWe get a final MSE of 0.826404 which is actually pretty good for out task at hand."
  },
  {
    "objectID": "posts/book-recommendation/similar-books.html#interpreting-the-embeddings",
    "href": "posts/book-recommendation/similar-books.html#interpreting-the-embeddings",
    "title": "making a book recommendation system",
    "section": "Interpreting the Embeddings",
    "text": "Interpreting the Embeddings\nLet us start by interpretting the book biases. We could have easily ranked the books by the ratings and checked the last five items but bias tells us something more interesting. These books are books that, even if a user was well matched to the latent factors of the book (e.g action, comedy etc), they still didn’t like the books and that is why the books have a low bias. In short, they are generally bad books.\nThese are the 5 books with the lowest bias from our model.\n\nbook_bias = learn.model.i_bias.weight.squeeze()\nidxs = book_bias.argsort()[:5]\n[dls.classes['original_title'][i] for i in idxs]\n\n['Trading Up',\n 'One Night @ The Call Center',\n 'Four Blondes',\n 'Lost',\n 'Eat That Frog!: 21 Great Ways to Stop Procrastinating and Get More Done in Less Time']\n\n\nSince our rating DataFrame included the GoodReads ID of each book, we can create a URL of a single book and check out the reviews from GoodReads.\nLet’s use Four Blondes as an example. We can locate its entry in our DataFrame using the following code:\n\nbooks.loc[books['original_title']=='Four Blondes']\n\n\n\n\n\n  \n    \n      \n      book_id\n      goodreads_book_id\n      authors\n      original_publication_year\n      original_title\n      average_rating\n    \n  \n  \n    \n      4008\n      4009\n      6613\n      Candace Bushnell\n      2000.0\n      Four Blondes\n      2.8\n    \n  \n\n\n\n\nThen using its goodreads_id, create a URL that we can follow to read its reviews:\n\nid = 6613\nurl = f'https://www.goodreads.com/book/show/{id}'\nurl\n\n'https://www.goodreads.com/book/show/6613'\n\n\nI haven’t read the book myself but I don’t think I will too. Reading the reviews from GoodReads looks like people didn’t really enjoy this book. Here is one extract of the reviews:\n\n\n\nFour Blondes Sample Review\n\n\nNow let us do the opposite, and check for the books with the highest book bias. This means that these are generally good books and even if you don’t enjoy such genres, there is a high chance you will enjoy this books:\n\nidxs = book_bias.argsort(descending=True)[:5]\n[dls.classes['original_title'][i] for i in idxs]\n\n['Harry Potter and the Deathly Hallows',\n 'A Thousand Splendid Suns',\n 'Where the Wild Things Are',\n 'Le Petit Prince',\n 'دیوان\\u200e\\u200e [Dīvān]']\n\n\nAt the very top is a book I enjoyed very much myself: Harry Potter and the Deathly Hallows. Since I may be biased towards that book, let us check for reviews of a book I haven’t read myself.\n\nid = int(books.loc[books['original_title']=='Le Petit Prince']['goodreads_book_id'].values)\nurl = f'https://www.goodreads.com/book/show/{id}'\nurl\n\n'https://www.goodreads.com/book/show/157993'\n\n\nI was suprised to find that ‘Le Petit Prince’ or ‘The Little Prince’ was a child-like novel which adults enjoy as well. Here is one review from GoodReads:\n\n\n\nLittle Prince Sample Review\n\n\nIt is definitely on my to-read list now.\nNext, let us try and intepret the Book Embedding Factors. Remember, we had 50 factors in our model. Well, intepreting a 50-dimensional Embedding turns out to be a difficult task for us human beings.\nLuckily we have a method called PCA which stands for Principal Component Analysis that can be used to reduce the dimensions to something that can be plotted in a graph.\nHere is the outcome:\n\ng = ratings.groupby('original_title')['rating'].count()\ntop_books = g.sort_values(ascending=False).index.values[:10000]\ntop_idxs = tensor([learn.dls.classes['original_title'].o2i[m] for m in top_books])\nbooks_w = learn.model.i_weights.weight[top_idxs].cpu().detach()\nbooks_pca = books_w.pca(3)\nfac0,fac1,fac2 = books_pca.t()\nidxs = list(range(50))\nX = fac0[idxs]\nY = fac2[idxs]\nplt.figure(figsize=(12,12))\nplt.scatter(X, Y)\nfor i, x, y in zip(top_books[idxs], X, Y):\n    plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11)\nplt.show()\n\n\n\n\nRepresentation of Books based on two strongest PCA components\n\n\n\n\nAs you can see, our model has learned for itself to seperate the books into different categories. If you look at the very top, we have the classic books, the likes of To Kill A MockingBird and The Great Gatsby. At the right side we have the likes of The Da Vinci Code and Angels and Demons which are more of Mystery Thriller Novels and are more of reality fiction novels. At the bottom we have The Hobit books which are more of fantasy novels and Mythology.\nWhat is magical is our model learned all of these on its own! We never explicitely told it what genres a particular book belongs to. We only fed it user ratings and it learned all about them itself."
  },
  {
    "objectID": "posts/book-recommendation/similar-books.html#generating-book-reviews",
    "href": "posts/book-recommendation/similar-books.html#generating-book-reviews",
    "title": "making a book recommendation system",
    "section": "Generating Book Reviews",
    "text": "Generating Book Reviews\nNow comes the interesting part. How to generate a book review for a user. It is actually a very simple concept. The intuition is two similar books should have a smaller distance between them (of the Embeddings) than two different books. So, we will require a reader to input the name of a book he or she enjoyed reading and we will generate a recommendation of books for them.\nLet us experiment with ‘The Da Vinci Code’ by Dan Brown and see the 5 most similar books to it using our model:\n\nbook_factors = learn.model.i_weights.weight\nidx = dls.classes['original_title'].o2i['The Da Vinci Code']\ndistances = nn.CosineSimilarity(dim=1)(book_factors, book_factors[idx][None])\nidx = distances.argsort(descending=True)[1:6]\n[dls.classes['original_title'][i] for i in idx]\n\n['The Sweet Far Thing',\n 'Angels & Demons ',\n 'Back Roads',\n \"What to Expect When You're Expecting\",\n 'What to Expect the First Year']\n\n\nAccording to our model, The Sweet Far Thing is the most similar book, so let us generate a URL and see the reviews:\n\nid = int(books.loc[books['original_title']=='The Sweet Far Thing']['goodreads_book_id'].values)\nurl = f'https://www.goodreads.com/book/show/{id}'\nurl\n\n'https://www.goodreads.com/book/show/127459'\n\n\nThe book description describes it as a Thriller novel which was the same case with The Da Vinci Code."
  },
  {
    "objectID": "posts/book-recommendation/similar-books.html#neural-network",
    "href": "posts/book-recommendation/similar-books.html#neural-network",
    "title": "making a book recommendation system",
    "section": "Neural Network",
    "text": "Neural Network\nNow, we will experiment with a Neural Network Model and see if it performs better than our Dot Product Model.\nTo turn our Model into a Deep Learning Model, we need to concatenate the Embeddings together and pass them through linear layers with non-linearities between them. Our Neural network will have 100 neurons and utilize a ReLU between them. Here is the code:\n\nclass CollabFilteringNN(Module):\n\n  def __init__(self, user_sz, book_sz, y_range=(0, 5.5), n_act=100):\n    self.user_factors = Embedding(*user_sz)\n    self.book_factors = Embedding(*book_sz)\n    self.layers = nn.Sequential(\n        nn.Linear(user_sz[1]+book_sz[1], n_act),\n        nn.ReLU(),\n        nn.Linear(n_act, 1)\n    )\n    self.y_range = y_range\n  \n  \n  def forward(self, x):\n    embs = self.user_factors(x[:,0]), self.book_factors(x[:,1])\n    x = self.layers(torch.cat(embs, dim=1))\n    return sigmoid_range(x, *self.y_range)\n\nWe then instantiate the neural network. We will use the get_emb_sz to get good Embedding sizes for our users and books. Remember since we will concatenate the Embeddings and not multiply them, they can now have different sizes.\n\nnn_model = CollabFilteringNN(*get_emb_sz(dls))\n\nWe can check out our model:\n\nnn_model\n\nCollabFilteringNN(\n  (user_factors): Embedding(1795, 106)\n  (book_factors): Embedding(4879, 186)\n  (layers): Sequential(\n    (0): Linear(in_features=292, out_features=100, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=100, out_features=1, bias=True)\n  )\n)\n\n\nWe will now create our Learner with the same MSE loss function, find a suitable learning rate and fit our model for 5 epochs using a regularization weight decay of 0.05:\n\nlearn = Learner(dls, nn_model, loss_func=MSELossFlat())\n\n\nlearn.lr_find()\n\n\n\n\nSuggestedLRs(lr_min=0.006918309628963471, lr_steep=3.311311274956097e-06)\n\n\n\n\n\n\nlearn = Learner(dls, nn_model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 1e-2, wd=0.05)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.775485\n      0.819169\n      00:25\n    \n    \n      1\n      0.805927\n      0.817436\n      00:29\n    \n    \n      2\n      0.777418\n      0.803896\n      00:30\n    \n    \n      3\n      0.716998\n      0.798450\n      00:26\n    \n    \n      4\n      0.652017\n      0.812329\n      00:25\n    \n  \n\n\n\nWe get roughly the same final MSE."
  },
  {
    "objectID": "posts/resnet-paper/resnet-implementation.html",
    "href": "posts/resnet-paper/resnet-implementation.html",
    "title": "deep residual learning for image recognition (2015)",
    "section": "",
    "text": "Imports\nimport os\nimport urllib\nimport urllib.request\nimport shutil\nfrom glob import glob\nimport cv2\nfrom collections import defaultdict\nfrom tqdm import tqdm\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# PyTorch stuff\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nimport torchvision\nimport torch.backends.cudnn as cudnn\n\n# Albumentations\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# fasctore\nfrom fastcore.all import *\n\n# Ignore excessive warnings\nimport logging\nlogging.propagate = False \nlogging.getLogger().setLevel(logging.ERROR)\n\ncudnn.benchmark = True"
  },
  {
    "objectID": "posts/resnet-paper/resnet-implementation.html#getting-the-data",
    "href": "posts/resnet-paper/resnet-implementation.html#getting-the-data",
    "title": "deep residual learning for image recognition (2015)",
    "section": "Getting the Data",
    "text": "Getting the Data\nFor our experiments, we are going to be using the Imagenette Dataset, created by Jeremy Howard, which is a subset of the Imagenet Dataset, but with only 10 classes and fewer images than the original.\n\nURL = 'https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz'\nFILE = 'imagenette2-320.tgz'\nFOLDER = 'data'\n\nWe create a small function that fetches the data and returns a Path object poining to where the data is:\n\ndef get_data(URL, FILE, FOLDER):\n  # This is a function that downloads and extracts the data\n  # then returns a pathlib object containing the location of the data\n\n  # Downloading\n  if not os.path.isfile(FILE):\n    print(f'Downloading {URL} and saving it as {FILE}')\n    print('-'*120)\n    urllib.request.urlretrieve(URL, FILE)\n    print('Finished Downloading')\n  else:\n    print(f'{FILE} already exists')\n  \n  # Extracting\n  print('\\n')\n  print(f'Extracting Files into {FOLDER}')\n  shutil.unpack_archive(FILE, FOLDER)\n  return Path(FOLDER)\n\n\npath = get_data(URL, FILE, FOLDER)\n\nDownloading https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz and saving it as imagenette2-320.tgz\n------------------------------------------------------------------------------------------------------------------------\nFinished Downloading\n\n\nExtracting Files into data\n\n\nWe are using the slightly modified Path object from fastcore which has useful functions, like ls which prints the files and folders present in our path. You can learn more about fastcore here and its functionalities.\n\n(path/'imagenette2-320').ls()\n\n(#3) [Path('imagenette2-320/val'),Path('imagenette2-320/train'),Path('imagenette2-320/noisy_imagenette.csv')]\n\n\nWe can look inside the training folder to see what we are working with:\n\n(path/'imagenette2-320'/'train').ls()\n\n(#10) [Path('imagenette2-320/train/n03888257'),Path('imagenette2-320/train/n02102040'),Path('imagenette2-320/train/n03028079'),Path('imagenette2-320/train/n03000684'),Path('imagenette2-320/train/n02979186'),Path('imagenette2-320/train/n03394916'),Path('imagenette2-320/train/n03425413'),Path('imagenette2-320/train/n03417042'),Path('imagenette2-320/train/n01440764'),Path('imagenette2-320/train/n03445777')]\n\n\nNow, we will recursively go through all the folders and create a list of all JPEG files:\n\nfiles = L(glob(f'{path}/**/*.JPEG', recursive=True)).map(Path)\nfiles[0]\n\nPath('imagenette2-320/val/n03888257/n03888257_19462.JPEG')\n\n\n\nlen(files)\n\n13394\n\n\nWe can see that we have around 13,000 images to work with, which is reasonable for our task at hand.\nAnother useful item that we are going to require for modelling purposes is the labels of all the images. Our images have been arranged folder-wise, where each folder represent a different class:\n\nlbls = files.map(Self.parent.name()).unique()\nlbls\n\n(#10) ['n03888257','n02102040','n03028079','n03000684','n02979186','n03394916','n03425413','n03417042','n01440764','n03445777']\n\n\nOur Neural Network expects numerical input so we need to convert those labels to numbers:\n\nv2i = lbls.val2idx()\nv2i\n\n{'n01440764': 8,\n 'n02102040': 1,\n 'n02979186': 4,\n 'n03000684': 3,\n 'n03028079': 2,\n 'n03394916': 5,\n 'n03417042': 7,\n 'n03425413': 6,\n 'n03445777': 9,\n 'n03888257': 0}\n\n\nNow we can create our Dataset, specify some transforms we need, split the data into training and validation and create a function that returns our Dataloaders:\n\nclass Imagenette(Dataset):\n  def __init__(self, files, v2i, transform=None):\n    self.files = files\n    self.v2i = v2i\n    self.transform = transform\n  \n  def __len__(self):\n    return len(self.files)\n  \n  def __getitem__(self, idx):\n    image_filepath = self.files[idx]\n    image = cv2.imread(f'{image_filepath}')\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    label = self.v2i[self.files[idx].parent.name]\n\n    # transform if available\n    if self.transform is not None:\n      image = self.transform(image=image)[\"image\"]\n    \n    return image, label\n\n\ntrain_transform = A.Compose(\n    [\n        A.SmallestMaxSize(max_size=160),\n        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n        A.RandomCrop(height=128, width=128),\n        A.Rotate(limit=40, p=0.9, border_mode=cv2.BORDER_CONSTANT),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.1),\n        A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n        A.RandomBrightnessContrast(p=0.5),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2(),\n    ]\n)\n\nval_transform = A.Compose(\n    [\n        A.SmallestMaxSize(max_size=160),\n        A.CenterCrop(height=128, width=128),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2(),\n    ]\n)\n\n\ntrain_filt = L(o.parent.parent.name=='train' for o in files)\ntrain,valid = files[train_filt],files[~train_filt]\nlen(train),len(valid)\n\n(9469, 3925)\n\n\n\ntrain_ds = Imagenette(train, v2i, train_transform)\nvalid_ds = Imagenette(valid, v2i, val_transform)\nx,y = train_ds[1000]\nx.shape,y\n\n(torch.Size([3, 128, 128]), 1)\n\n\n\nimshow(x, title=y);\n\n\n\n\n\ndef get_dls(bs):\n  train_dl = DataLoader(\n    train_ds, \n    batch_size=bs,\n    shuffle=True,\n    num_workers=2,\n    pin_memory=True)\n\n  val_dl = DataLoader(\n    valid_ds, \n    batch_size=bs, \n    shuffle=False,\n    num_workers=2,\n    pin_memory=True)\n  \n  return train_dl, val_dl\n\nWe can now explore the paper since we have data preparation out of our way."
  },
  {
    "objectID": "posts/resnet-paper/resnet-implementation.html#base-vanilla-model",
    "href": "posts/resnet-paper/resnet-implementation.html#base-vanilla-model",
    "title": "deep residual learning for image recognition (2015)",
    "section": "Base Vanilla Model",
    "text": "Base Vanilla Model\nWe are going to start with a small base model, which will just be a vanilla model of Conv Layers stacked onto each other.\nWe specify a function that takes in number of inputs, number of features, kernel size and the stride and returns a sequence of a Conv Layer, a BatchNorm Layer and finally the activation layer, which is going to be Relu. This is a typical sequence in Deep Learning.\n\ndef ConvLayer(ni, nf, ks, stride):\n  layers = nn.Sequential(\n      nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=False),\n      nn.BatchNorm2d(nf),\n      nn.ReLU())\n  return layers\n\nAnd now define what a block will look like in our model. Note we go for a default kernel of size 3 and stride 2.\n\ndef block(ni, nf, ks=3, stride=2):\n  return ConvLayer(ni, nf, ks, stride)\n\nWe now create our Small Network that will serve as our Base Model. It is a bunch of Conv Layers followed by a Global Average Pool and finally a Linear Classifier that outputs 10 activations:\n\nclass SmallNetwork(nn.Module):\n  def __init__(self):\n    super(SmallNetwork, self).__init__()\n    self.convs = nn.Sequential(\n        block(3, 16, ks=5),\n        block(16, 32),\n        block(32, 64),\n        block(64, 128),\n        block(128, 256))\n    self.pool = nn.AdaptiveAvgPool2d(1)\n    self.linear = nn.Linear(256, len(v2i))\n  \n  def forward(self, x):\n    out = self.convs(x)\n    out = self.pool(out)\n    out = nn.Flatten()(out)\n    out = self.linear(out)\n\n    return out\n\n\n\nCode\ndef calculate_accuracy(preds, target):\n  correct = 0\n  total = 0\n\n  predicted = torch.argmax(preds, axis=1)\n  total += target.shape[0]\n  correct += int((predicted==target).sum())\n\n  return correct / total\n\nclass MetricMonitor:\n    def __init__(self, float_precision=3):\n        self.float_precision = float_precision\n        self.reset()\n\n    def reset(self):\n        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n\n    def update(self, metric_name, val):\n        metric = self.metrics[metric_name]\n\n        metric[\"val\"] += val\n        metric[\"count\"] += 1\n        metric[\"avg\"] = metric[\"val\"] / metric[\"count\"]\n\n    def __str__(self):\n        return \" | \".join(\n            [\n                \"{metric_name}: {avg:.{float_precision}f}\".format(\n                    metric_name=metric_name, avg=metric[\"avg\"], float_precision=self.float_precision\n                )\n                for (metric_name, metric) in self.metrics.items()\n            ]\n        )\n\ndef init_weights(m):\n  if type(m) == nn.Linear or type(m) == nn.Conv2d:\n    nn.init.xavier_uniform_(m.weight)\n\n\nWe define our training and validation loops:\n\ndef train(train_loader, model, criterion, optimizer, epoch, scheduler=None):\n    metric_monitor = MetricMonitor()\n    model.train()\n    stream = tqdm(train_dl)\n    for i, (images, target) in enumerate(stream, start=1):\n        images = images.to(device, non_blocking=True)\n        target = target.to(device, non_blocking=True)\n        output = model(images)\n        loss = criterion(output, target)\n        accuracy = calculate_accuracy(output, target)\n        metric_monitor.update(\"Loss\", loss.item())\n        metric_monitor.update(\"Accuracy\", accuracy)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        if scheduler:\n          scheduler.step()\n        stream.set_description(\n            \"Epoch: {epoch}. Train       {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n        )\n\n\ndef validate(val_loader, model, criterion, epoch):\n    metric_monitor = MetricMonitor()\n    model.eval()\n    stream = tqdm(valid_dl)\n    with torch.no_grad():\n        for i, (images, target) in enumerate(stream, start=1):\n            images = images.to(device, non_blocking=True)\n            target = target.to(device, non_blocking=True)\n            output = model(images)\n            loss = criterion(output, target)\n            accuracy = calculate_accuracy(output, target)\n\n            metric_monitor.update(\"Loss\", loss.item())\n            metric_monitor.update(\"Accuracy\", accuracy)\n            stream.set_description(\n                \"Epoch: {epoch}. Validation  {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n            )\n\nWe use a batch size of 64 for our experiments, and train our network for 10 epoch using a learning rate of 0.003\n\ntrain_dl, valid_dl = get_dls(64)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nnum_epochs=10\nlr = 3e-3\n\nsm_model = SmallNetwork()\nsm_model = sm_model.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(sm_model.parameters(), lr=lr)\nscheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr,\n                                          steps_per_epoch=int(len(train_dl)),\n                                          epochs=num_epochs,\n                                          anneal_strategy='linear')\n\nsm_model.apply(init_weights);\n\n\nfor epoch in range(1, num_epochs + 1):\n  train(train_dl, sm_model, criterion, optimizer, epoch, scheduler)\n  validate(valid_dl, sm_model, criterion, epoch)\n  print()\n\nEpoch: 1. Train       Loss: 1.925 | Accuracy: 0.328: 100%|██████████| 148/148 [00:30<00:00,  4.80it/s]\nEpoch: 1. Validation  Loss: 1.771 | Accuracy: 0.394: 100%|██████████| 62/62 [00:10<00:00,  6.02it/s]\n\n\n\n\n\nEpoch: 2. Train       Loss: 1.725 | Accuracy: 0.409: 100%|██████████| 148/148 [00:29<00:00,  4.94it/s]\nEpoch: 2. Validation  Loss: 1.639 | Accuracy: 0.473: 100%|██████████| 62/62 [00:10<00:00,  6.09it/s]\n\n\n\n\n\nEpoch: 3. Train       Loss: 1.641 | Accuracy: 0.441: 100%|██████████| 148/148 [00:30<00:00,  4.91it/s]\nEpoch: 3. Validation  Loss: 1.589 | Accuracy: 0.496: 100%|██████████| 62/62 [00:10<00:00,  6.11it/s]\n\n\n\n\n\nEpoch: 4. Train       Loss: 1.534 | Accuracy: 0.487: 100%|██████████| 148/148 [00:30<00:00,  4.88it/s]\nEpoch: 4. Validation  Loss: 1.278 | Accuracy: 0.587: 100%|██████████| 62/62 [00:10<00:00,  6.10it/s]\n\n\n\n\n\nEpoch: 5. Train       Loss: 1.428 | Accuracy: 0.526: 100%|██████████| 148/148 [00:30<00:00,  4.92it/s]\nEpoch: 5. Validation  Loss: 1.295 | Accuracy: 0.582: 100%|██████████| 62/62 [00:10<00:00,  6.07it/s]\n\n\n\n\n\nEpoch: 6. Train       Loss: 1.325 | Accuracy: 0.562: 100%|██████████| 148/148 [00:29<00:00,  4.96it/s]\nEpoch: 6. Validation  Loss: 1.487 | Accuracy: 0.547: 100%|██████████| 62/62 [00:10<00:00,  6.12it/s]\n\n\n\n\n\nEpoch: 7. Train       Loss: 1.244 | Accuracy: 0.592: 100%|██████████| 148/148 [00:30<00:00,  4.91it/s]\nEpoch: 7. Validation  Loss: 1.118 | Accuracy: 0.642: 100%|██████████| 62/62 [00:10<00:00,  6.04it/s]\n\n\n\n\n\nEpoch: 8. Train       Loss: 1.185 | Accuracy: 0.611: 100%|██████████| 148/148 [00:30<00:00,  4.90it/s]\nEpoch: 8. Validation  Loss: 1.056 | Accuracy: 0.661: 100%|██████████| 62/62 [00:10<00:00,  6.07it/s]\n\n\n\n\n\nEpoch: 9. Train       Loss: 1.114 | Accuracy: 0.634: 100%|██████████| 148/148 [00:30<00:00,  4.87it/s]\nEpoch: 9. Validation  Loss: 0.968 | Accuracy: 0.694: 100%|██████████| 62/62 [00:10<00:00,  6.02it/s]\n\n\n\n\n\nEpoch: 10. Train       Loss: 1.066 | Accuracy: 0.657: 100%|██████████| 148/148 [00:30<00:00,  4.89it/s]\nEpoch: 10. Validation  Loss: 0.895 | Accuracy: 0.714: 100%|██████████| 62/62 [00:10<00:00,  6.05it/s]\n\n\n\n\n\n\n\n\nAfter the 10 epochs, we get a Training Loss of about 1.09 and a Validation Loss of 0.89 which yields an Accuracy of about 72%.\nNow we can start performing the experiments from the paper."
  },
  {
    "objectID": "posts/resnet-paper/resnet-implementation.html#is-learning-better-networks-as-easy-as-stacking-more-layers",
    "href": "posts/resnet-paper/resnet-implementation.html#is-learning-better-networks-as-easy-as-stacking-more-layers",
    "title": "deep residual learning for image recognition (2015)",
    "section": "Is Learning Better Networks as easy as stacking more layers?",
    "text": "Is Learning Better Networks as easy as stacking more layers?\nThis was a question the researchers were very interested in, in their paper. Evidence had revealved that network depth was of crucual importance as the previous leading results on the Imagenet Challenge all exploited “very deep networks”. Driven by this understanding, a natural question arose, Is learning better networks as easy as stacking more layers?\nQuoting their paper > When deeper networks are able to start converging, a degradation problem has been exposed: with the network depth increasing, accuracy gets saturated and then degrades rapidly. Unexpectedly, such degradation is not caused by overfitting, and adding more layers to a suitably deep model leads to higher training error as shown below.\n\n\n\nimage.png\n\n\nLet us try and understand that paragraph. The researchers were wondering whether stacking more layers onto each other which yields deeper networks gave better models. Past evidence has already shown how important deep models could be, but was deep models as easy as just stacking more layers?\nTo investigate that phenomenal, the researchers perfomed an experiment using two networks. One with 20 layers and one substantially deeper with 56 layers. Surprisingly, the 20-layer network achieved both a lower training error and test error. This means that the 20-layer network was better than the 56-layer network. Hence, the deeper network experienced what the researchers called degredation.\nFurthermore, the degradation wasn’t caused by overfitting the training data. If this was the case, we would observe the deeper network having a lower training error than the shallow one, then having a higher test error than the shallow one since it would have memorized the data instead of learning from it (as deeper models often do).\nBut in their experiments, they observed that the shallower network had both lower training and test errors. This meant that the deeper counterparts weren’t really learning the data.\nThe degradation observed here implies that not all systems are similary easy to optimize.\nLet us experimentally try to see the degradation problem for ourself. To do this, we will modify our block function to return 3 Conv Layers instead of one, that way, we will end up with a model that is 3 times deeper than our base model:\n\ndef block(ni, nf, ks=3, stride=2):\n  return nn.Sequential(\n      ConvLayer(ni, nf, ks, stride),\n      ConvLayer(nf, nf, ks, stride),\n      ConvLayer(nf, nf, ks, stride)\n  )\n\n\nclass LargerNetwork(nn.Module):\n  def __init__(self):\n    super(LargerNetwork, self).__init__()\n    self.convs = nn.Sequential(\n        block(3, 16, ks=5),\n        block(16, 32),\n        block(32, 64),\n        block(64, 128),\n        block(128, 256))\n    self.pool = nn.AdaptiveAvgPool2d(1)\n    self.linear = nn.Linear(256, len(v2i))\n  \n  def forward(self, x):\n    out = self.convs(x)\n    out = self.pool(out)\n    out = nn.Flatten()(out)\n    out = self.linear(out)\n\n    return out\n\nWe train the model for the same amount of epochs using the same hyperparameters:\n\nnum_epochs=10\nlr = 3e-3\n\nlg_model = LargerNetwork()\nlg_model = lg_model.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(lg_model.parameters(), lr=lr)\nscheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr,\n                                          steps_per_epoch=int(len(train_dl)),\n                                          epochs=num_epochs,\n                                          anneal_strategy='linear')\n\nlg_model.apply(init_weights);\n\n\nfor epoch in range(1, num_epochs + 1):\n  train(train_dl, lg_model, criterion, optimizer, epoch, scheduler)\n  validate(valid_dl, lg_model, criterion, epoch)\n  print()\n\nEpoch: 1. Train       Loss: 2.347 | Accuracy: 0.149: 100%|██████████| 148/148 [00:31<00:00,  4.69it/s]\nEpoch: 1. Validation  Loss: 2.159 | Accuracy: 0.195: 100%|██████████| 62/62 [00:10<00:00,  6.04it/s]\n\n\n\n\n\nEpoch: 2. Train       Loss: 2.130 | Accuracy: 0.225: 100%|██████████| 148/148 [00:31<00:00,  4.76it/s]\nEpoch: 2. Validation  Loss: 2.056 | Accuracy: 0.275: 100%|██████████| 62/62 [00:10<00:00,  5.96it/s]\n\n\n\n\n\nEpoch: 3. Train       Loss: 2.065 | Accuracy: 0.263: 100%|██████████| 148/148 [00:31<00:00,  4.74it/s]\nEpoch: 3. Validation  Loss: 2.079 | Accuracy: 0.255: 100%|██████████| 62/62 [00:10<00:00,  6.09it/s]\n\n\n\n\n\nEpoch: 4. Train       Loss: 2.001 | Accuracy: 0.297: 100%|██████████| 148/148 [00:31<00:00,  4.76it/s]\nEpoch: 4. Validation  Loss: 1.917 | Accuracy: 0.334: 100%|██████████| 62/62 [00:10<00:00,  6.01it/s]\n\n\n\n\n\nEpoch: 5. Train       Loss: 1.940 | Accuracy: 0.319: 100%|██████████| 148/148 [00:30<00:00,  4.79it/s]\nEpoch: 5. Validation  Loss: 1.865 | Accuracy: 0.355: 100%|██████████| 62/62 [00:10<00:00,  6.05it/s]\n\n\n\n\n\nEpoch: 6. Train       Loss: 1.877 | Accuracy: 0.350: 100%|██████████| 148/148 [00:31<00:00,  4.77it/s]\nEpoch: 6. Validation  Loss: 1.896 | Accuracy: 0.352: 100%|██████████| 62/62 [00:10<00:00,  6.02it/s]\n\n\n\n\n\nEpoch: 7. Train       Loss: 1.833 | Accuracy: 0.360: 100%|██████████| 148/148 [00:30<00:00,  4.80it/s]\nEpoch: 7. Validation  Loss: 1.629 | Accuracy: 0.433: 100%|██████████| 62/62 [00:10<00:00,  6.07it/s]\n\n\n\n\n\nEpoch: 8. Train       Loss: 1.768 | Accuracy: 0.391: 100%|██████████| 148/148 [00:30<00:00,  4.78it/s]\nEpoch: 8. Validation  Loss: 1.618 | Accuracy: 0.451: 100%|██████████| 62/62 [00:10<00:00,  6.01it/s]\n\n\n\n\n\nEpoch: 9. Train       Loss: 1.711 | Accuracy: 0.408: 100%|██████████| 148/148 [00:31<00:00,  4.73it/s]\nEpoch: 9. Validation  Loss: 1.559 | Accuracy: 0.479: 100%|██████████| 62/62 [00:10<00:00,  5.97it/s]\n\n\n\n\n\nEpoch: 10. Train       Loss: 1.683 | Accuracy: 0.418: 100%|██████████| 148/148 [00:31<00:00,  4.66it/s]\nEpoch: 10. Validation  Loss: 1.562 | Accuracy: 0.471: 100%|██████████| 62/62 [00:10<00:00,  5.95it/s]\n\n\n\n\n\n\n\n\nAfter the 10 epochs, we prove what the paper was claiming. Stacking Layers onto each other doesn’t work. Adding more layers to a suitably deep model leads to a higher training error and a higher test error.\nTherefore, the question then becomes, what then constitues of a better model if just plainly stacking layers won’t work?"
  },
  {
    "objectID": "posts/resnet-paper/resnet-implementation.html#what-about-identity-mappings",
    "href": "posts/resnet-paper/resnet-implementation.html#what-about-identity-mappings",
    "title": "deep residual learning for image recognition (2015)",
    "section": "What about Identity Mappings?",
    "text": "What about Identity Mappings?\nThe researchers went on and investigated\n\nLet us consider a shallower architecture and its deeper counterpart that adds more layers onto it. There exists a solution by construction to the deeper model: the added layers are identity mappings, and the other layers are copied from the learned shallower model. The existence of this constructed solution indicates that a deeper model should produce no higher training error than its shallower counterpart.\n\nTo put that in more English like language, here is what they mean: Train a shallow model first, say 20 layers. Then after, make this shallow model deeper by adding more layers onto it, say 30 layers. The only catch is, the layers that are added have to be Identity mappings, which simply means they do nothing. They simply get an input and output the same input. So they just sort of pass the input around.\nThink about this new form of deeper model. The model is now 50 layers deep, but the first 20 layers are similar to our shallow model, and the added 30 layers do absolutely nothing to the inputs they recieve. Therefore, our 50 layer model should produce the exact same training and test errors as the 20 layer model.\nLet us see that in code.\nPyTorch provides us with an Identity Layer that works as follows, the output is exactly the same as the input provided.\n\nx = torch.randn(1, 3, 64, 64)\nm = nn.Identity()\nm(x).shape\n\ntorch.Size([1, 3, 64, 64])\n\n\nNow we create a function that returns 30 of this Identity Layers.\n\ndef identity_layers():\n  layers = []\n  for _ in range(30):\n    layers.append(nn.Identity())\n\n  return nn.Sequential(*layers).to(device)\n\nAnd now we append them to the Base Model we had trained earlier and validate it on our data.\n\nmodel_identity = nn.Sequential(\n    sm_model,\n    identity_layers()\n)\n\n\nvalidate(valid_dl, model_identity, criterion, 1)\n\nEpoch: 1. Validation  Loss: 0.895 | Accuracy: 0.714: 100%|██████████| 62/62 [00:10<00:00,  5.95it/s]\n\n\nWe get the same validation loss and accuracy as our base model since the Identity Layers don’t do anything, proving that there are always deep networks that should be at least as good as any shallow network. But for some reason, SGD does not seem able to find them (or unable to do so in feasible time). This again proves that not all systems are easy to optimize."
  },
  {
    "objectID": "posts/resnet-paper/resnet-implementation.html#residual-learning",
    "href": "posts/resnet-paper/resnet-implementation.html#residual-learning",
    "title": "deep residual learning for image recognition (2015)",
    "section": "Residual Learning",
    "text": "Residual Learning\nSince stacking layers didn’t work, although theoretically they should, the researchers proposed to address the degradation problem of deep networks using a deep residual learning framework.\n\nInstead of hoping each few stacked layers directly fit a desired underlying mapping, we explicitely let these layers fit a residual mapping.\n\n\n\n\nimage.png\n\n\nAgain, let us try explaining that in an understandable manner. Above is the Residual Block that the researchers proposed.\nAssume that the output of this block (after the relu) is supposed to be \\[y\\] and the residual block (the sequence of weight layers) learns the function \\[F(x)\\]\nWe see that the input is added just before the relu, so the whole equation becomes: \\[y = F(x) + x\\]\nThis function can be written in another format, making F(x) the subject of the formula: \\[F(x) = y - x\\] which is the residual mapping that the weight layers learn.\nIn other words, If the outcome of a given layer is supposed to be y, we’re not asking the block to predict y, we are asking it to predict the difference between the desired outcome, y and the input x. Or in short, what should the layer do its input x to make it like the desired output y.\nThe job of the layer now isn’t to predict certain features, but to minimize the error between x and the desired y. This proves to be an easier task, that asking our block to just predict a desired outcome y out of nowhere. This is achievied using skip connections (how the input is added after passing through the weight layers just before the activation - or in other terms, how it skips throught the connection).\n\nIntuition: This is my intuition of the Residual Learning Framework. Giving the layer something to work with makes it easier for it to learn. Imagine walking into a barber shop or a salon. Now imagine telling the attendant to give you a haircut and it must be meaningful. Hypothetically, the attendant can give you a meaningful haircut (it is possible) but it will be hard to do that out of nowhere (it might not be that feasible). Imagine instead if you came in with a picture of what you want and gave it to him or her. Now it will be easier for the barber or hair salon to make your hair (the input x) similar to the picture (the output y). Or in other words, the attendant will now find it easier to minimize the error between the input and the desired output.\n\nLet us see this in code format, which is always easier.\nWe change our definition of ConvLayer to accomodate the skip connection. Now our layer will return two different things depending on whether the layer is a normal weight layer (the first weight layer in the picture above) which includes a Convolution, followed by a BatchNorm then Relu or an identity layer (the second weight layer without an activation function) which consists of a Convolution, followed by BatchNorm, whose gamma value is initialized to zero. Recall that a BatcNorm layer does gamma*x + beta. Therefore, passing through this second layer won’t do anything because x + conv(x) will just result in x, acting as an identity mapping.\n\ndef ConvLayer(ni, nf, ks, stride, identity=False):\n  layers = []\n  layers.append(nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, \n                          bias=False))\n  norm = nn.BatchNorm2d(nf)\n  # if identity conv block, init gamma to zero and no activation\n  if identity == True:\n    nn.init.zeros_(norm.weight)\n    layers.append(norm)\n    return nn.Sequential(*layers)\n  # normal conv block\n  else:\n    layers.append(norm)\n    layers.append(nn.ReLU())\n    return nn.Sequential(*layers)\n\nWe can test out a few setups for intuiton. First, we return a normal ConvLayer (not an identity):\n\nConvLayer(3, 32, 3, 2, identity=False)\n\nSequential(\n  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (2): ReLU()\n)\n\n\nAnd then an identity ConvLayer:\n\nConvLayer(3, 32, 3, 2, identity=True)\n\nSequential(\n  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)\n\n\nNow, according to the image above, a ResNet Block will always consist of a normal weight layer followed by an identity weight layer so we can create a function that return that block of code:\n\ndef _conv_block(ni, nf, ks, stride):\n  return nn.Sequential(\n      ConvLayer(ni, nf, ks, stride),\n      ConvLayer(nf, nf, ks, stride=1, identity=True)\n  )\n\nAgain, we can test it out to get an intuition of how it works:\n\n_conv_block(3, 32, 3, 2)\n\nSequential(\n  (0): Sequential(\n    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n  )\n  (1): Sequential(\n    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n)\n\n\n\n\nCode\ndef noop(x=None):\n  return x\n\n\nFinally, we can create the logic of the ResNet block of code. The forward loop will be equal to x + _conv_block(x) then passed through the ReLU, just like the picture we had above\nBut we should also handle for two cases that may arise:\n\nIf the number of inputs (ni) is different from the number of features or channels outputted (nf), we are going to have a problem during the adding because the input, x and the output of the _conv_block will have different shapes. Therefore, we will need to change the channels of the inputs using a 1x1 Convolution which we will call idconv. To learn more about 1x1 Convolutions and how they can be used to change the number of channels, read this.\nIf we use a stride greater than 1 on the convolutions, their output maps are going to be halved, again resulting in a different shape than the input, therefore they cannot be added together. Therefore, we will need to reduce the feature map of the input too using and Average Pool with stride 2.\n\nNote, noop here means no operation, meaning nothing is done on the input if ni==nf and the stide==1, otherwise, the above situations are handled appropriately.\n\nclass ResBlock(nn.Module):\n  def __init__(self, ni, nf, ks=3, stride=2):\n    super().__init__()\n    self.convs = _conv_block(ni, nf, ks, stride)\n    self.idconv = noop if ni==nf else nn.Conv2d(ni, nf, kernel_size=1)\n    self.pool = noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True)\n  \n  def forward(self, x):\n    return F.relu(self.convs(x) + self.idconv(self.pool(x)))\n\nWe will now modify the block function we used to create the large model, and let it return Residual Blocks instead of just stacking ordinary layers on top of each other.\n\ndef block(ni, nf, ks=3, stride=2):\n  return nn.Sequential(\n      ResBlock(ni, nf, ks, stride),\n      ResBlock(nf, nf, ks, stride),\n      ResBlock(nf, nf, ks, stride)\n  )\n\nWe then define a Network that is as deep as our LargerNetwork, but with residual blocks instead. Other than that, our ResidualNetwork is as deep as our previous model that performed poorly. That way, we can compare both of them and see the direct benefit of using what the researchers called the deep residual learning framework.\n\nclass ResidualNetwork(nn.Module):\n  def __init__(self):\n    super(ResidualNetwork, self).__init__()\n    self.convs = nn.Sequential(\n        block(3, 16, ks=5),\n        block(16, 32),\n        block(32, 64),\n        block(64, 128),\n        block(128, 256))\n    self.pool = nn.AdaptiveAvgPool2d(1)\n    self.linear = nn.Linear(256, len(v2i))\n  \n  def forward(self, x):\n    out = self.convs(x)\n    out = self.pool(out)\n    out = nn.Flatten()(out)\n    out = self.linear(out)\n\n    return out\n\nAgain, we train for the same number of epochs using the same hyperparameters:\n\nnum_epochs=10\nlr = 3e-3\n\nrs_model = ResidualNetwork()\nrs_model = rs_model.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(rs_model.parameters(), lr=lr)\nscheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr,\n                                          steps_per_epoch=int(len(train_dl)),\n                                          epochs=num_epochs,\n                                          anneal_strategy='linear')\n\nrs_model.apply(init_weights);\n\n\nfor epoch in range(1, num_epochs + 1):\n  train(train_dl, rs_model, criterion, optimizer, epoch, scheduler)\n  validate(valid_dl, rs_model, criterion, epoch)\n  print()\n\nEpoch: 1. Train       Loss: 2.104 | Accuracy: 0.252: 100%|██████████| 148/148 [00:34<00:00,  4.25it/s]\nEpoch: 1. Validation  Loss: 1.946 | Accuracy: 0.316: 100%|██████████| 62/62 [00:10<00:00,  5.80it/s]\n\n\n\n\n\nEpoch: 2. Train       Loss: 1.905 | Accuracy: 0.329: 100%|██████████| 148/148 [00:33<00:00,  4.37it/s]\nEpoch: 2. Validation  Loss: 1.754 | Accuracy: 0.382: 100%|██████████| 62/62 [00:10<00:00,  5.92it/s]\n\n\n\n\n\nEpoch: 3. Train       Loss: 1.842 | Accuracy: 0.357: 100%|██████████| 148/148 [00:33<00:00,  4.46it/s]\nEpoch: 3. Validation  Loss: 1.865 | Accuracy: 0.378: 100%|██████████| 62/62 [00:10<00:00,  5.92it/s]\n\n\n\n\n\nEpoch: 4. Train       Loss: 1.749 | Accuracy: 0.396: 100%|██████████| 148/148 [00:33<00:00,  4.44it/s]\nEpoch: 4. Validation  Loss: 1.535 | Accuracy: 0.474: 100%|██████████| 62/62 [00:10<00:00,  5.90it/s]\n\n\n\n\n\nEpoch: 5. Train       Loss: 1.652 | Accuracy: 0.436: 100%|██████████| 148/148 [00:32<00:00,  4.49it/s]\nEpoch: 5. Validation  Loss: 1.444 | Accuracy: 0.519: 100%|██████████| 62/62 [00:10<00:00,  5.90it/s]\n\n\n\n\n\nEpoch: 6. Train       Loss: 1.546 | Accuracy: 0.472: 100%|██████████| 148/148 [00:33<00:00,  4.43it/s]\nEpoch: 6. Validation  Loss: 1.436 | Accuracy: 0.525: 100%|██████████| 62/62 [00:10<00:00,  5.85it/s]\n\n\n\n\n\nEpoch: 7. Train       Loss: 1.486 | Accuracy: 0.494: 100%|██████████| 148/148 [00:33<00:00,  4.37it/s]\nEpoch: 7. Validation  Loss: 1.403 | Accuracy: 0.547: 100%|██████████| 62/62 [00:10<00:00,  5.78it/s]\n\n\n\n\n\nEpoch: 8. Train       Loss: 1.408 | Accuracy: 0.528: 100%|██████████| 148/148 [00:34<00:00,  4.35it/s]\nEpoch: 8. Validation  Loss: 1.245 | Accuracy: 0.584: 100%|██████████| 62/62 [00:10<00:00,  5.83it/s]\n\n\n\n\n\nEpoch: 9. Train       Loss: 1.350 | Accuracy: 0.546: 100%|██████████| 148/148 [00:34<00:00,  4.34it/s]\nEpoch: 9. Validation  Loss: 1.162 | Accuracy: 0.619: 100%|██████████| 62/62 [00:10<00:00,  5.82it/s]\n\n\n\n\n\nEpoch: 10. Train       Loss: 1.301 | Accuracy: 0.562: 100%|██████████| 148/148 [00:34<00:00,  4.33it/s]\nEpoch: 10. Validation  Loss: 1.114 | Accuracy: 0.640: 100%|██████████| 62/62 [00:10<00:00,  5.69it/s]\n\n\n\n\n\n\n\n\nWe get a really good result just by switching normal stacked Conv Layers with Residual Layers. Recall that this new network is just as deep as the Big Model we created earlier! The Residual Learning framework does really work."
  },
  {
    "objectID": "posts/resnet-paper/resnet-implementation.html#resnet-18",
    "href": "posts/resnet-paper/resnet-implementation.html#resnet-18",
    "title": "deep residual learning for image recognition (2015)",
    "section": "Resnet 18",
    "text": "Resnet 18\nOur model earlier wasn’t designed that well (although it worked regardless). We just stacked Residual Layers ontop of each other without consideration of other factors.\nLuckily for us, the authors of the paper came up with a number of designs that they named depending on how many layers the network hard. You can see the configurations in the picture below:\n\n\n\nimage.png\n\n\nFor our simple use case, we can use the 18-layer Residual Network, or as it is commonly known, the Resnet-18.\nNote that most frameworks provide these models readily, whether you need a pre-trained one for fine-tuning or you need just the structure and you train it for yourself. For example, the resnet-18 by PyTorch can be found here.\nBut since we are learning, why not implement the model ourselves? It will make for a good challenge.\nHere is my from-scratch implementation of the network as per my understanding and using the image above. You might want to pause here for a second and try and understand what I did. It is pretty self explanatory though.\n\nclass Resnet18(nn.Module):\n  def __init__(self):\n    super(Resnet18, self).__init__()\n    self.convs = nn.Sequential(\n        ConvLayer(3, 64, ks=7, stride=2),\n        nn.MaxPool2d(kernel_size=3, stride=2),\n        ResBlock(64, 64, stride=1),\n        ResBlock(64, 64, stride=1),\n        ResBlock(64, 128),\n        ResBlock(128, 128, stride=1),\n        ResBlock(128, 256),\n        ResBlock(256, 256, stride=1),\n        ResBlock(256, 512),\n        ResBlock(512, 512, stride=1))\n    self.avgpool = nn.AdaptiveAvgPool2d(1)\n    self.linear = nn.Linear(512, 10)\n  \n  def forward(self, x):\n    out = self.convs(x)\n    out = self.avgpool(out)\n    out = nn.Flatten()(out)\n    out = self.linear(out)\n    return out\n\nWe can instantiate our Module and see if its layers match the Resnet-18 layer structure by the researchers:\n\nresnet18 = Resnet18()\nresnet18\n\nResnet18(\n  (convs): Sequential(\n    (0): Sequential(\n      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n    )\n    (1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (2): ResBlock(\n      (convs): Sequential(\n        (0): Sequential(\n          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU()\n        )\n        (1): Sequential(\n          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (3): ResBlock(\n      (convs): Sequential(\n        (0): Sequential(\n          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU()\n        )\n        (1): Sequential(\n          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (4): ResBlock(\n      (convs): Sequential(\n        (0): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU()\n        )\n        (1): Sequential(\n          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (idconv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n    )\n    (5): ResBlock(\n      (convs): Sequential(\n        (0): Sequential(\n          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU()\n        )\n        (1): Sequential(\n          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (6): ResBlock(\n      (convs): Sequential(\n        (0): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU()\n        )\n        (1): Sequential(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (idconv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n    )\n    (7): ResBlock(\n      (convs): Sequential(\n        (0): Sequential(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU()\n        )\n        (1): Sequential(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (8): ResBlock(\n      (convs): Sequential(\n        (0): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU()\n        )\n        (1): Sequential(\n          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (idconv): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n    )\n    (9): ResBlock(\n      (convs): Sequential(\n        (0): Sequential(\n          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU()\n        )\n        (1): Sequential(\n          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=1)\n  (linear): Linear(in_features=512, out_features=10, bias=True)\n)\n\n\nAgain, we train the model using the same hyperparameters and for the same number of epochs, for comparisons sake.\n\nnum_epochs=10\nlr = 3e-3\n\nresnet18 = Resnet18()\nresnet18 = resnet18.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(resnet18.parameters(), lr=lr)\nscheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr,\n                                          steps_per_epoch=int(len(train_dl)),\n                                          epochs=num_epochs,\n                                          anneal_strategy='linear')\n\nresnet18.apply(init_weights);\n\n\nfor epoch in range(1, num_epochs+1):\n  train(train_dl, resnet18, criterion, optimizer, epoch, scheduler)\n  validate(valid_dl, resnet18, criterion, epoch)\n  print()\n\n  0%|          | 0/148 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\nEpoch: 1. Train       Loss: 1.977 | Accuracy: 0.303: 100%|██████████| 148/148 [00:37<00:00,  3.95it/s]\nEpoch: 1. Validation  Loss: 1.840 | Accuracy: 0.337: 100%|██████████| 62/62 [00:11<00:00,  5.39it/s]\n\n\n\n\n\nEpoch: 2. Train       Loss: 1.709 | Accuracy: 0.418: 100%|██████████| 148/148 [00:35<00:00,  4.16it/s]\nEpoch: 2. Validation  Loss: 1.491 | Accuracy: 0.504: 100%|██████████| 62/62 [00:11<00:00,  5.44it/s]\n\n\n\n\n\nEpoch: 3. Train       Loss: 1.626 | Accuracy: 0.446: 100%|██████████| 148/148 [00:36<00:00,  4.10it/s]\nEpoch: 3. Validation  Loss: 1.404 | Accuracy: 0.541: 100%|██████████| 62/62 [00:11<00:00,  5.47it/s]\n\n\n\n\n\nEpoch: 4. Train       Loss: 1.476 | Accuracy: 0.507: 100%|██████████| 148/148 [00:35<00:00,  4.17it/s]\nEpoch: 4. Validation  Loss: 1.229 | Accuracy: 0.597: 100%|██████████| 62/62 [00:11<00:00,  5.48it/s]\n\n\n\n\n\nEpoch: 5. Train       Loss: 1.333 | Accuracy: 0.559: 100%|██████████| 148/148 [00:35<00:00,  4.21it/s]\nEpoch: 5. Validation  Loss: 1.073 | Accuracy: 0.653: 100%|██████████| 62/62 [00:11<00:00,  5.55it/s]\n\n\n\n\n\nEpoch: 6. Train       Loss: 1.249 | Accuracy: 0.590: 100%|██████████| 148/148 [00:34<00:00,  4.23it/s]\nEpoch: 6. Validation  Loss: 1.045 | Accuracy: 0.660: 100%|██████████| 62/62 [00:11<00:00,  5.63it/s]\n\n\n\n\n\nEpoch: 7. Train       Loss: 1.127 | Accuracy: 0.631: 100%|██████████| 148/148 [00:35<00:00,  4.21it/s]\nEpoch: 7. Validation  Loss: 0.941 | Accuracy: 0.696: 100%|██████████| 62/62 [00:10<00:00,  5.64it/s]\n\n\n\n\n\nEpoch: 8. Train       Loss: 1.048 | Accuracy: 0.657: 100%|██████████| 148/148 [00:34<00:00,  4.24it/s]\nEpoch: 8. Validation  Loss: 0.920 | Accuracy: 0.704: 100%|██████████| 62/62 [00:10<00:00,  5.69it/s]\n\n\n\n\n\nEpoch: 9. Train       Loss: 0.971 | Accuracy: 0.680: 100%|██████████| 148/148 [00:34<00:00,  4.25it/s]\nEpoch: 9. Validation  Loss: 0.806 | Accuracy: 0.743: 100%|██████████| 62/62 [00:10<00:00,  5.66it/s]\n\n\n\n\n\nEpoch: 10. Train       Loss: 0.895 | Accuracy: 0.705: 100%|██████████| 148/148 [00:34<00:00,  4.25it/s]\nEpoch: 10. Validation  Loss: 0.736 | Accuracy: 0.765: 100%|██████████| 62/62 [00:10<00:00,  5.64it/s]"
  },
  {
    "objectID": "posts/resnet-paper/resnet-implementation.html#references-and-useful-material",
    "href": "posts/resnet-paper/resnet-implementation.html#references-and-useful-material",
    "title": "deep residual learning for image recognition (2015)",
    "section": "References and Useful Material",
    "text": "References and Useful Material\n\nDeep Residual Learning For Image Recognition - https://arxiv.org/pdf/1512.03385.pdf\nChapter 14 of Practical Deep Learning For Coders - https://colab.research.google.com/github/fastai/fastbook/blob/master/14_resnet.ipynb\nWeights And Biases PyTorch Paper Reading Stream - https://www.youtube.com/watch?v=nspf00KpU-g&list=PLD80i8An1OEG_vpqwQgwH1gIxeb9r30u5&index=8"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about me",
    "section": "",
    "text": "I am a software engineer from Kenya. My major interests revolve around AI and deep learning.\nI code mostly in Python, but can very hackily use JavaScript too when need be. I am also learning Rust and adding it to my arsenal.\nI currently work in the AI team at Tambua Health."
  },
  {
    "objectID": "about.html#contact-me",
    "href": "about.html#contact-me",
    "title": "about me",
    "section": "contact me",
    "text": "contact me\nYou can contact me on social media with the links provided above or email me at my personal email or contact me on Signal or WhatsApp if you already have my number.\nI am most likely to respond on Twitter, either DM or @ me on a tweet."
  },
  {
    "objectID": "about.html#books",
    "href": "about.html#books",
    "title": "about me",
    "section": "books",
    "text": "books\nI try to keep a list of books I have read and their reviews on my goodreads profile. Both fiction and non-fiction.\nHere is a non-exhaustive list of books that I have read that have been the most memorable for me:\n\nIntroduction to Psychology - James Kalat\nThe Demon-Haunted World: Science as a Candle in the Dark.\nSurely you are joking, Mr. Feynman!\nThinking, Fast and Slow.\nDeep Learning for Coders.\nNeural Networks from Scratch - link\nDeschooling Society\nThe Teenage Liberation Handbook: How to Quit School and Get a Real Life and Education."
  },
  {
    "objectID": "about.html#blogs-i-enjoy",
    "href": "about.html#blogs-i-enjoy",
    "title": "about me",
    "section": "blogs I enjoy",
    "text": "blogs I enjoy\nI update this regularly. For now, these are the blogs I follow and enjoy:\n\nGeorge Hotz\nPaul Graham\nAndrej Karpathy\nJulian Schrittwieser\nfast.ai\nEnid Kathambi (Kenyan Finance blog)"
  },
  {
    "objectID": "about.html#notes",
    "href": "about.html#notes",
    "title": "about me",
    "section": "notes",
    "text": "notes\nI heavily take notes whenever I can, these can be from podcasts, youtube videos I watch, talks I attend, online MOOCs etc. For taking notes, I use Obsidian + a private GitHub repo backup across my machines. I previously used Notion which I would recommend too\nI share some of them here on my website, mostly so that I can have a central place where I can scheme through them from time to time. You can find the public ones here: notes\nI thought about running a podcast on my own in my young years but still haven’t got to do it. In the meantime, I enjoy listening to other people’s podcasts and writing notes. I also share my notes here:"
  },
  {
    "objectID": "about.html#music",
    "href": "about.html#music",
    "title": "about me",
    "section": "music",
    "text": "music\nI love listening to music. I always have my headphones on me everywhere I go.\nI enjoy chill and underground rap and hip-hop music mostly, but I listen to any good music that gets my head bopping, across all genres.\nYou can connect with me on Spotify (I don’t use it that often these days) and on Apple Music, both @jimmiemunyi and get to listen to my playlists and share yours (please!)"
  },
  {
    "objectID": "about.html#meditation",
    "href": "about.html#meditation",
    "title": "about me",
    "section": "meditation",
    "text": "meditation\nI practive Vipassana Meditaion. It keeps me in touch with myself. My friends tell me that they too have practices that keep them in touch with themselves: for some its exercising and the gym, others pray to some deity, others love taking walks etc. I think everyone should have their way of keeping sane and keeping in touch with themselves, as long as its:\n\nuniversal. Can be practiced with anyone from any background.\ninvolves no dogma.\nsimple to understand. Doesn’t have strange practices that you ‘should just believe in’.\n\nVipassana ticks all the above boxes for me and hence I practice it. I have also experienced positive results from it. But I do not claim this to be the only way. To each his own."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog posts",
    "section": "",
    "text": "there’s no such thing as not a computer person\n\n\n\n\n\nDebunking the myth.\n\n\n\n\n\n\nNov 10, 2022\n\n\n4 min\n\n\n\n\n\n\n\n\nmaking transfer learning work in plain PyTorch\n\n\n\n\n\nTips to effectively use Transfer Learning in the Vision Domain\n\n\n\n\n\n\nOct 17, 2021\n\n\n7 min\n\n\n\n\n\n\n\n\ndeep residual learning for image recognition (2015)\n\n\n\n\n\nImplementing the ResNet Paper\n\n\n\n\n\n\nSep 28, 2021\n\n\n12 min\n\n\n\n\n\n\n\n\nlearning from sound\n\n\n\n\n\nClassifying respiratory sounds with PyTorch and torchaudio\n\n\n\n\n\n\nSep 14, 2021\n\n\n10 min\n\n\n\n\n\n\n\n\nmaking a book recommendation system\n\n\n\n\n\nUsing Collaborative Filtering to recommend books.\n\n\n\n\n\n\nFeb 15, 2021\n\n\n12 min\n\n\n\n\n\n\n\n\nsign language inference\n\n\n\n\n\nCreating a realtime sign language intepreter.\n\n\n\n\n\n\nJan 21, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\nsign language classification\n\n\n\n\n\nTraining Our Model for Classifying Sign Language\n\n\n\n\n\n\nJan 20, 2021\n\n\n6 min\n\n\n\n\n\n\n\n\nhandling imbalance in medical datasets\n\n\n\n\n\nHow to train models in the medical domain where datasets are often imbalanced\n\n\n\n\n\n\nOct 14, 2020\n\n\n5 min\n\n\n\n\n\n\n\n\ntraining state of the art models with little data and compute using transfer learning\n\n\n\n\n\nA Beginner’s Practical look into Transfer Learning\n\n\n\n\n\n\nOct 2, 2020\n\n\n9 min\n\n\n\n\n\n\nNo matching items"
  }
]