[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am a Machine Learning Engineer who loves building projects that bring AI into the real world.\nI currently work in the AI team at Tambua Health.\nI also love contributing to Open Source Software.\nIn this blog, I write technical posts about interesting topics I encounter along the way, and also personal blog posts about my thoughts.\nWelcome to my space."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\nKabarak University | Nakuru, Kenya | Bsc. in Telecommunications | May 2017 - Aug 2021"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\nTambua Health | Machine Learning Engineer | Nov 2021 - present"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Training Our Model for Classifying Sign Language\n\n\n\n\n\n\nJan 20, 2021\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nHow to train models in the medical domain where datasets are often imbalanced\n\n\n\n\n\n\nOct 14, 2020\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nA Beginner’s Practical look into Transfer Learning\n\n\n\n\n\n\nOct 2, 2020\n\n\n9 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/sign-language/2021-01-21-Sign-Language-Inference-with-WebCam.html",
    "href": "projects/sign-language/2021-01-21-Sign-Language-Inference-with-WebCam.html",
    "title": "Sign Language Inference",
    "section": "",
    "text": "After training our model in Part A, we are now going to develop an application to run inference with for new data.\nI am going to be utilizing opencv to get live video from my webcam, then run our model against each frame in the video and get the prediction of what Sign Language Letter I am holding up.\nHere is an example of what the output will look like:\n\n\n\n\nExample of me using the Realtime system\n\n\nThe whole code + training notebooks from Part A can be found in this github repo.\nThis tutorial assumes some basic understanding of the cv2 library and general understanding of how to run inference using a model."
  },
  {
    "objectID": "projects/sign-language/2021-01-21-Sign-Language-Inference-with-WebCam.html#imports",
    "href": "projects/sign-language/2021-01-21-Sign-Language-Inference-with-WebCam.html#imports",
    "title": "Sign Language Inference",
    "section": "Imports",
    "text": "Imports\nInstall fastai and opencv-python.\nNext, this are the packages I utilize for this App. fastai is going to be used to run Inference with, cv2 is going to handle all the WebCam functionality and we are going to utilize deque and Counter from collections to apply a nifty trick I am going to show you.\n\nfrom collections import deque, Counter\n\nimport cv2\nfrom fastai.vision.all import *"
  },
  {
    "objectID": "projects/sign-language/2021-01-21-Sign-Language-Inference-with-WebCam.html#loading-our-inference-model",
    "href": "projects/sign-language/2021-01-21-Sign-Language-Inference-with-WebCam.html#loading-our-inference-model",
    "title": "Sign Language Inference",
    "section": "Loading our Inference Model",
    "text": "Loading our Inference Model\n\nprint('Loading our Inference model...')\n# load our inference model\ninf_model = load_learner('model/sign_language.pkl')\nprint('Model Loaded')\n\nThe next part of our code loads the model we pickled in Part A and prints some useful information."
  },
  {
    "objectID": "projects/sign-language/2021-01-21-Sign-Language-Inference-with-WebCam.html#rolling-average-predictions",
    "href": "projects/sign-language/2021-01-21-Sign-Language-Inference-with-WebCam.html#rolling-average-predictions",
    "title": "Sign Language Inference",
    "section": "Rolling Average Predictions",
    "text": "Rolling Average Predictions\nWhen I first made the App, I noticed one problem when using it. A slight movement of my hand was changing the predictions. This is known as flickering. The video below shows how flickering affects our App:\n\n\n\nOutput Video Flickering\n\n\nThe Video you saw in the beginning shows how ‘stable’ our model is after using rolling predictions.\n\n# define a deque to get rolling average of predictions\n# I go with the last 10 predictions\nrolling_predictions = deque([], maxlen=10)\n\n# get the most common item in the deque\ndef most_common(D):\n    data = Counter(D)\n    return data.most_common(1)[0][0]\n\nTo solve this, a utilized the deque from Collections. I used 10 as the maxlength of the deque since I wanted the App, when running inference, to output the most common prediction out of the last 10 predictions. This makes it more stable than when we are using only the current one.\nThe function most_common will return the most common item in our deque."
  },
  {
    "objectID": "projects/sign-language/2021-01-21-Sign-Language-Inference-with-WebCam.html#hand-area",
    "href": "projects/sign-language/2021-01-21-Sign-Language-Inference-with-WebCam.html#hand-area",
    "title": "Sign Language Inference",
    "section": "Hand Area",
    "text": "Hand Area\n\ndef hand_area(img):\n    # specify where hand should go\n    hand = img[50:324, 50:324]\n    # the images in the model were trainind on 200x200 pixels\n    hand = cv2.resize(hand, (200,200))\n    return hand\n\nNext, we define a function that tells our model which part of the video to run inference on. We do not want to run inference on the whole video which will include our face! We will eventually draw a blue rectangle in this area so that you’ll know where to place your hand."
  },
  {
    "objectID": "projects/sign-language/2021-01-21-Sign-Language-Inference-with-WebCam.html#capture-video-on-the-webcam-and-define-our-writer",
    "href": "projects/sign-language/2021-01-21-Sign-Language-Inference-with-WebCam.html#capture-video-on-the-webcam-and-define-our-writer",
    "title": "Sign Language Inference",
    "section": "Capture Video on the WebCam and Define Our Writer",
    "text": "Capture Video on the WebCam and Define Our Writer\n\n# capture video on the webcam\ncap = cv2.VideoCapture(0)\n\n# get the dimensions on the frame\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\n\n# define codec and create our VideoWriter to save the video\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter('sign-language.mp4', fourcc, 12, (frame_width, frame_height))\n\nHere, we define a VideoCapture that will record our video. The parameter 0 means capture on the first WebCam it finds. If you have multiple WebCams, this is the parameter you want to play around with until you find the correct one.\nNext, we get the dimensions of the frame being recorded by the VideoCapture. We are going to use this dimensions when writing (outputting) the recorded video\nFinally, we create a VideoWriter that we are going to use to output the video and write it to our Hard Disk. To do that, opencv requires us to define a codec to use, and so we create a VideoWriter_fourcc exactly for that purpose and we use ‘mp4v’ with it.\nIn our writer, we first pass the name we want for the output file, here I use ‘sign-language.mp4’ which will be written in the current directory. You can change this location if you wish to. Next we pass in the codec. After that you pass in your fps (frame rate per second). I found that 12 worked best with my configuration but you probably want to play around with that until you get the best one for you. Finally, we pass in the frame sizes, which we had gotten earlier."
  },
  {
    "objectID": "projects/sign-language/2021-01-21-Sign-Language-Inference-with-WebCam.html#the-main-video-loop",
    "href": "projects/sign-language/2021-01-21-Sign-Language-Inference-with-WebCam.html#the-main-video-loop",
    "title": "Sign Language Inference",
    "section": "The Main Video Loop",
    "text": "The Main Video Loop\n\n# read video\nwhile True:\n    # capture each frame of the video\n    ret, frame = cap.read()\n\n    # flip frame to feel more 'natural' to webcam\n    frame = cv2.flip(frame, flipCode = 1)\n\n    # draw a blue rectangle where to place hand\n    cv2.rectangle(frame, (50, 50), (324, 324), (255, 0, 0), 2)\n\n    # get the image\n    inference_image = hand_area(frame)\n\n    # get the current prediction on the hand\n    pred = inf_model.predict(inference_image)\n    # append the current prediction to our rolling predictions\n    rolling_predictions.append(pred[0])\n\n    # our prediction is going to be the most common letter\n    # in our rolling predictions\n    prediction_output = f'The predicted letter is {most_common(rolling_predictions)}'\n\n    # show predicted text\n    cv2.putText(frame, prediction_output, (10, 350), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n    # show the frame\n    cv2.imshow('frame', frame)\n    # save the frames to out file\n    out.write(frame)\n\n\n    # press `q` to exit\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\nThis is a long piece of code so lets break it down bit by bit:\n\n# read video\nwhile True:\n    # capture each frame of the video\n    _ , frame = cap.read()\n\n    # flip frame to feel more 'natural' to webcam\n    frame = cv2.flip(frame, flipCode = 1)\n    \n\n\n    # ......\n    # truncated code here\n    # ......\n\n\n\n    \n    # show the frame\n    cv2.imshow('frame', frame)\n    # save the frames to out file\n    out.write(frame)\n\n\n    # press `q` to exit\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\nWe create a infinite While loop that will always be running, until the user presses the ‘q’ letter on the keyboard, as defined by our last if statement at the very bottom of the loop.\nAfter that, we use the reader we created earlier and call cap.read() on it which returns the current frame of the video, and another variable that we are not going to use.\nA little intuition how videos works. A frame is somewhat equivalent to just one static image. Think of it as that. So for a video what usually happens it these single frames are played one after the other quickly, like 30-60 times faster hence creating the illusion of a continious video.\nSo for our App, we are going to get each frame, and run it through our model (which expects the input to be an image, so this will work) and return the current prediction. This is also why we decided to use rolling average predictions and not the just the current prediction. To reduce the flickering that may occur by passing a different frame each second.\nNext:\n    frame = cv2.flip(frame, flipCode = 1)\nThis flips our frame to make it feel more natural. What I mean is, without flipping, the output image felt reversed, where if I raised my left arm it seemed like I was raising my right. Try running the App with this part commented out and you’ll get what I mean.\nThis shows the frames one after the other and the out writes it to disk\n    cv2.imshow('frame', frame)\n    # save the frames to out file\n    out.write(frame)\n\n\n# read video\nwhile True:\n    # ......\n    # truncated code here\n    # ......\n\n    # draw a blue rectangle where to place hand\n    cv2.rectangle(frame, (50, 50), (324, 324), (255, 0, 0), 2)\n\n    # get the image\n    inference_image = hand_area(frame)\n\n    # get the current prediction on the hand\n    pred = inf_model.predict(inference_image)\n    # append the current prediction to our rolling predictions\n    rolling_predictions.append(pred[0])\n\n    # our prediction is going to be the most common letter\n    # in our rolling predictions\n    prediction_output = f'The predicted letter is {most_common(rolling_predictions)}'\n\n    # show predicted text\n    cv2.putText(frame, prediction_output, (10, 350), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n\n\n    # ......\n    # truncated code here\n    # ......\n\n\n    # press `q` to exit\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\nNext, we draw a blue rectangle where the user should place the hand. The first parameter is where we want to draw the rectangle and we tell opencv to draw it on our current frame. The next two parameter describe the area where we want our rectangle to be. Note that this dimensions are exactly the same as those in the hand_area function we created earlier. This is to make sure we are running inference on the correct area. Lastly we pass in the color of the rectangle (in BGR formart) and the thickness of the line (2).\ncv2.rectangle(frame, (50, 50), (324, 324), (255, 0, 0), 2)\nNext, from our whole frame, we just extract the hand area and store it. This is the image we are going to pass to our model\ninference_image = hand_area(frame)\nNext, we pass our extracted image to our inference model and get the predictions and append that prediction to our rolling predictions deque. Remember that this deque will only hold the most recent 10 predictions and discard everything else\npred = inf_model.predict(inference_image)\n\nrolling_predictions.append(pred[0])\nWe get the most common Letter predicted in our Deque and use opencv to write that letter to the video. The parameters are almost similar to the rectangle code, with a slight variation since here we have to pass in the font(hershey simplex) and font size (0.9)\nprediction_output = f'The predicted letter is {most_common(rolling_predictions)}'\n\ncv2.putText(frame, prediction_output, (10, 350), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\nThe final part of the code just releases the resources we had acquired initially: the Video reader, the Video Writer and then destroys all windows created.\n\n# release VideoCapture()\ncap.release()\n# release out file\nout.release()\n# close all frames and video windows\ncv2.destroyAllWindows()\n\nAnd that’s all in this project. Hope you enjoyed it.\nIn future, I am going to look for ways to improve this system and how to actually make it useful."
  },
  {
    "objectID": "posts/sign-language/sign-language-classification.html",
    "href": "posts/sign-language/sign-language-classification.html",
    "title": "Sign Language Classification",
    "section": "",
    "text": "Importing Packages\nWe are going to be using fastai so let’s import it:\n\nfrom fastai.vision.all import *\n\n\n\nThe Data\nThe dataset we are going to be using is American Sign Language Dataset from Kaggle. It contains 87,000 images each of 200x200 pixels. It has 29 classes: 26 for the letters A-Z and 3 classes for space, delete and nothing. We are going to use the Kaggle API to get the data.\n\n!kaggle datasets download -d grassknoted/asl-alphabet\n\nDownloading asl-alphabet.zip to /content\n100% 1.03G/1.03G [00:05<00:00, 222MB/s]\n\n\n\nLet’s unzip the data and get rid of the zip file:\n\n!unzip *zip -d data && rm -rf *zip\n\nWe create a Pathlib object pointing to our data folder and look inside to see what it contains:\n\npath = Path('data')\nPath.BASE_PATH = path\n\n\npath.ls()\n\n(#2) [Path('asl_alphabet_test'),Path('asl_alphabet_train')]\n\n\nLet’s peek into one of those folders:\n\n(path/'asl_alphabet_train').ls()\n\n(#29) [Path('asl_alphabet_train/X'),Path('asl_alphabet_train/G'),Path('asl_alphabet_train/V'),Path('asl_alphabet_train/I'),Path('asl_alphabet_train/space'),Path('asl_alphabet_train/N'),Path('asl_alphabet_train/W'),Path('asl_alphabet_train/P'),Path('asl_alphabet_train/H'),Path('asl_alphabet_train/Z')...]\n\n\nWe have 29 folders, as explained earlier.\n\n\nData Preprocessing\nNow we are ready to create a DataBlock blueprint to hold our data. We use the fastai DataBlock API which is a convenient way to define how to handle our data.\nSince we don’t have validation data provided, we will split 20% of the training images and use it as our validation data.\nWe then create a DataLoaders object from the DataBlock, we will use a batch-size of 64.\n\nsigns = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    get_y=parent_label,\n    splitter=RandomSplitter(seed=42, valid_pct=0.2),\n    item_tfms=Resize(200),\n    batch_tfms=aug_transforms()\n)\n\ndls = signs.dataloaders(path/'asl_alphabet_train', bs=64)\n\nLet’s look into one batch of the data:\n\ndls.show_batch()\n\n\n\n\nThis is the number of steps we are going to take in an epoch:\n\nlen(dls.train)\n\n1087\n\n\n\n\nUsing Transfer Learning to Create A Model\nNow we can create a model and use Transfer Learning to train it on our data. Transfer Learning is important since it enables us to get good results with less training and data.\nFor those who wish to replicate this experiment, we use: resnet18 architecture, Cross Entropy Loss since this is a Classification Task, and for our optimizer, we select the Adam Optimizer. We will output error rate and accuracy as our metrics to help as analyze how our model is doing.\nWe use the Learning Rate Finder provided by fastai, using insights from Leslie Smith’s work, that enable us to find us a good learning rate, in short time instead of us trying a couple of learning rates experimentally and seeing what works.\nIf you are interested in reading more about the Learning Rate Finder, read this paper.\nFor our tast, it looks like a learning rate of 1x10-2 will work, so we fine-tine (transfer learn) for 4 epochs.\n\nlearn = cnn_learner(dls, resnet18, loss_func=CrossEntropyLossFlat(),\n                    metrics=[error_rate, accuracy], opt_func=Adam)\n\nlearn.lr_find()\n\nDownloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n\n\n\n\n\n\n\n\n\n\n\nSuggestedLRs(lr_min=0.017378008365631102, lr_steep=0.015848932787775993)\n\n\n\n\n\n\nlearn = cnn_learner(dls, resnet18, loss_func=CrossEntropyLossFlat(),\n                    metrics=[error_rate, accuracy], opt_func=Adam)\n\nlearn.fine_tune(4, base_lr=1e-2)\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.230984\n      0.065030\n      0.019310\n      0.980690\n      04:11\n    \n  \n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.160366\n      0.875754\n      0.117989\n      0.882011\n      05:24\n    \n    \n      1\n      0.038482\n      0.008822\n      0.002529\n      0.997471\n      05:26\n    \n    \n      2\n      0.008333\n      0.000855\n      0.000230\n      0.999770\n      05:25\n    \n    \n      3\n      0.002372\n      0.000346\n      0.000057\n      0.999943\n      05:25\n    \n  \n\n\n\nWe get a very good accuracy only after 4 epochs.\nNow we can tackle the small test set that comes with this dataset, although we will scale up to a better test dataset in a few moments.\n\n\nTesting Our Model\n\ntest_images = (path/'asl_alphabet_test').ls()\ntest_images\n\n(#28) [Path('asl_alphabet_test/U_test.jpg'),Path('asl_alphabet_test/space_test.jpg'),Path('asl_alphabet_test/N_test.jpg'),Path('asl_alphabet_test/R_test.jpg'),Path('asl_alphabet_test/H_test.jpg'),Path('asl_alphabet_test/P_test.jpg'),Path('asl_alphabet_test/T_test.jpg'),Path('asl_alphabet_test/C_test.jpg'),Path('asl_alphabet_test/X_test.jpg'),Path('asl_alphabet_test/V_test.jpg')...]\n\n\nWe have 28 images in this test set, each for the classes of data we have.\nLet’s take the first two images and predict them using our model.\n\nU = (path/'asl_alphabet_test'/'U_test.jpg')\nspace = (path/'asl_alphabet_test'/'space_test.jpg')\n\n\nlearn.predict(U)[0]\n\n\n\n\n'U'\n\n\n\nlearn.predict(space)[0]\n\n\n\n\n'space'\n\n\nThat looks like its working well. To predict on all the images in the test set, we are going to need a way to get the labels of the images, so as to compare with our prediction.\nLet’s work with one image first:\n\nu_test = test_images[0]\nu_test\n\nPath('asl_alphabet_test/U_test.jpg')\n\n\nAs you can see, the label of the test images is contained in the filename. So we are going to use regular expressions to extract the label from the filenames.\nHere is a simple regular expression that does the job:\n\nre.findall('(.+)_test.jpg$', u_test.name)[0]\n\n'U'\n\n\nAnd our prediction on that image:\n\nlearn.predict(u_test)[0]\n\n\n\n\n'U'\n\n\nAnd now, a way to compare our prediction, to the true label of the test set:\n\nre.findall('(.+)_test.jpg$', u_test.name)[0] == learn.predict(u_test)[0]\n\n\n\n\nTrue\n\n\nLet us write a function that is going to extract the labels, and store them in a list:\n\ndef get_test_names(images):\n  labels = []\n\n  for i in images:\n    label = re.findall('(.+)_test.jpg$', i.name)[0]\n    labels.append(label)\n  \n  return labels\n\nWe can now get all the labels for the 28 images in our test set:\n\ntest_labels = get_test_names(test_images)\n\n\nlen(test_labels)\n\n28\n\n\nNow we need a function to run inference on the images. It is going to take in the images, our model and the labels we just got as parameters and output the mean accuracy of our predictions:\n\ndef run_inference(images, model, labels):\n  corrects = []\n  # get the number of images to inference\n  num_images = len(images)\n\n  for i in range(num_images):\n    # get the inference for an image\n    prediction = model.predict(images[i])[0]\n\n    # compare with the label for that image\n    is_equal = (prediction==labels[i])\n\n    # append result to the list\n    corrects.append(is_equal)\n  \n  # convert the list of inferences to float Tensor\n  corrects = torch.Tensor(corrects).float()\n  \n  # return the mean accuracy\n  return corrects.mean().item()\n\nWe can use that function to get the mean accuracy of our model on the small test dataset.\n\ntest_accuracy = run_inference(test_images, learn, test_labels)\n\ntest_accuracy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.0\n\n\nWe get 100% accuracy. That’s fishy, you should always be worried when your model achieves a high accuracy like this. There could be data leaking. To add to that, this is a small dataset.\nLuckily, there is another dataset recommended to be used as a test set for this dataset. It contains 870 images, 30 images for each category.\nLet us use the Kaggle API again to get this new dataset:\n\n!kaggle datasets download -d danrasband/asl-alphabet-test\n\nDownloading asl-alphabet-test.zip to /content\n 70% 17.0M/24.3M [00:00<00:00, 24.5MB/s]\n100% 24.3M/24.3M [00:00<00:00, 33.3MB/s]\n\n\nAnd unzip it to a test folder:\n\n!unzip *zip -d test && rm -rf *zip\n\n\ntest_path = Path('test')\ntest_path.ls()\n\n(#29) [Path('test/X'),Path('test/G'),Path('test/V'),Path('test/I'),Path('test/space'),Path('test/N'),Path('test/W'),Path('test/P'),Path('test/H'),Path('test/Z')...]\n\n\nWe use the get_image_files to recursively get images from the newly-created test path. We get 870 images, so that seems to be working fine.\n\ntest_files = get_image_files(test_path)\n\nlen(test_files)\n\n870\n\n\nTo run inference, we are required to perform the same data preprocessing we perfomed on the training images. To make this easier, fastai suggest we use a test_dl that is created using the following syntax:\n\ntest_dl = learn.dls.test_dl(test_files, with_label=True)\n\ntest_dl.show_batch()\n\n\n\n\nWe can now get the predicitons on all the test images easily using the get_preds function and store it in a test_preds variable.\n\ntest_preds = learn.get_preds(dl=test_dl)  \n\n\n\n\nThis are our current vocabs of our data:\n\nlearn.dls.vocab\n\n['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n\n\nTo extract the true labels from the test images, we are again going to turn to regular expressions. But this time, we are required to write a regular expression robust enough to handle this three cases that represent how the rest of the images are named:\n\nexample_name = test_dl.items[370].name\nexample_name\n\n'del0002_test.jpg'\n\n\n\nre.findall(r'([A-Za-z]+)\\d+_test.jpg$', example_name)[0] \n\n'del'\n\n\n\nexample_name_2 = test_dl.items[690].name\nexample_name_2\n\n'nothing0013_test.jpg'\n\n\n\nre.findall(r'([A-Za-z]+)\\d+_test.jpg$', example_name_2)[0]\n\n'nothing'\n\n\n\nexample_name_3 = test_dl.items[0].name\nexample_name_3\n\n'X0023_test.jpg'\n\n\n\nre.findall(r'([A-Za-z]+)\\d+_test.jpg$', example_name_3)[0]\n\n'X'\n\n\nNow that we have that robust expression, we can proceed to check the accuracy of our prediction that we calculated:\nWe create a list to hold the result of our comparisons, from the predictions and the true labels, which we are going to use to calculate the final accuracy.\nWe also create a category_corrects dictionary, to tally for us, for each category, how many we predicted correct, so that we can see how our model performs on each category individually.\n\n# create a list to hold True or False when comparing\ncorrects = []\n\n# count how many predictions we get correct per category\ncategory_corrects = dict.fromkeys(learn.dls.vocab, 0)\n\n# for each enumerated predictions\nfor index, item in enumerate(test_preds[0]):\n  # get the predicted vocab\n  prediction = learn.dls.categorize.decode(np.argmax(item))\n  # get the confidence of the prediction\n  confidence = max(item) \n  confidence_percent = float(confidence)\n  # get the true label for the image we are predicting\n  image_name = test_dl.items[index].name\n  label = re.findall(r'([A-Za-z]+)\\d+_test.jpg$', image_name)[0]\n  # get the comparison and append it to our corrects list\n  is_correct = (prediction==label)\n  corrects.append(is_correct)\n\n  # if we got the prediction correct for that category,\n  # increase the count by one\n  if is_correct:\n    category_corrects[prediction] += 1\n\n\n# convert the list of inferences to float Tensor\ncorrects = torch.Tensor(corrects).float()\n\n# print the mean accuracy\nprint(f'Accuracy on the test set: {corrects.mean().item():.4f}')\n\nAccuracy on the test set: 0.6195\n\n\nAs you can see, using this better test set, we can see that the accuracy reduces.\nSince this is out of domain data, my intuition is that a better dataset that varies more could be collected and used in future. However for now, let’s work with these results.\nRemember, the test set is used as a final measure, and we shouldn’t use it to improve our model\nLet us check on the per-category prediction:\n\ncategory_corrects\n\n{'A': 15,\n 'B': 30,\n 'C': 17,\n 'D': 25,\n 'E': 19,\n 'F': 13,\n 'G': 13,\n 'H': 30,\n 'I': 29,\n 'J': 22,\n 'K': 21,\n 'L': 29,\n 'M': 22,\n 'N': 14,\n 'O': 19,\n 'P': 30,\n 'Q': 29,\n 'R': 17,\n 'S': 7,\n 'T': 0,\n 'U': 11,\n 'V': 0,\n 'W': 27,\n 'X': 4,\n 'Y': 28,\n 'Z': 11,\n 'del': 23,\n 'nothing': 14,\n 'space': 20}\n\n\nA plot would be better to analyze the information:\n\nfig, ax = plt.subplots(figsize=(20, 10))\nax.bar(*zip(*category_corrects.items()))\nplt.show()\n\n\n\n\nOur model performs really poorly on the letters T, V, and X!! We will see if this is going to be a problem in our Application that we create in part 2.\n\n\nMaking our Inference Model More Robust\nSince I plan on using this model to create a Computer Vision Model, I deciced to retrain my model, adding the new dataset in order to make it more robust, since that data varied more.\nNOTE: This is not how it should be done, you should never use your test set to train the model. I only combined the two datasets into one in order to get a better model since good Sign Language Data is hard to come by and the test set we used isn’t the official test set, just a recommended dataset that could have been used for training a different model. Also, I didn’t use my new model to get a better prediction on the test set.\n\nsigns = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    get_y=parent_label,\n    splitter=RandomSplitter(seed=42),\n    item_tfms=Resize(200),\n    batch_tfms=aug_transforms()\n)\n\ndls = signs.dataloaders(path/'asl_alphabet_train', bs=64)\n\n\nlen(dls.train)\n\n1098\n\n\n\nlearn = cnn_learner(dls, resnet18, loss_func=CrossEntropyLossFlat(),\n                    metrics=[error_rate, accuracy], opt_func=Adam)\n\nlearn.fine_tune(4, base_lr=1e-2)\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.248037\n      0.073583\n      0.020769\n      0.979231\n      04:13\n    \n  \n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.121224\n      0.061025\n      0.016502\n      0.983498\n      05:27\n    \n    \n      1\n      0.036989\n      0.008055\n      0.001878\n      0.998122\n      05:27\n    \n    \n      2\n      0.007163\n      0.003641\n      0.001081\n      0.998919\n      05:27\n    \n    \n      3\n      0.003785\n      0.002509\n      0.000569\n      0.999431\n      05:27\n    \n  \n\n\n\nI can now export my model, which I will use in the next Part, to create a Computer Vision Model to predict new data. Stay tuned!\n\nlearn.export('sign_language.pkl')\n\nHere is the link to the second part"
  },
  {
    "objectID": "posts/state-of-art-models-with-transfer-learning/transfer-learning.html",
    "href": "posts/state-of-art-models-with-transfer-learning/transfer-learning.html",
    "title": "Training State of the Art Models with Little Data and Compute using Transfer Learning",
    "section": "",
    "text": "What is Transfer Learning?\nSo what really is it Transfer Learning?\nIn simple terms, Transfer Learning is the approach of transferring knowledge from one Deep Learning Model to another. In more technical terms, Transfer Learning is the approach of using an already pretained model, and adapting it to a new problem.\nThis simple approach helps developers get state of the art results with little data and little compute.\nTraining a model from scratch requires a lot of compute and a lot of data. For example the pretrained model we are going to use was trained on ImageNet database which contains 1.2 million images with 1000 categories. In practice, very few people train the enitre Network from scratch, we often leverage the knowledge gained from these pretrained models and adapt them to our specific dataset.\nTo learn more about Transfer Learning, you can use these notes.\n\n\nA Little Intuition Before We Begin.\nDeep Learning models consists of many layers inside them, each learning its own unique features. In 2013, two researches published a paper called Visualizing and Understanding Convolutional Networks that helped visualize what is going on inside the layers and what they actually learn. In the interest of keeping this post beginner friendly, I won’t go much into the technical details of the paper but, here are some images showing what the layers in the neural network learn.\nIn the first layer, the two reasearchers showed that the network learns general features like diagonal, vertical and horizontal edges. These are the building blocks.\n\n\n\nFirst layer of the CNN (courtesy of Matthew D. Zeiler and Rob Fergus)\n\n\nIn the second layer, the Network starts to learn simple patterns that are also general to any Computer Vision Data like circles, etc. \nAnd it keeps on improving layer by layer, building from the building blocks.\n\n\n\nThird layer of the CNN (courtesy of Matthew D. Zeiler and Rob Fergus)\n\n\nSo as you can see, the first layers of a Convolutional Network learn general patterns that are common to all images. This is why we don’t want to discard these knowledge because it can be used for any dataset.\nWhat actually happens in transfer learning, specifically for Computer Vision Tasks is the first layers are freezed (no learning goes on) and the final layer (the one that actually does the classification e.g dog from cat) is chopped of and replaced with a classification layer that is specific to the dataset, i.e our final layer will be trained to specifically distinguish Millipedes, Centipedes and Spiders.\nLet’s get straight into the practical example, shall we?\n\n\nTraining A Computer Vision Model Using Transfer Learning\nLet’s start by handling our imports:\n\nfrom fastai.vision.all import *\n\nFor the DataSet, we are going to scrap the internet for centipedes, millipedes and spiders. We are going to use a very handy tool, jmd_imagescraper, that uses DuckDuckGo for the image scraping and returns some nice images. The developer also provides a nice ImageCleaner that we are going to use later to clean up the dataset. Let’s import them too.\n\nfrom jmd_imagescraper.core import *\nfrom jmd_imagescraper.imagecleaner import *\n\nWe create a directory called ‘data’. We then use the image scrapper to get the images of the three classes we are interested in and save them each to their specific directories inside the ‘data’ directory, i.e. the centipedes will be stored inside a directory called ‘Centipedes’ and the millipedes will be stored inside the ‘Millipede’ directory and likewise for the spiders (This arrangement is going to prove useful later!). We download 150 images for each.\n\nroot = Path().cwd()/\"data\"\n\nduckduckgo_search(root, 'Centipede', 'centipede', max_results=150)\nduckduckgo_search(root, 'Millipede', 'millipede', max_results=150)\nduckduckgo_search(root, 'Spider', 'spider', max_results=150)\n\nLet us see how our ‘data’ directory looks after the downloading completes:\n\npath = Path('data')\n\n\npath.ls()\n\n(#3) [Path('Spider'),Path('Centipede'),Path('Millipede')]\n\n\nAs you can see, we have three directories inside, each corresponding to the images it containes. If we look inside a specific directory, e.g. Centipede, we see the individual images downloaded, and the total number of images downloaded (150) prefixed before the list:\n\n(path/'Centipede').ls()\n\n(#150) [Path('Centipede/135_dd009ed0.jpg'),Path('Centipede/084_69e0099b.jpg'),Path('Centipede/129_5409e84e.jpg'),Path('Centipede/077_cc3b3dd9.jpg'),Path('Centipede/097_cdfd1abf.jpg'),Path('Centipede/030_55b8c176.jpg'),Path('Centipede/090_ef7667e5.jpg'),Path('Centipede/028_5b5b8f46.jpg'),Path('Centipede/052_ec993151.jpg'),Path('Centipede/056_86c51270.jpg')...]\n\n\nOkay, now that we have got our images ready, we can begin the next step which is processing them. We use a handy function provided by fastai called get_image_files, which simply recursively goes through the directory and gets all the images inside them.\n\nfns = get_image_files(path)\nfns\n\n(#450) [Path('Spider/019_ffed6440.jpg'),Path('Spider/100_59bd4277.jpg'),Path('Spider/056_21ce5818.jpg'),Path('Spider/114_33c06a31.jpg'),Path('Spider/001_f7a867bc.jpg'),Path('Spider/139_3d7b9ec9.jpg'),Path('Spider/007_f8419240.jpg'),Path('Spider/113_3082658a.jpg'),Path('Spider/135_347f4e6e.jpg'),Path('Spider/144_e94c648a.jpg')...]\n\n\nWe have 450 images, which makes sense. Did any image get corrupted during downloading? Let us verify the images.\n\nfailed = verify_images(fns)\nfailed\n\n\n\n\n(#0) []\n\n\nLuckily, no image got corrupted. Good, now let’s go on.\nLet us open one and see how it looks:\n\nim = Image.open(fns[2])\nim.to_thumb(128, 128)\n\n\n\n\nSo far everything is looking good!\nSince we have gotten the images, now we can start processing them in a format that our learner expects. We are going to use the DataBlock API from fastai.\nI am going to give a brief explanation of what is going on, but I highly recommend going through their documentaion about the DataBlock API, where they explain everything in detail.\nLet us first see how the code looks like:\n\nimages = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    get_y=parent_label,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    item_tfms=RandomResizedCrop(224, min_scale=0.3),\n    batch_tfms=aug_transforms()\n)\n\nLet us go step by step:\n\nblocks=(ImageBlock, CategoryBlock)\n\nThis simply tells the dataloader the format of the data it is receiving, i.e. here, our independent variable is going to be and Image, hence the ImageBlock, and the labels or the dependent variables are going to be a category (either ‘Centipede’, ‘Millipede’, or ‘Spider’)\n\nget_items=get_image_files\n\nThis tells our dataloader how to get the items, using the get_image_files we used before\n\nget_y=parent_label\n\nThis instructs our dataloader on how to get the labels of the images, by getting the parent name of the directory the image is in (That’s why we arranged the pictures in images in their repsective directories).\n\nsplitter=RandomSplitter(valid_pct=0.2, seed=42)\n\nThis provides a way of splitting the dataset into a training and a validation set. Here we split the validation set into 20% of the total data. The seed option is there to ensure we get the same validation set every time.\n\nitem_tfms=RandomResizedCrop(224, min_scale=0.3)\n\nThis is simply a transformation done on every image individually. Here we resize the images to 224 x 224. Images should be the same size when fed into the Neural Network. We go an extra step of randomly picking a different crop of the images every time, i.e. a minimum of 30% of the total image every time. Randomly picking a different section of the image every time helps the Network generalize well to new data.\nAnd finally this,\n\nbatch_tfms=aug_transforms()\n\nperforms data augmentation on the images. Data Augmentation deserves a whole post by itself to explain, but for intuition on why we do this, let me give a brief explanation. When using our model in the real world, people will provide images in very different formats, taken from different angles, some from cameras with low pixel capturing capabilities which provides somewhat blurry images. But we still need the model to generalize well to all of these cases! Hence data augmentation. Data Augmentation transforms the images to different versions, flipping it, rotating it, darkening it and many other transforms, to help the model generalize well in the real world. We use a batch transform here that applies the transforms in batches in the GPU which is way faster.\nLet us load the images into a dataloader, which is what the learner expects, and show one batch of the images.\n\ndls = images.dataloaders(path)\n\n\ndls.show_batch(max_n=9)\n\n\n\n\nAs you can see, the images are being Randomly Resized, cropping every time to 30% of the image.\nLet us see what the augmentation transforms did to our data, by adding the unique parameter:\n\ndls.train.show_batch(max_n=8, nrows=2, unique=True)\n\n\n\n\nAs you can see, these all are the same images but transformed differently.\nNow we are ready to create the model.\n\n\nTraining The Model\nRemember all the talk of using a pretrained model? Well, here is where we apply it.\nWe are using the resnet18 pretrained model from PyTorch and fine tuning it for 5 epochs for our specific dataset. The ‘18’ suffix simply means it has 18 layers, which is going to be sufficient for our simple model. However, there are deeper models like resnet34, resnet50, resnet101 and resnet152 with the respective number of layers. Deeper models take more time to train, and often produce better results but not always! As a rule of thumb, start simple then upgrade if need be.\nWe load our dataloaders (dls) created earlier and we are going to output ‘error_rate’ and ‘accuracy’ as our metrics, to guide us on how well our model is performing.\nWe are going to use a cnn_learner which is simply a Convolutional Neural Network Learner which is the type of Neural Network widely used for Computer Visions tasks.\n\nlearn = cnn_learner(dls, resnet18, metrics=[error_rate, accuracy])\n\nlearn.fine_tune(5)\n\nDownloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.545470\n      0.389558\n      0.155556\n      0.844444\n      00:03\n    \n  \n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.454676\n      0.267906\n      0.100000\n      0.900000\n      00:03\n    \n    \n      1\n      0.342936\n      0.232648\n      0.066667\n      0.933333\n      00:03\n    \n    \n      2\n      0.291200\n      0.193626\n      0.077778\n      0.922222\n      00:03\n    \n    \n      3\n      0.237957\n      0.190752\n      0.066667\n      0.933333\n      00:03\n    \n    \n      4\n      0.205695\n      0.206321\n      0.066667\n      0.933333\n      00:03\n    \n  \n\n\n\nAfter 5 epochs, we get an error rate of 6.7% which corresponds to an accuracy of 93.3%. That is really good considering our small dataset and the time we used to train this model, approximately 20 seconds, but as you will see, we can improve this.\nYou may be asking yourself why we didn’t clean the dataset first before training. It is good to train your model as soon as possible to provide you with a baseline which you can start improving from. And we will clean the dataset later, with the help of the training results and then retrain with a clean dataaset and see if it improves.\nLet us inspect what errors our initial model is making. The Classification Confusion Matrix can aid in displaying this in a good format.\n\ninterp = ClassificationInterpretation.from_learner(learn)\n\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\nThe dark colors on that diagonal indicate that the model is perfoming well. But it is still making mistakes, for example it classified Centipedes as Millipedes 4 times and Spiders as Centipedes twice.\nLet us see the specific images it is getting wrong and try to understand why it is confusing them by plotting the top losses of our model.\n\ninterp.plot_top_losses(5, nrows=5)\n\n\n\n\nStraight away we can see that some of the mistakes it is making is because of unclean data. For example the 2nd and 4th images have nothing to do with our data.\nThis is why we need to clean the data. As you can see, training the model first helps us with the cleaning process.\nWe are going to use the ImageCleaner provides by the jmd_imagescrapper developer.\n\ndisplay_image_cleaner(root)\n\n\n\n\nCleaner\n\n\nI did deleted a few of the images from the datasets that didn’t fit the criteria and we were left with 394 images (but useful ones!).\n\nfns = get_image_files(path)\nfns\n\n(#394) [Path('Spider/019_ffed6440.jpg'),Path('Spider/100_59bd4277.jpg'),Path('Spider/056_21ce5818.jpg'),Path('Spider/114_33c06a31.jpg'),Path('Spider/001_f7a867bc.jpg'),Path('Spider/139_3d7b9ec9.jpg'),Path('Spider/007_f8419240.jpg'),Path('Spider/113_3082658a.jpg'),Path('Spider/135_347f4e6e.jpg'),Path('Spider/144_e94c648a.jpg')...]\n\n\nOkay, now we create a new dataloader with the clean images.\n\ndls = images.dataloaders(path)\n\nWill training with only clean data help improve our model? Lets train a new model and see. We are going to use the exact details we used before, but I am fine-tuning for 10 epochs this time.\n\nlearn = cnn_learner(dls, resnet18, metrics=[error_rate, accuracy])\n\nlearn.fine_tune(10)\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      2.070883\n      0.254761\n      0.076923\n      0.923077\n      00:02\n    \n  \n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.478606\n      0.169040\n      0.038462\n      0.961538\n      00:02\n    \n    \n      1\n      0.413722\n      0.118356\n      0.038462\n      0.961538\n      00:02\n    \n    \n      2\n      0.333819\n      0.103353\n      0.038462\n      0.961538\n      00:02\n    \n    \n      3\n      0.260725\n      0.119273\n      0.025641\n      0.974359\n      00:02\n    \n    \n      4\n      0.213143\n      0.118922\n      0.025641\n      0.974359\n      00:02\n    \n    \n      5\n      0.185268\n      0.092165\n      0.025641\n      0.974359\n      00:02\n    \n    \n      6\n      0.156762\n      0.087852\n      0.012821\n      0.987179\n      00:02\n    \n    \n      7\n      0.138017\n      0.083028\n      0.025641\n      0.974359\n      00:02\n    \n    \n      8\n      0.118409\n      0.083742\n      0.025641\n      0.974359\n      00:02\n    \n    \n      9\n      0.111713\n      0.082776\n      0.025641\n      0.974359\n      00:02\n    \n  \n\n\n\nWe went upto an error rate of just 2.6% which means that our model is correct 97.4% of the time!\nAs you have seen practically, Transfer Learning is a very important technique in Deep Learning that can go a long way. We only used 394 images here and trained for approximately for 20 seconds and got a model which is pretty accurate.\nStay tuned for more."
  },
  {
    "objectID": "posts/pneumonia-classification/handling-imbalance-medical-datasets.html",
    "href": "posts/pneumonia-classification/handling-imbalance-medical-datasets.html",
    "title": "Handling Imbalance in Medical Datasets",
    "section": "",
    "text": "Solving the Imbalance Problem\nThe solution to the problem, as with many solutions to problems in Deep Learning, is simple and something that can be implemented easily. Quoting from their conclusions in the paper: * The method of addressing class imbalance that emerged as dominant in almost all analyzed scenarios was oversampling. * Oversampling should be applied to the level that completely eliminates the imbalance. * Oversampling does not cause over\ftting of convolutional neural networks.\nBasically, Oversampling is artificially making the minority class bigger by replicating it a couple of times. The paper recommends we replicate it until is completely eliminates the imbalance, therefore, our new oversampled ‘NORMAL’ class is going to be the original images, repeated three times. And we don’t have to worry about overfitting of our model too!\n\nos_normal = get_image_files(path/'train'/'NORMAL') * 3\npneumonia = get_image_files(path/'train'/'PNEUMONIA')\n\nprint(f\"Normal Images: {len(os_normal)}. Pneumonia Images: {len(pneumonia)}\")\n\nNormal Images: 4023. Pneumonia Images: 3875\n\n\n\ndata = [['Normal', len(os_normal)], ['Pneumonia', len(pneumonia)]]\nos_df = pd.DataFrame(data, columns=['Class', 'Count'])\n\nsns.barplot(x=os_df['Class'], y=os_df['Count']);\n\n\n\n\nAfter the Oversampling, the distribution between the classes is almost at per. Now our dataset it balanced and we can train a new model on this Balanced Data.\nNow we need a new way to split our dataset when loading it to a DataLoader. Our new Oversampled Path is going to be the Oversampled ‘NOMARL’ class, the original ‘PNEUMONIA’ and the validation data.\nThen we create two variables, train_idx and val_idx, that represent the indexes of the respective category of the images, whether train or validation.\n\nos_path = os_normal + pneumonia + val\n\n\ntrain_idx = [i for i, fname in enumerate(os_path) if 'train' in str(fname)]\nval_idx = [i for i, fname in enumerate(os_path) if 'val' in str(fname)]\n\n\nL(train_idx), L(val_idx)\n\n((#7898) [0,1,2,3,4,5,6,7,8,9...],\n (#16) [7898,7899,7900,7901,7902,7903,7904,7905,7906,7907...])\n\n\nNow we have 7898 images in the Train instead of the original 5216, and we still have 16 Validation images. We load them up in a dataloaders object, which our learner expects, find the new optimal learning rate and train the model:\n\ndblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    get_y=parent_label,\n    splitter=lambda x: [train_idx, val_idx],\n    item_tfms=Resize(460),\n    batch_tfms=augs\n)\n\n\ndls = dblock.dataloaders(path)\n\nlearn = cnn_learner(dls, resnet18, metrics=[error_rate, accuracy])\n\nlearn.lr_find()\n\n\n\n\nSuggestedLRs(lr_min=0.025118863582611083, lr_steep=0.0030199517495930195)\n\n\n\n\n\n\nlearn.fit_one_cycle(3, lr_max=2.5e-2)\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.346110\n      0.308598\n      0.125000\n      0.875000\n      02:18\n    \n    \n      1\n      0.196921\n      0.040522\n      0.000000\n      1.000000\n      02:17\n    \n    \n      2\n      0.107927\n      0.038475\n      0.000000\n      1.000000\n      02:19\n    \n  \n\n\n\nAfter just three epochs, we get 100% accuracy on the Validation Set. The Oversampling Solution worked well for us.\nHowever, as I mentioned before, we only have 16 images on the Validation Set, so its not a good measure on how well our model generalizes.\nSo I combined the Validation and Test Set into one, and used that as my Validation Set to test how well my model generalizes.\n\nmerged_path = os_normal + pneumonia + val + test\n\ntrain_idx = [i for i, fname in enumerate(merged_path) if 'train' in str(fname)]\nval_idx = [i for i, fname in enumerate(merged_path) if 'train' not in str(fname)]\n\n\nL(train_idx), L(val_idx)\n\n((#7898) [0,1,2,3,4,5,6,7,8,9...],\n (#640) [7898,7899,7900,7901,7902,7903,7904,7905,7906,7907...])\n\n\nWe now have 640 images as our validation. How does our model perform with this new data?\n\nlearn.fit_one_cycle(5, lr_max=1e-4)\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.087615\n      0.043269\n      0.014062\n      0.985937\n      02:30\n    \n    \n      1\n      0.079633\n      0.041382\n      0.015625\n      0.984375\n      02:30\n    \n    \n      2\n      0.067601\n      0.054884\n      0.018750\n      0.981250\n      02:31\n    \n    \n      3\n      0.053627\n      0.027576\n      0.006250\n      0.993750\n      02:30\n    \n    \n      4\n      0.037838\n      0.025329\n      0.004688\n      0.995313\n      02:27\n    \n  \n\n\n\n99.5% accuracy after 5 epochs looks good, looks like our model generalizes well.\nSee you next time!"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "technical\n\n\nproject\n\n\ncomputer-vision\n\n\n\nCreating a realtime sign language intepreter.\n\n\n\n\n\n\nJan 20, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  }
]